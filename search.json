[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Contact info",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/Hw0/index.html",
    "href": "posts/Hw0/index.html",
    "title": "Data Visualization of Palmer Penguins",
    "section": "",
    "text": "Data Visualization of Palmer Penguins\nPalmer penguins are fun and we should visualize some of their datas. These are the three different species of Palmer penguins.  image source\nImporting packages and reading penguins datasets.\n\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)  # accessing the csv and set as a panda dataframe\n\nFrom the example in this Github page of Palmer Penguins, it looks very intuitive to plot a scatter plot with flipper length and body mass from the data. We can also make the visualization clear using the ‘hue’ arguement to show different color for each datapoint for each species.\n\nsns.scatterplot(penguins, x = 'Body Mass (g)', y = 'Flipper Length (mm)', hue = 'Species').set(title='Flipper Length and Body Mass relationships in three species of penguins')  # plotting a scatterplot with Body mass and Flipper length with different species of penguins. \nplt.legend(bbox_to_anchor=(1, 1),loc=2)  # setting legends outside of the graph for clarity\nplt.title('Body Mass and Flipper Length Relationships based on different speicies of penguins')  # title for the graph\n\nText(0.5, 1.0, 'Body Mass and Flipper Length Relationships based on different speicies of penguins')"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!"
  },
  {
    "objectID": "posts/Hw4/index.html",
    "href": "posts/Hw4/index.html",
    "title": "Heat Equation with Jax",
    "section": "",
    "text": "In this blog post, I’m going to make a super cool optimization for heat equation with jax and jit(Just-In-Time compilation)."
  },
  {
    "objectID": "posts/Hw4/index.html#set-up-the-basics",
    "href": "posts/Hw4/index.html#set-up-the-basics",
    "title": "Heat Equation with Jax",
    "section": "Set up the basics",
    "text": "Set up the basics\nFor this problem, we will use N = 101 sized matrix and epsilon as diffusion coefficient, and import needed packages.\n\n# import plotly.io as pio\n# pio.renderers.default = \"iframe\"\n# to make sure that graphs are properly displayed\n\n\nN = 101\nepsilon = 0.2\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport jax\nfrom jax.experimental import sparse\nimport jax.numpy as jnp\nimport inspect\nimport time\n\nThen, we can set up a matrix with 1 unit of heat in the middle point of the matrix.\n\n# construct initial condition: 1 unit of heat at midpoint. \nu0 = np.zeros((N, N))\nu0[int(N/2), int(N/2)] = 1.0\nplt.imshow(u0)\n\n\n\n\n\n\n\n\nImport our function defined in the heat_equation.py file.\n\nfrom heat_equation import get_A\nfrom heat_equation import get_sparse_A\nfrom heat_equation import advance_time_matvecmul\nfrom heat_equation import advance_time_spmatvec\nfrom heat_equation import advance_time_numpy\nfrom heat_equation import advance_time_jax\n\nget_A function get the corresponding A matrix for matrix multiplication for our N.\n\nprint(inspect.getsource(get_A))\n\ndef get_A(N):\n    \"\"\"\n    take N as input for matrix size and output matrix A for matrix multiplication used in heat equation\n    \"\"\"\n    n = N * N\n    diagonals = [-4 * np.ones(n), np.ones(n-1), np.ones(n-1), np.ones(n-N), np.ones(n-N)]\n    diagonals[1][(N-1)::N] = 0\n    diagonals[2][(N-1)::N] = 0\n    A = np.diag(diagonals[0]) + np.diag(diagonals[1], 1) + np.diag(diagonals[2], -1) + np.diag(diagonals[3], N) + np.diag(diagonals[4], -N)\n    return A\n\n\n\nadvance_time_matvecmul function multiply the input u matrix with A to simulate time advancement and reshape it back to N x N.\n\nprint(inspect.getsource(advance_time_matvecmul))\n\ndef advance_time_matvecmul(A, u, epsilon):\n    \"\"\"Advances the simulation by one timestep, via matrix-vector multiplication\n    Args:\n        u: N x N grid state at timestep k\n        epsilon: stability constant\n\n    Returns:\n        N x N Grid state at instant k+1\n    \"\"\"\n    N = u.shape[0]\n    u = u + epsilon * (A @ u.flatten()).reshape((N, N))\n    return u\n\n\n\nThis function runs our iteration for 2 * M times and out put a time to run for M times, then output a image representation for every 300 iterations. This function will also time these advancement iterations.\n\ndef iterate_throught_func(advance_func, M = 2700, init = u0, epsilon = epsilon, **kwargs):\n    \"\"\"\n    this function take advance_func function, M iterations, init initial matrix, epsilon diffusion coefficient, \n    and output after M iterations, and output a heatmap for every 300 iterations\n    also times how long does it take to advance function for M times\n    \"\"\"\n    fig, axs = plt.subplots(3, 3, figsize = (10, 10))\n    fig.tight_layout()\n    u_i = init\n    storage = []\n    # use time() to time the total run time of 2700 iterations\n    start = time.time()\n    for i in range(M):\n        storage.append(u_i)\n        u_i = advance_func(u = u_i, epsilon = epsilon, **kwargs)\n    end = time.time()\n    print(end - start)\n    # run it the second time to get picture\n    u_i = init\n    storage = []\n    for i in range(M):\n        storage.append(u_i)\n        u_i = advance_func(u = u_i, epsilon = epsilon, **kwargs)\n    for i in range(int(M/300)):\n        axs[int(i/3), int(i%3)].imshow(storage[i*300])\n        axs[int(i/3), int(i%3)].set_title(f\"Heatmap at t = {i*300}\")\n\n\niterate_throught_func(advance_time_matvecmul, 2700, A = get_A(N))  # advance our first function for 2700 times\n\n36.03676176071167"
  },
  {
    "objectID": "posts/Hw4/index.html#with-jax",
    "href": "posts/Hw4/index.html#with-jax",
    "title": "Heat Equation with Jax",
    "section": "2. With Jax",
    "text": "2. With Jax\nget_sparse_A will return a matrix A in sparsed form with sparse.BC00.fromdense(), and utilize it in our function to multiply. With sparsed matrix, matrix multiplication will be much faster for each iteration of time advancement. Then, I have added advance_time_spmatvec = jax.jit(advance_time_spmatvec) to jit our function which will speed up our function.\n\nprint(inspect.getsource(get_sparse_A)) \nprint(inspect.getsource(advance_time_spmatvec))\n\ndef get_sparse_A(N): \n    \"\"\"\n    take N as input for matrix size and output sparsed matrix A for matrix multiplication used in heat equation\n    \"\"\"\n    A_sp_matrix = sparse.BCOO.fromdense(get_A(N))\n    return A_sp_matrix\n\ndef advance_time_spmatvec(A, u, epsilon):\n    \"\"\"\n    take N as input for matrix size and output sparsed matrix A for matrix multiplication used in heat equation\n    then jit this function with jax jit in next line\n    \"\"\"\n    N = u.shape[0]\n    u = u + epsilon * (A @ u.flatten()).reshape((N, N))\n    return u\n\n\n\n\niterate_throught_func(advance_time_spmatvec, 2700, A = get_sparse_A(N))\n\n0.7655937671661377"
  },
  {
    "objectID": "posts/Hw4/index.html#with-numpy",
    "href": "posts/Hw4/index.html#with-numpy",
    "title": "Heat Equation with Jax",
    "section": "3. With Numpy",
    "text": "3. With Numpy\nadvance_time_numpy will utilize np.roll function to add value to four different direction and reduce the original point based on the heat diffusion funciton. With np.roll() we can apply the heat equation iteration in a vectorized form instead of doing O(N^3) level of matrix multiplication.\n\nprint(inspect.getsource(advance_time_numpy))\n\ndef advance_time_numpy(u, epsilon):\n    \"\"\"\n    take matrix u and diffusion coefficient epsilon as input, output u in the next iteration\n    by utilizing np.roll function to add value to four different direction and reduce the original point\n    \"\"\"\n    u = np.pad(u, 2)\n    u = u + epsilon * (np.roll(u, 1, axis = 0) + np.roll(u, -1, axis = 0) + np.roll(u, 1, axis = 1) + np.roll(u, -1, axis = 1) - 4 * u)\n    return u[2:-2, 2: -2]\n\n\n\n\niterate_throught_func(advance_time_numpy, 2700)\n\n0.25177907943725586"
  },
  {
    "objectID": "posts/Hw4/index.html#with-jax-1",
    "href": "posts/Hw4/index.html#with-jax-1",
    "title": "Heat Equation with Jax",
    "section": "4. With jax",
    "text": "4. With jax\nThis is similar to the numpy method which we use jnp.roll instead of np.roll to add value to four different direction and reduce the original point based on the heat diffusion funciton, and we can also jit the function to make it to run faster.\n\nprint(inspect.getsource(advance_time_jax))\n\ndef advance_time_jax(u, epsilon):\n    \"\"\"\n    take matrix u and diffusion coefficient epsilon as input, output u in the next iteration\n    by utilizing np.roll function to add value to four different direction and reduce the original point\n    then jit this function with jax jit in next line\n    \"\"\"\n    u = jnp.pad(u, 2)\n    u = u + epsilon * (jnp.roll(u, 1, axis = 0) + jnp.roll(u, -1, axis = 0) + jnp.roll(u, 1, axis = 1) + jnp.roll(u, -1, axis = 1) - 4 * u)\n    return u[2:-2, 2: -2]\n\n\n\n\niterate_throught_func(advance_time_jax, 2700)\n\n0.06670808792114258"
  },
  {
    "objectID": "posts/Hw4/index.html#compare-the-methods",
    "href": "posts/Hw4/index.html#compare-the-methods",
    "title": "Heat Equation with Jax",
    "section": "5. Compare the methods",
    "text": "5. Compare the methods\nThe matrix multiplication is the slowest. Then, with sparsed matrix and jitted first function give us an about 20x faster than Part 1.  Utilizing np.roll() give the cpu a much easier task to calculate by only moving the heat points around and add them together, and jax just give a better performance than np in general. Method one appears to be the easiest, since the code is already provided."
  },
  {
    "objectID": "posts/Hw2/index.html",
    "href": "posts/Hw2/index.html",
    "title": "Web Scraping with Scrapy on TMDB",
    "section": "",
    "text": "In this blog post, I’m going to make a super cool web scraper that scrapes your favorite movie in the TMDB.  Here’s a link to my project repository."
  },
  {
    "objectID": "posts/Hw2/index.html#setups-with-terminal-command",
    "href": "posts/Hw2/index.html#setups-with-terminal-command",
    "title": "Web Scraping with Scrapy on TMDB",
    "section": "1. Setups with Terminal Command",
    "text": "1. Setups with Terminal Command\nHere’s how we set up the project. Assuming that you already got scrapy, first, we want to set up a scrapy project using terminal commands. Go to the folder that you want to create the project.\nconda activate PIC16B-24W \nscrapy startproject TMDB_scraper \ncd TMDB_scraper\nWe will see that scrapy.cfg is in this folder, this is where we want to run the other terminal commands. Then, let’s create a py file named tmdb_spider.ipynb in \\TMDB_scraper\\spiders which is all the spider files are located. Add the following lines to the file:\n# to run \n# scrapy crawl tmdb_spider -o movies.csv -a subdir=671-harry-potter-and-the-philosopher-s-stone\n\nimport scrapy\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n    def __init__(self, subdir=None, *args, **kwargs):\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]"
  },
  {
    "objectID": "posts/Hw2/index.html#setup-the-spider-file",
    "href": "posts/Hw2/index.html#setup-the-spider-file",
    "title": "Web Scraping with Scrapy on TMDB",
    "section": "2. Setup the spider file",
    "text": "2. Setup the spider file\nLet’s go through different parse functions that I used to consctruct this spider.\ndef parse(self, response): \n    \"\"\"\n    add /cast to the url to go to full cast page\n    and use parse_full_credits and call parse_full_credit\n    \"\"\"\n    current_url = response.urljoin(f'{response.url}/cast') \n    yield scrapy.Request(current_url, callback=self.parse_full_credits)\nThis is the parse() implementation in our spider class. This will be the first function that will be ran when we call this spider in the terminal. This works very similar to scrapy.request(start_url, self.parse) which return us a response object to play with. Use urljoin to add /cast at the end to take us to the full cast list for your favorite movies. Then, we go to the next function parse_full_credits.\ndef parse_full_credits(self, response):\n        \"\"\"\n        This function is going to parse the full cast and crew page, \n        div @class=info is where all the actors who have an acting role in the movie is\n        use xpath to take different actors' name and information and put them in a dictionary\n        Then, added credit_department=Acting to let next function to go to actor's personal page with acting credits\n        \"\"\"\n        self.actors = {\"actor\": response.xpath(\"//div[@class='info']/p/a/text()\").getall(), \n                         \"web\": response.xpath(\"//div[@class='info']/p/a/@href\").getall()}\n        for i in range(len(self.actors[\"actor\"])):\n            yield scrapy.Request(\"https://www.themoviedb.org\" + self.actors[\"web\"][i] + \"?credit_department=Acting\"\n                                 , callback=self.parse_actor_page)\nTo write this method, I used xpath to take different actors’ name using text(), and their corresponding link with @href from the same element which is from the div[@class ='info']/p/a. We also use string manipulation to go to the actor page with the specific credit category for acting, because we dont want to see a movie that our favorite actors are only crew for the movie. We can set up a loop for each actor in our favorite movie then call our next function parse_actor_page.\ndef parse_actor_page(self, response):\n        \"\"\"\n        This function is parsing actor's personal page, \n        Xpath direction h2[@class = 'title'] contains information of all the movies or tv names that the actor have an acting role in this page. \n        Then, return a dictionary with actor name and movies and tv names for each movie or tv\n        \"\"\"\n        actor_name = response.xpath(\"//h2[@class='title']/a/text()\").get()\n        movie_or_TV_name_list = response.xpath(\"//a[@class='tooltip']/bdi/text()\").getall()\n        for movie_or_TV_name in movie_or_TV_name_list:\n            yield {\"actor\" : actor_name, \"movie_or_TV_name\" : movie_or_TV_name}\nFinally, for parse_actor_page we want to yield a single dictionary with certain actors for each movies or TV they work in, which we get by associating xpath a[@class='tooltip']/bdi’s text in the html elements and loop through all the movie or TV names."
  },
  {
    "objectID": "posts/Hw2/index.html#making-recommandations",
    "href": "posts/Hw2/index.html#making-recommandations",
    "title": "Web Scraping with Scrapy on TMDB",
    "section": "3. Making Recommandations",
    "text": "3. Making Recommandations\nSince my professor’s favorite movie is Harry Potter and the Philosopher’s stone. Let’s run the spider with scrapy crawl tmdb_spider -o movies.csv -a subdir=671-harry-potter-and-the-philosopher-s-stone, and getting results.csv file with columns for actor names and the movies and TV shows on which they featured in.\n\nimport pandas as pd\nimport plotly.express as px\nimport plotly.io as pio\npio.renderers.default = \"iframe\"\ndf = pd.read_csv('results.csv')\ndf\n\n\n\n\n\n\n\n\nactor\nmovie_or_TV_name\n\n\n\n\n0\nDaniel Radcliffe\nHave I Got a Bit More News for You\n\n\n1\nDaniel Radcliffe\nDavid Holmes: The Boy Who Lived\n\n\n2\nDaniel Radcliffe\n100 Years of Warner Bros.\n\n\n3\nDaniel Radcliffe\nMulligan\n\n\n4\nDaniel Radcliffe\nDigman!\n\n\n...\n...\n...\n\n\n2952\nRupert Grint\nThe View\n\n\n2953\nRupert Grint\nGMTV\n\n\n2954\nRupert Grint\nThe Tonight Show with Jay Leno\n\n\n2955\nRupert Grint\nAn Audience with...\n\n\n2956\nRupert Grint\nToday\n\n\n\n\n2957 rows × 2 columns\n\n\n\nWe can see that this movie list is populated with our star actor Daniel Radcliffe. Let’s show three movies or TV shows for each actor.\n\nnew_list = pd.DataFrame()\nfor actor in df.actor.unique():\n    new_list = pd.concat([new_list, df[df.actor == actor][0:3]], ignore_index = True)\nnew_list\n\n\n\n\n\n\n\n\nactor\nmovie_or_TV_name\n\n\n\n\n0\nDaniel Radcliffe\nHave I Got a Bit More News for You\n\n\n1\nDaniel Radcliffe\nDavid Holmes: The Boy Who Lived\n\n\n2\nDaniel Radcliffe\n100 Years of Warner Bros.\n\n\n3\nJamie Yeates\nCreating the World of Harry Potter\n\n\n4\nJamie Yeates\nHarry Potter and the Chamber of Secrets\n\n\n...\n...\n...\n\n\n176\nEmma Watson\nPickled\n\n\n177\nEmma Watson\nPrada Paradoxe\n\n\n178\nRupert Grint\nEnemy of Man\n\n\n179\nRupert Grint\nDavid Holmes: The Boy Who Lived\n\n\n180\nRupert Grint\nKnock at the Cabin\n\n\n\n\n181 rows × 2 columns\n\n\n\nThis is a small enough list for us to read for our favorite movie!  Now we can also make some pretty plots for this dataframe. For example, these is a bar graph for occurance of movies for all the actors appeared in this movie.\n\npx.bar(df.actor.value_counts(), labels = {'value': \"movies and TV\", 'variable' : 'occurance'}, title=\"Appearance in Movies or Shows for Actors in HP and the Philosopher's stone\")  # using value counts to get occurance of a value in a dataframe column. \n\n\n\n\nSuppose that I only want to see Harry Potter movies, this is also an option to do that.\n\nhpdf = df[df.movie_or_TV_name.str[0:12] == \"Harry Potter\"].reset_index(drop=True)  # using string method for all movie and tv names to get starting phrase with Harry Potter\nhpdf\n\n\n\n\n\n\n\n\nactor\nmovie_or_TV_name\n\n\n\n\n0\nDaniel Radcliffe\nHarry Potter 20th Anniversary: Return to Hogwarts\n\n\n1\nDaniel Radcliffe\nHarry Potter and the Deathly Hallows: Part 2\n\n\n2\nDaniel Radcliffe\nHarry Potter and the Deathly Hallows: Part 1\n\n\n3\nDaniel Radcliffe\nHarry Potter and the Half-Blood Prince\n\n\n4\nDaniel Radcliffe\nHarry Potter and the Order of the Phoenix\n\n\n...\n...\n...\n\n\n242\nRupert Grint\nHarry Potter and the Order of the Phoenix\n\n\n243\nRupert Grint\nHarry Potter and the Goblet of Fire\n\n\n244\nRupert Grint\nHarry Potter and the Prisoner of Azkaban\n\n\n245\nRupert Grint\nHarry Potter and the Chamber of Secrets\n\n\n246\nRupert Grint\nHarry Potter and the Philosopher's Stone\n\n\n\n\n247 rows × 2 columns\n\n\n\nI am also interested that how many of the actors act through the entire HP movies, and how many actors only acted a few times, and I also included Harry Potter 20th Anniversary.\n\nprint(hpdf.actor.value_counts())  # use the same trick to get appearance counts\npx.bar(hpdf.actor.value_counts(), labels = {'variable': 'occurance', 'value': \"movies\"},title=\"Appearance in Each Harry Potter Movies or Shows for Actors\")\n\nactor\nTom Felton           10\nMatthew Lewis        10\nDaniel Radcliffe      9\nBonnie Wright         9\nEmma Watson           9\n                     ..\nSaunders Triplets     1\nJean Southern         1\nKieri Kennedy         1\nLeila Hoffman         1\nElizabeth Spriggs     1\nName: count, Length: 63, dtype: int64\n\n\n\n\n\nThis data only contains the actors that appeared in the first film which is a bummer. It is also very interesting that Tom Felton and Mathew Lewis appears more times than Daniel Radcliffe who is the protagonist of the series!"
  },
  {
    "objectID": "posts/Hw5/index.html",
    "href": "posts/Hw5/index.html",
    "title": "Image Classification with Keras",
    "section": "",
    "text": "In this blog post, I’m going to make a super cool machine learning example for image classification. First, we need to set up our package and environment. In this example, we will use keras and a dataset from tensor flow dataset called cats vs dogs which we need to classify the two animals based on pictures.\nimport os\nos.environ['KERAS_BACKEND'] = \"tensorflow\"  # actually tensorflow is the default backend. \n\nfrom keras import utils\nimport tensorflow_datasets as tfds\nimport matplotlib.pyplot as plt\nimport keras\nfrom keras import layers\nfrom random import randint\nIn this step, we need to split train test and validation data.\ntrain_ds, validation_ds, test_ds = tfds.load(\n    \"cats_vs_dogs\",\n    # 40% for training, 10% for validation, and 10% for test (the rest unused)\n    split=[\"train[:40%]\", \"train[40%:50%]\", \"train[50%:60%]\"],\n    as_supervised=True,  # Include labels\n)\n\nprint(f\"Number of training samples: {train_ds.cardinality()}\")\nprint(f\"Number of validation samples: {validation_ds.cardinality()}\")\nprint(f\"Number of test samples: {test_ds.cardinality()}\")\n\nNumber of training samples: 9305\nNumber of validation samples: 2326\nNumber of test samples: 2326\nIn this step, we are resizing all the pictures into 150x150, and batch our datasets with batch(batch_size), prefetch(tf_data.AUTOTUNE) and cache() to reduce our run time.\nresize_fn = keras.layers.Resizing(150, 150)\n\ntrain_ds = train_ds.map(lambda x, y: (resize_fn(x), y))\nvalidation_ds = validation_ds.map(lambda x, y: (resize_fn(x), y))\ntest_ds = test_ds.map(lambda x, y: (resize_fn(x), y))\nfrom tensorflow import data as tf_data\nbatch_size = 64\n\ntrain_ds = train_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\nvalidation_ds = validation_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\ntest_ds = test_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\nThen, we define a show(ds) function that takes our datasets to show three random dog and cats.\nclass_names = ['cat', 'dog']\ndef show(ds):\n  \"\"\"\n  imput a batch of cat and dog pictures, and show three random cat and dogs\n  utilizing randint(0, 63) to get a random picture everytime\n  \"\"\"\n  plt.figure(figsize=(10, 10))\n  for images, labels in ds.take(1):  # take(1) will get a single picture\n    count = 0\n    j = 0\n    while j &lt; 3:\n      i = randint(0, 63)\n      if labels[i] == 0:             # make sure we get a cat to display\n        ax = plt.subplot(2, 3, j+1)\n        plt.imshow(images[i].numpy().astype(\"uint8\"))\n        plt.title(class_names[labels[i]])\n        j += 1                        # add one iteration to make sure we only display three pictures for each\n      else:\n        continue\n      count = 0\n    j = 0\n    while j &lt; 3:\n      i = randint(0, 63)\n      if labels[i] == 1:              # make sure we get a dog to display\n        ax = plt.subplot(2, 3, j+4)\n        plt.imshow(images[i].numpy().astype(\"uint8\"))\n        plt.title(class_names[labels[i]])\n        j += 1                         # add one iteration to make sure we only display three pictures for each\n      else:\n        continue\nshow(train_ds)\n\n2024-03-02 19:48:39.796322: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\nWe utilize as_numpy_iterator() and for loop to output the number of cats and dogs to calculate a baseline accuracy based on proportion.\nlabels_iterator = train_ds.unbatch().map(lambda image, label: label).as_numpy_iterator()\ncat = 0\ndog = 0\ntest_labels = []\ntest_images = []\nfor i in labels_iterator:   # with each iteration to take one label and add counts to each number\n    test_labels.append(i)\n    if i == 0:\n        cat += 1\n    else:\n        dog += 1\nprint(cat, dog)\n\n4637 4668\nprint(\"baseline accuracy is\", dog/(cat+dog))    # our default accuracy is the more labels counts divide by all label counts\n\nbaseline accuracy is 0.5016657710908113"
  },
  {
    "objectID": "posts/Hw5/index.html#basic-model-1",
    "href": "posts/Hw5/index.html#basic-model-1",
    "title": "Image Classification with Keras",
    "section": "Basic Model 1",
    "text": "Basic Model 1\nFirst, we use Dropout to drop 0.2 amount of data everytime before training for randomized event. Then, we can utilize Conv2D and MaxPooling2D layers to reduce the size of features for learning. Afterwards, we Flatten and Dense our features to the size of number of class.\n\nmodel1 = keras.models.Sequential([\n    layers.Input((150, 150, 3)),        # input 150, 150, 3\n    layers.Dropout(0.2),                # drop random 20% to randomize each training\n    layers.Conv2D(64, (3, 3), activation='relu'),      # conv2d to reduce amount of features\n    layers.MaxPooling2D((2, 2)),                       # condense amount of features\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.Flatten(),                                  # flat all features into 1d\n    layers.Dense(64, activation='relu'),               # dense features before the final dense\n    layers.Dense(2) # number of classes\n])\nmodel1.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dropout (Dropout)           (None, 150, 150, 3)       0         \n                                                                 \n conv2d (Conv2D)             (None, 148, 148, 64)      1792      \n                                                                 \n max_pooling2d (MaxPooling2  (None, 74, 74, 64)        0         \n D)                                                              \n                                                                 \n conv2d_1 (Conv2D)           (None, 72, 72, 64)        36928     \n                                                                 \n max_pooling2d_1 (MaxPoolin  (None, 36, 36, 64)        0         \n g2D)                                                            \n                                                                 \n conv2d_2 (Conv2D)           (None, 34, 34, 64)        36928     \n                                                                 \n flatten (Flatten)           (None, 73984)             0         \n                                                                 \n dense (Dense)               (None, 64)                4735040   \n                                                                 \n dense_1 (Dense)             (None, 2)                 130       \n                                                                 \n=================================================================\nTotal params: 4810818 (18.35 MB)\nTrainable params: 4810818 (18.35 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\nmodel1.compile(optimizer='adam',\n              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\nhistory1 = model1.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 11s 63ms/step - accuracy: 0.5159 - loss: 76.1365 - val_accuracy: 0.5696 - val_loss: 0.6758\nEpoch 2/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 53ms/step - accuracy: 0.5825 - loss: 0.6631 - val_accuracy: 0.5684 - val_loss: 0.6837\nEpoch 3/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 54ms/step - accuracy: 0.6374 - loss: 0.6212 - val_accuracy: 0.5619 - val_loss: 0.6831\nEpoch 4/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 54ms/step - accuracy: 0.6731 - loss: 0.5693 - val_accuracy: 0.5623 - val_loss: 0.7289\nEpoch 5/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 11s 74ms/step - accuracy: 0.6965 - loss: 0.5291 - val_accuracy: 0.5585 - val_loss: 0.7798\nEpoch 6/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 11s 74ms/step - accuracy: 0.7303 - loss: 0.4808 - val_accuracy: 0.5383 - val_loss: 0.8293\nEpoch 7/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 54ms/step - accuracy: 0.6968 - loss: 0.5329 - val_accuracy: 0.5494 - val_loss: 0.8959\nEpoch 8/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 54ms/step - accuracy: 0.7210 - loss: 0.5069 - val_accuracy: 0.5482 - val_loss: 0.8249\nEpoch 9/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 11s 74ms/step - accuracy: 0.7247 - loss: 0.5064 - val_accuracy: 0.5370 - val_loss: 1.0278\nEpoch 10/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 53ms/step - accuracy: 0.7753 - loss: 0.4601 - val_accuracy: 0.5696 - val_loss: 1.0642\nEpoch 11/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 54ms/step - accuracy: 0.8312 - loss: 0.3351 - val_accuracy: 0.5327 - val_loss: 1.2938\nEpoch 12/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 53ms/step - accuracy: 0.8702 - loss: 0.2647 - val_accuracy: 0.5464 - val_loss: 1.2194\nEpoch 13/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 11s 74ms/step - accuracy: 0.8606 - loss: 0.2829 - val_accuracy: 0.5473 - val_loss: 1.3734\nEpoch 14/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 55ms/step - accuracy: 0.8708 - loss: 0.2841 - val_accuracy: 0.5443 - val_loss: 1.4037\nEpoch 15/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 11s 74ms/step - accuracy: 0.8942 - loss: 0.2424 - val_accuracy: 0.5460 - val_loss: 1.5973\nEpoch 16/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 11s 74ms/step - accuracy: 0.9099 - loss: 0.2066 - val_accuracy: 0.5615 - val_loss: 1.5195\nEpoch 17/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 53ms/step - accuracy: 0.9230 - loss: 0.1856 - val_accuracy: 0.5709 - val_loss: 1.5209\nEpoch 18/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 11s 74ms/step - accuracy: 0.9338 - loss: 0.1692 - val_accuracy: 0.5636 - val_loss: 1.6108\nEpoch 19/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 53ms/step - accuracy: 0.9346 - loss: 0.1631 - val_accuracy: 0.5658 - val_loss: 1.3993\nEpoch 20/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 53ms/step - accuracy: 0.9485 - loss: 0.1382 - val_accuracy: 0.5731 - val_loss: 1.5646\n\n\nWe utilizing plot to show us accuracy and loss for training and validation graph.\n\ndef plot_history(history):\n    acc = history.history['accuracy']\n    val_acc = history.history['val_accuracy']\n\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n\n    plt.figure(figsize=(8, 8))\n    plt.subplot(2, 1, 1)\n    plt.plot(acc, label='Training Accuracy')\n    plt.plot(val_acc, label='Validation Accuracy')\n    plt.legend(loc='lower right')\n    plt.ylabel('Accuracy')\n    plt.ylim([min(plt.ylim()),1])\n    plt.title('Training and Validation Accuracy')\n\n    plt.subplot(2, 1, 2)\n    plt.plot(loss, label='Training Loss')\n    plt.plot(val_loss, label='Validation Loss')\n    plt.legend(loc='upper right')\n    plt.ylabel('Cross Entropy')\n    plt.ylim([0,1.0])\n    plt.title('Training and Validation Loss')\n    plt.xlabel('epoch')\n    plt.show()\n\n\nplot_history(history1)\n\n\n\n\n\n\n\n\n\nThe accuracy of model1 stabilized between 55% and 60% during training. \nCompare that to the baseline, this model is 5-10 percent better. \nI observe a little overfitting at the end since the val accuracy dropped a little."
  },
  {
    "objectID": "posts/Hw5/index.html#model-2",
    "href": "posts/Hw5/index.html#model-2",
    "title": "Image Classification with Keras",
    "section": "3. Model 2",
    "text": "3. Model 2\nWe can add Randomflip and RandomRotation after input, I use horizontal flip since utilizing vertical flip will yield worse result and also use random rotation factor with 0.1 pi.  Let’s see an example before implement it into model2.\n\nplt.figure(figsize=(12, 12))\nfor images, labels in train_ds.take(1):\n    ax = plt.subplot(2, 3, 1)\n    plt.imshow(images[3].numpy().astype(\"uint8\"))\n    plt.title('Original')\nlayer1 = layers.RandomFlip()                    # random flip layer\nlayer2 = layers.RandomRotation(factor =  0.1)   # random rotaion layer 1\nlayer3 = layers.RandomRotation(factor =  0.3)   # random rotation layer 2\n # we then output pictures through these layer for image[3] a cute cat picture\nax = plt.subplot(2, 3, 2)\nplt.imshow(layer1(images[3].numpy().astype(\"uint8\")).numpy().astype(\"uint8\"))\nplt.title('RandomFlip1')\nax = plt.subplot(2, 3, 3)\nplt.imshow(layer1(images[3].numpy().astype(\"uint8\")).numpy().astype(\"uint8\"))\nplt.title('RandomFlip2')\nax = plt.subplot(2, 3, 4)\nplt.imshow(layer2(images[3].numpy().astype(\"uint8\")).numpy().astype(\"uint8\"))\nplt.title('RandomRotation1')\nax = plt.subplot(2, 3, 5)\nplt.imshow(layer2(images[3].numpy().astype(\"uint8\")).numpy().astype(\"uint8\"))\nplt.title('RandomRotation2')\nax = plt.subplot(2, 3, 6)\nplt.imshow(layer3(images[3].numpy().astype(\"uint8\")).numpy().astype(\"uint8\"))\nplt.title('RandomRotation3')\n\nText(0.5, 1.0, 'RandomRotation3')\n\n\n\n\n\n\n\n\n\nThen, we can implement it into the model’s layer.\n\nmodel2 = keras.models.Sequential([\n    layers.Input((150, 150, 3)),\n    layers.RandomFlip(mode = 'horizontal'),     # flip random images for horizontal direction\n    layers.RandomRotation(factor =  0.1),       # rotate random image for (-0.1pi) or (0.1pi)\n    layers.Dropout(0.2),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.Flatten(),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(2) # number of classes\n])\nmodel2.summary()\n\nModel: \"sequential_7\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ random_flip_3 (RandomFlip)           │ (None, 150, 150, 3)         │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ random_rotation_3 (RandomRotation)   │ (None, 150, 150, 3)         │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_7 (Dropout)                  │ (None, 150, 150, 3)         │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_21 (Conv2D)                   │ (None, 148, 148, 64)        │           1,792 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_14 (MaxPooling2D)      │ (None, 74, 74, 64)          │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_22 (Conv2D)                   │ (None, 72, 72, 64)          │          36,928 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_15 (MaxPooling2D)      │ (None, 36, 36, 64)          │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_23 (Conv2D)                   │ (None, 34, 34, 64)          │          36,928 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ flatten_7 (Flatten)                  │ (None, 73984)               │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_14 (Dense)                     │ (None, 64)                  │       4,735,040 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_15 (Dense)                     │ (None, 2)                   │             130 │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 4,810,818 (18.35 MB)\n\n\n\n Trainable params: 4,810,818 (18.35 MB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\nmodel2.compile(optimizer='adam',\n              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\nhistory2 = model2.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 290ms/step - accuracy: 0.5360 - loss: 72.9558 - val_accuracy: 0.5400 - val_loss: 0.6841\nEpoch 2/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 43s 292ms/step - accuracy: 0.5446 - loss: 0.6838 - val_accuracy: 0.5460 - val_loss: 0.6807\nEpoch 3/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 35s 237ms/step - accuracy: 0.5495 - loss: 0.6803 - val_accuracy: 0.5193 - val_loss: 0.6901\nEpoch 4/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 290ms/step - accuracy: 0.5389 - loss: 0.6837 - val_accuracy: 0.5211 - val_loss: 0.6901\nEpoch 5/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 44s 300ms/step - accuracy: 0.5454 - loss: 0.6851 - val_accuracy: 0.5374 - val_loss: 0.6809\nEpoch 6/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 290ms/step - accuracy: 0.5509 - loss: 0.6786 - val_accuracy: 0.5447 - val_loss: 0.6795\nEpoch 7/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 290ms/step - accuracy: 0.5447 - loss: 0.6778 - val_accuracy: 0.5318 - val_loss: 0.6878\nEpoch 8/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 35s 238ms/step - accuracy: 0.5496 - loss: 0.6792 - val_accuracy: 0.5464 - val_loss: 0.6788\nEpoch 9/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 35s 238ms/step - accuracy: 0.5655 - loss: 0.6768 - val_accuracy: 0.5284 - val_loss: 0.6847\nEpoch 10/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 35s 236ms/step - accuracy: 0.5700 - loss: 0.6767 - val_accuracy: 0.5365 - val_loss: 0.6845\nEpoch 11/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 290ms/step - accuracy: 0.5496 - loss: 0.6767 - val_accuracy: 0.5520 - val_loss: 0.6802\nEpoch 12/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 35s 239ms/step - accuracy: 0.5675 - loss: 0.6766 - val_accuracy: 0.5632 - val_loss: 0.6769\nEpoch 13/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 35s 239ms/step - accuracy: 0.5655 - loss: 0.6693 - val_accuracy: 0.5615 - val_loss: 0.6774\nEpoch 14/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 34s 236ms/step - accuracy: 0.5613 - loss: 0.6718 - val_accuracy: 0.5567 - val_loss: 0.6768\nEpoch 15/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 35s 236ms/step - accuracy: 0.5574 - loss: 0.6788 - val_accuracy: 0.5430 - val_loss: 0.6843\nEpoch 16/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 290ms/step - accuracy: 0.5640 - loss: 0.6770 - val_accuracy: 0.5967 - val_loss: 0.6618\nEpoch 17/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 35s 237ms/step - accuracy: 0.5720 - loss: 0.6664 - val_accuracy: 0.6019 - val_loss: 0.6606\nEpoch 18/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 290ms/step - accuracy: 0.5999 - loss: 0.6596 - val_accuracy: 0.6071 - val_loss: 0.6574\nEpoch 19/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 290ms/step - accuracy: 0.6149 - loss: 0.6486 - val_accuracy: 0.6238 - val_loss: 0.6453\nEpoch 20/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 290ms/step - accuracy: 0.6232 - loss: 0.6408 - val_accuracy: 0.6324 - val_loss: 0.6439\n\n\n\nplot_history(history2)\n\n\n\n\n\n\n\n\n\nThe accuracy of model2 stabilized slightly above 60% during training. \nCompare that to the baseline, this model is 10 percent better. \nI didn’t observe overfitting since the val accuracy kept increasing."
  },
  {
    "objectID": "posts/Hw5/index.html#model-3",
    "href": "posts/Hw5/index.html#model-3",
    "title": "Image Classification with Keras",
    "section": "4. Model 3",
    "text": "4. Model 3\nIn this iteration of the model, we added a layer of preprocessor for standardization after input.\n\ni = keras.Input(shape=(150, 150, 3))\n# The pixel values have the range of (0, 255), but many models will work better if rescaled to (-1, 1.)\n# outputs: `(inputs * scale) + offset`\nscale_layer = keras.layers.Rescaling(scale=1 / 127.5, offset=-1)\nx = scale_layer(i)\npreprocessor = keras.Model(inputs = i, outputs = x)\n\n\nmodel3 = keras.models.Sequential([\n    layers.Input((150, 150, 3)),\n    preprocessor,               # add preprocessor to standardize after input\n    layers.RandomFlip(mode = 'horizontal'),\n    layers.RandomRotation(factor =  0.1),\n    layers.Dropout(0.2),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.Flatten(),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(2) # number of classes\n])\nmodel3.summary()\n\nModel: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n model (Functional)          (None, 150, 150, 3)       0         \n                                                                 \n random_flip (RandomFlip)    (None, 150, 150, 3)       0         \n                                                                 \n random_rotation (RandomRot  (None, 150, 150, 3)       0         \n ation)                                                          \n                                                                 \n dropout_1 (Dropout)         (None, 150, 150, 3)       0         \n                                                                 \n conv2d_3 (Conv2D)           (None, 148, 148, 64)      1792      \n                                                                 \n max_pooling2d_2 (MaxPoolin  (None, 74, 74, 64)        0         \n g2D)                                                            \n                                                                 \n conv2d_4 (Conv2D)           (None, 72, 72, 64)        36928     \n                                                                 \n max_pooling2d_3 (MaxPoolin  (None, 36, 36, 64)        0         \n g2D)                                                            \n                                                                 \n conv2d_5 (Conv2D)           (None, 34, 34, 64)        36928     \n                                                                 \n flatten_1 (Flatten)         (None, 73984)             0         \n                                                                 \n dense_2 (Dense)             (None, 64)                4735040   \n                                                                 \n dense_3 (Dense)             (None, 2)                 130       \n                                                                 \n=================================================================\nTotal params: 4810818 (18.35 MB)\nTrainable params: 4810818 (18.35 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\nmodel3.compile(optimizer='adam',\n              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\nhistory3 = model3.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 [==============================] - 122s 829ms/step - loss: 0.6917 - accuracy: 0.5808 - val_loss: 0.6528 - val_accuracy: 0.6337\nEpoch 2/20\n146/146 [==============================] - 651s 4s/step - loss: 0.6289 - accuracy: 0.6344 - val_loss: 0.6190 - val_accuracy: 0.6612\nEpoch 3/20\n146/146 [==============================] - 107s 730ms/step - loss: 0.5899 - accuracy: 0.6798 - val_loss: 0.5592 - val_accuracy: 0.7334\nEpoch 4/20\n146/146 [==============================] - 103s 704ms/step - loss: 0.5586 - accuracy: 0.7114 - val_loss: 0.5291 - val_accuracy: 0.7455\nEpoch 5/20\n146/146 [==============================] - 105s 719ms/step - loss: 0.5392 - accuracy: 0.7189 - val_loss: 0.5165 - val_accuracy: 0.7502\nEpoch 6/20\n146/146 [==============================] - 109s 743ms/step - loss: 0.5218 - accuracy: 0.7369 - val_loss: 0.5213 - val_accuracy: 0.7549\nEpoch 7/20\n146/146 [==============================] - 110s 755ms/step - loss: 0.4991 - accuracy: 0.7542 - val_loss: 0.5120 - val_accuracy: 0.7571\nEpoch 8/20\n146/146 [==============================] - 114s 777ms/step - loss: 0.4793 - accuracy: 0.7692 - val_loss: 0.4963 - val_accuracy: 0.7769\nEpoch 9/20\n146/146 [==============================] - 119s 818ms/step - loss: 0.4610 - accuracy: 0.7840 - val_loss: 0.5279 - val_accuracy: 0.7554\nEpoch 10/20\n146/146 [==============================] - 107s 729ms/step - loss: 0.4514 - accuracy: 0.7910 - val_loss: 0.5552 - val_accuracy: 0.7420\nEpoch 11/20\n146/146 [==============================] - 105s 718ms/step - loss: 0.4388 - accuracy: 0.8004 - val_loss: 0.4987 - val_accuracy: 0.7769\nEpoch 12/20\n146/146 [==============================] - 105s 721ms/step - loss: 0.4227 - accuracy: 0.8055 - val_loss: 0.5246 - val_accuracy: 0.7713\nEpoch 13/20\n146/146 [==============================] - 107s 730ms/step - loss: 0.4079 - accuracy: 0.8143 - val_loss: 0.5109 - val_accuracy: 0.7782\nEpoch 14/20\n146/146 [==============================] - 107s 731ms/step - loss: 0.3969 - accuracy: 0.8253 - val_loss: 0.4877 - val_accuracy: 0.7842\nEpoch 15/20\n146/146 [==============================] - 108s 738ms/step - loss: 0.3765 - accuracy: 0.8323 - val_loss: 0.4731 - val_accuracy: 0.7885\nEpoch 16/20\n146/146 [==============================] - 108s 736ms/step - loss: 0.3737 - accuracy: 0.8352 - val_loss: 0.4981 - val_accuracy: 0.7898\nEpoch 17/20\n146/146 [==============================] - 107s 731ms/step - loss: 0.3666 - accuracy: 0.8355 - val_loss: 0.4616 - val_accuracy: 0.7966\nEpoch 18/20\n146/146 [==============================] - 105s 721ms/step - loss: 0.3588 - accuracy: 0.8447 - val_loss: 0.4868 - val_accuracy: 0.7893\nEpoch 19/20\n146/146 [==============================] - 110s 753ms/step - loss: 0.3389 - accuracy: 0.8521 - val_loss: 0.4909 - val_accuracy: 0.8022\nEpoch 20/20\n146/146 [==============================] - 111s 760ms/step - loss: 0.3312 - accuracy: 0.8576 - val_loss: 0.4923 - val_accuracy: 0.8014\n\n\n\nplot_history(history3)\n\n\n\n\n\n\n\n\n\nThe accuracy of model3 stabilized between 85% and 86% during training. \nCompare that to the baseline, this model is about 35 percent better. \nI observe a little overfitting at the end since the val accuracy dropped a little."
  },
  {
    "objectID": "posts/Hw5/index.html#model-4",
    "href": "posts/Hw5/index.html#model-4",
    "title": "Image Classification with Keras",
    "section": "5. Model 4",
    "text": "5. Model 4\nWe are utilizing a exisiting base model MobileNetV3Large as a layer, and then we delete our preprocess layer since MobileNetV3Large already contains it.\n\nIMG_SHAPE = (150, 150, 3)\nbase_model = keras.applications.MobileNetV3Large(input_shape=IMG_SHAPE,\n                                               include_top=False,\n                                               weights='imagenet')\nbase_model.trainable = False\n\ni = keras.Input(shape=IMG_SHAPE)\nx = base_model(i, training = False)\nbase_model_layer = keras.Model(inputs = i, outputs = x)\n\nWARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\n\n\nWARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\n\n\n\nmodel4 = keras.models.Sequential([\n    layers.Input((150, 150, 3)),\n    layers.RandomFlip(mode = 'horizontal'),\n    layers.RandomRotation(factor =  0.1),\n    base_model_layer,   # add base model layer\n    layers.Flatten(),\n    layers.Dense(64),\n    layers.Dense(2) # number of classes\n])\nmodel4.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n random_flip_1 (RandomFlip)  (None, 150, 150, 3)       0         \n                                                                 \n random_rotation_1 (RandomR  (None, 150, 150, 3)       0         \n otation)                                                        \n                                                                 \n model (Functional)          (None, 5, 5, 960)         2996352   \n                                                                 \n flatten (Flatten)           (None, 24000)             0         \n                                                                 \n dense (Dense)               (None, 64)                1536064   \n                                                                 \n dense_1 (Dense)             (None, 2)                 130       \n                                                                 \n=================================================================\nTotal params: 4532546 (17.29 MB)\nTrainable params: 1536194 (5.86 MB)\nNon-trainable params: 2996352 (11.43 MB)\n_________________________________________________________________\n\n\n\nmodel4.compile(optimizer='adam',\n              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\nhistory4 = model4.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 [==============================] - 38s 245ms/step - loss: 1.1496 - accuracy: 0.9368 - val_loss: 0.3478 - val_accuracy: 0.9660\nEpoch 2/20\n146/146 [==============================] - 32s 220ms/step - loss: 0.2353 - accuracy: 0.9612 - val_loss: 0.1140 - val_accuracy: 0.9725\nEpoch 3/20\n146/146 [==============================] - 33s 225ms/step - loss: 0.0759 - accuracy: 0.9746 - val_loss: 0.0872 - val_accuracy: 0.9708\nEpoch 4/20\n146/146 [==============================] - 33s 226ms/step - loss: 0.0542 - accuracy: 0.9785 - val_loss: 0.0862 - val_accuracy: 0.9738\nEpoch 5/20\n146/146 [==============================] - 33s 229ms/step - loss: 0.0571 - accuracy: 0.9804 - val_loss: 0.0886 - val_accuracy: 0.9742\nEpoch 6/20\n146/146 [==============================] - 34s 232ms/step - loss: 0.0423 - accuracy: 0.9842 - val_loss: 0.0976 - val_accuracy: 0.9686\nEpoch 7/20\n146/146 [==============================] - 34s 233ms/step - loss: 0.0438 - accuracy: 0.9836 - val_loss: 0.1011 - val_accuracy: 0.9695\nEpoch 8/20\n146/146 [==============================] - 34s 235ms/step - loss: 0.0414 - accuracy: 0.9846 - val_loss: 0.1086 - val_accuracy: 0.9695\nEpoch 9/20\n146/146 [==============================] - 35s 240ms/step - loss: 0.0384 - accuracy: 0.9856 - val_loss: 0.1145 - val_accuracy: 0.9708\nEpoch 10/20\n146/146 [==============================] - 34s 236ms/step - loss: 0.0366 - accuracy: 0.9874 - val_loss: 0.1079 - val_accuracy: 0.9729\nEpoch 11/20\n146/146 [==============================] - 35s 238ms/step - loss: 0.0400 - accuracy: 0.9865 - val_loss: 0.1161 - val_accuracy: 0.9742\nEpoch 12/20\n146/146 [==============================] - 35s 241ms/step - loss: 0.0360 - accuracy: 0.9881 - val_loss: 0.1130 - val_accuracy: 0.9742\nEpoch 13/20\n146/146 [==============================] - 35s 242ms/step - loss: 0.0330 - accuracy: 0.9884 - val_loss: 0.1358 - val_accuracy: 0.9716\nEpoch 14/20\n146/146 [==============================] - 36s 244ms/step - loss: 0.0327 - accuracy: 0.9886 - val_loss: 0.1168 - val_accuracy: 0.9699\nEpoch 15/20\n146/146 [==============================] - 35s 237ms/step - loss: 0.0343 - accuracy: 0.9889 - val_loss: 0.1455 - val_accuracy: 0.9729\nEpoch 16/20\n146/146 [==============================] - 36s 244ms/step - loss: 0.0513 - accuracy: 0.9844 - val_loss: 0.1241 - val_accuracy: 0.9699\nEpoch 17/20\n146/146 [==============================] - 35s 243ms/step - loss: 0.0375 - accuracy: 0.9866 - val_loss: 0.1460 - val_accuracy: 0.9630\nEpoch 18/20\n146/146 [==============================] - 34s 236ms/step - loss: 0.0385 - accuracy: 0.9897 - val_loss: 0.1641 - val_accuracy: 0.9592\nEpoch 19/20\n146/146 [==============================] - 36s 244ms/step - loss: 0.0359 - accuracy: 0.9880 - val_loss: 0.1828 - val_accuracy: 0.9656\nEpoch 20/20\n146/146 [==============================] - 38s 260ms/step - loss: 0.0386 - accuracy: 0.9886 - val_loss: 0.1577 - val_accuracy: 0.9686\n\n\n\nplot_history(history4)\n\n\n\n\n\n\n\n\n\nThe accuracy of model4 stabilized between 96% and 98% during training. \nCompare that to the baseline, this model is about 46 percent better. \nI observe a lot overfitting at the end since the val accuracy drops and increase after epoch 3."
  },
  {
    "objectID": "posts/Hw5/index.html#tests",
    "href": "posts/Hw5/index.html#tests",
    "title": "Image Classification with Keras",
    "section": "6. Tests",
    "text": "6. Tests\nLets test our best performing model4 on our unseen test_ds!\n\nresults = model4.evaluate(test_ds)\nprint(f'Test loss is {results[0]}, test accuracy is {results[1]}')\n\n37/37 [==============================] - 6s 164ms/step - loss: 0.1822 - accuracy: 0.9656\nTest loss is 0.1821718066930771, test accuracy is 0.9656062126159668\n\n\nThe last test accuracy is about 96.56%, and this looks pretty good!"
  },
  {
    "objectID": "posts/Hw3/index.html",
    "href": "posts/Hw3/index.html",
    "title": "Web Development with Flask - A Simple Implementation of Online Message Bank",
    "section": "",
    "text": "In this blog post, I’m going to make a super cool simple online message bank that you can develop yourself with flask. Here’s a link to my project repository which is built upon this example."
  },
  {
    "objectID": "posts/Hw3/index.html#setups-with-terminal-command",
    "href": "posts/Hw3/index.html#setups-with-terminal-command",
    "title": "Web Development with Flask - A Simple Implementation of Online Message Bank",
    "section": "1. Setups with Terminal Command",
    "text": "1. Setups with Terminal Command\nAfter cloning the repository, you can run this page on your local machine with flask run."
  },
  {
    "objectID": "posts/Hw3/index.html#explaining-functions-in-app.py",
    "href": "posts/Hw3/index.html#explaining-functions-in-app.py",
    "title": "Web Development with Flask - A Simple Implementation of Online Message Bank",
    "section": "2. Explaining Functions in app.py",
    "text": "2. Explaining Functions in app.py\n\n(1) get_message_db()\ndef get_message_db():\n  # Use create table if not exists to create a message_db with handle and message for text. \n  try:\n      return g.message_db\n  except:\n      g.message_db = sqlite3.connect(\"messages_db.sqlite\")\n      cmd = 'CREATE TABLE IF NOT EXISTS messages_table (handle TEXT, message TEXT)' \n      cursor = g.message_db.cursor()\n      cursor.execute(cmd)\n      return g.message_db\nThis is the first function we use to check and create a table called messages_table in our database file. With the sql command CREATE TABLE IF NOT EXISTS table (column1, column2), we can create a table in our database if it doesn’t exists. This table includes two text columns with handle and message.\n\n\n(2) basic route defintions\n@app.route('/')\ndef main():\n    return render_template('hello.html')\n\n@app.route('/hello')\ndef hello():\n    return render_template('hello.html')\nmain() and hello() are the most basic functions that call render_template with our html files that we have written.\n\n\n(3) insert_message(request)\ndef insert_message(request):\n    \"\"\"\n    use sql command insert into table .. values .. to insert input into the database. \n    \"\"\"\n    cmd = f'INSERT INTO messages_table (handle, message) VALUES (\"{request.form[\"handle\"]}\", \"{request.form[\"message\"]}\")'\n    cursor = g.message_db.cursor()\n    cursor.execute(cmd)\n    g.message_db.commit()\n    pass\nThis function is used to insert values into our table in the database. The values from the website will form a request object that can be used as input of this function. Then, we can use sql command INSERT INTO table (column) VALUES (value) to insert both handle and message into two corresponding columns in the table.\n\n\n(4) submit()\n@app.route('/submit', methods=['POST', 'GET'])\ndef submit():\n    get_message_db()\n    if request.method == 'GET':\n        return render_template('submit.html')     \n    else:\n        # split successful and error case for inputs. \n        insert_message(request)\n        try: \n            insert_message(request)\n            return render_template('submit.html', thanks = True, handle = request.form['handle'])\n        except:\n            return render_template('submit.html', error = True)\nWith method 'POST' and 'GET', we can split two version of returned webpage. Then, after successfully running insert(request) for this page, we can render template with thanks or error.\n\n\n(5) random_message(n)\ndef random_messages(n): \n    # fetch n elements from the table with random() \n    cmd = f'SELECT * FROM messages_table ORDER BY RANDOM() LIMIT {n};'\n    cursor = get_message_db().cursor()\n    result = cursor.execute(cmd).fetchall()\n    return result\nIn this function, we can fetch n ammounts of random entries in our database’s table with the SELECT * FROM table ORDER BY RANDOM LIMIT n command. Then, this function return a list with two columns and n rows.\n\n\n(6) view()\n@app.route('/view')\ndef view():\n    result = random_messages(5)\n    return render_template('view.html', result = result)\nThis function will call random_messages(5) to return 5 rows of handle and message. Then, we feed this list into view.html as result function, and render it. Then, we will cover how to implement function arguments in the html files."
  },
  {
    "objectID": "posts/Hw3/index.html#explaining-html-template-files",
    "href": "posts/Hw3/index.html#explaining-html-template-files",
    "title": "Web Development with Flask - A Simple Implementation of Online Message Bank",
    "section": "3. Explaining HTML Template Files",
    "text": "3. Explaining HTML Template Files\n{% extends 'base.html' %}\n\n{% block header %}\n  &lt;h1&gt;{% block title %}View Messages{% endblock %}&lt;/h1&gt;\n{% endblock %}\n\n{% block content %}\n  &lt;p&gt;\n    Displaying up to five random messages from past submissions! \n    &lt;br&gt;\n    {% for entry in result %} \n    &lt;message&gt;{{entry[1]}} &lt;/message&gt;\n                               - &lt;i&gt;{{entry[0]}}&lt;/i&gt;\n    &lt;br&gt;\n  {% endfor %}\n  &lt;/p&gt;\n{% endblock %}\nThis is the view.html template. With base.html on top of this webpage, &lt;h1&gt; is a type of text format used for header. Then, we use &lt;p&gt; text format for the remainder of the text. Then, we use for loop function in jinja to display each random entries. I have also made a text format called &lt;message&gt; to differentiate message and handle, and I also italized handle with &lt;i&gt;.\n\n4. Implementation of the Website\nWe can input our name and message into the website in the submit page.   \nThen, we can view a bunch of previous messages that is in the database in the view page."
  },
  {
    "objectID": "posts/Hw1/index.html",
    "href": "posts/Hw1/index.html",
    "title": "Building Global Temperature Database and Use Ploty Express to Make Interesting Figures",
    "section": "",
    "text": "import sqlite3\nimport numpy as np\nimport pandas as pd\nfrom plotly import express as px\nimport plotly.io as pio\nfrom scipy import stats\nimport calendar\npio.renderers.default=\"iframe\""
  },
  {
    "objectID": "posts/Hw1/index.html#importing-the-necessary-modules.",
    "href": "posts/Hw1/index.html#importing-the-necessary-modules.",
    "title": "Building Global Temperature Database and Use Ploty Express to Make Interesting Figures",
    "section": "",
    "text": "import sqlite3\nimport numpy as np\nimport pandas as pd\nfrom plotly import express as px\nimport plotly.io as pio\nfrom scipy import stats\nimport calendar\npio.renderers.default=\"iframe\""
  },
  {
    "objectID": "posts/Hw1/index.html#creating-a-database",
    "href": "posts/Hw1/index.html#creating-a-database",
    "title": "Building Global Temperature Database and Use Ploty Express to Make Interesting Figures",
    "section": "Creating a Database",
    "text": "Creating a Database\nFrom the file temps.csv, station-metadata.csv, fips-10-4-to-iso-country-codes.csv, we can build a database with three such tables. From the class we did a little data wrangling to split the month variable for temps csv, and I am going to do the same.\nI will first work on the temperature table. By using chunksize argument to make temps.csv into managable chunks and use prepare_df function to generate month column based on original “valuex” columns and divide temperature by 100 to get the Celsius for temperature.\n\nconn = sqlite3.connect(\"temps.db\") # create a database in current directory called temps.db\n\n\ndf_iter = pd.read_csv(\"temps.csv\", chunksize = 100000)\ndef prepare_df(df):\n    \"\"\"\n    Using prepare_df to change all the valuex to a month column using stack and setindex function. \n    Finnally change the temperature to celsius degree.\n    \"\"\"\n    df = df.set_index(keys=[\"ID\", \"Year\"])\n    df = df.stack()\n    df = df.reset_index()\n    df = df.rename(columns = {\"level_2\"  : \"Month\" , 0 : \"Temp\"})\n    df[\"Month\"] = df[\"Month\"].str[5:].astype(int)\n    df[\"Temp\"]  = df[\"Temp\"] / 100\n    return(df)\n\n\nfor i, df in enumerate(df_iter):\n    df = prepare_df(df)\n    df.to_sql(\"temperatures\", conn, if_exists = \"replace\" if i == 0 else \"append\", index = False)\n\nThen, using read_csv function to load both csv as tables into the temps.db database.\n\nstations = pd.read_csv(\"station-metadata.csv\")\nstations.to_sql(\"stations\", conn, if_exists = \"replace\", index=False)  # stations.csv to a table in database\ncountries = pd.read_csv(\"fips-10-4-to-iso-country-codes.csv\")\ncountries.to_sql(\"countries\", conn, if_exists = \"replace\", index=False)  # countries.csv to a table in database\n\n279\n\n\nCheck that there are three tables and its columns and close the connection.\n\ncursor = conn.cursor()\ncursor.execute(\"SELECT name FROM sqlite_master WHERE type = 'table'\")\nprint(cursor.fetchall())\n\n[('temperatures',), ('stations',), ('countries',)]\n\n\n\ncursor.execute(\"SELECT sql FROM sqlite_master WHERE type='table';\")  # use sqlite master to show all the tables and columns in databse\nfor result in cursor.fetchall():\n    print(result[0])\n\nCREATE TABLE \"temperatures\" (\n\"ID\" TEXT,\n  \"Year\" INTEGER,\n  \"Month\" INTEGER,\n  \"Temp\" REAL\n)\nCREATE TABLE \"stations\" (\n\"ID\" TEXT,\n  \"LATITUDE\" REAL,\n  \"LONGITUDE\" REAL,\n  \"STNELEV\" REAL,\n  \"NAME\" TEXT\n)\nCREATE TABLE \"countries\" (\n\"FIPS 10-4\" TEXT,\n  \"ISO 3166\" TEXT,\n  \"Name\" TEXT\n)\n\n\n\nconn.close()"
  },
  {
    "objectID": "posts/Hw1/index.html#write-a-query-function",
    "href": "posts/Hw1/index.html#write-a-query-function",
    "title": "Building Global Temperature Database and Use Ploty Express to Make Interesting Figures",
    "section": "2. Write a Query Function",
    "text": "2. Write a Query Function\nImport query_climate_database from climate_database.py which I used sql manipulation to produce panda dataframe needed for this task. Since the first two letters of country id in the station table is coresponding to FIPS 10-4 columns for countries table, we can use JOIN and SUBSTRING function to help us to display the full country name.\n\nfrom climate_database import query_climate_database\nimport inspect\nprint(inspect.getsource(query_climate_database))  # import query function and inspect it\n\ndef query_climate_database(db_file, country, year_begin, year_end, month):\n    \"\"\"\n    This is a function that uses database file and corresponding criteria to filter out \n    and output a dataframe with corresponding requirements using sql query\n    \"\"\"\n    conn = sqlite3.connect(db_file)\n    cmd = \\\n        '''SELECT S.NAME , S.LATITUDE, S.LONGITUDE, C.Name, T.Year, T.Month, T.Temp\n        FROM stations S\n        LEFT JOIN temperatures T on S.id = T.id\n        LEFT JOIN countries C on C.[FIPS 10-4]= SUBSTRING(S.id, 1, 2)\n        WHERE C.NAME = {country} AND T.year &gt;= {year_begin} AND T.year &lt;= {year_end} AND T.month = {month}\n        ''' .format(country = repr(country), year_begin = year_begin, year_end = year_end, month = month)\n    result_df = pd.read_sql_query(cmd, conn)\n    conn.close()\n    result_df = result_df.rename(columns={'Name' : 'Country'})\n    return result_df\n\n\n\n\nquery_climate_database(db_file = \"temps.db\",  # test for India\n                       country = \"India\", \n                       year_begin = 1980, \n                       year_end = 2020,  \n                       month = 1)\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nCountry\nYear\nMonth\nTemp\n\n\n\n\n0\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1980\n1\n23.48\n\n\n1\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1981\n1\n24.57\n\n\n2\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1982\n1\n24.19\n\n\n3\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1983\n1\n23.51\n\n\n4\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1984\n1\n24.81\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3147\nDARJEELING\n27.050\n88.270\nIndia\n1983\n1\n5.10\n\n\n3148\nDARJEELING\n27.050\n88.270\nIndia\n1986\n1\n6.90\n\n\n3149\nDARJEELING\n27.050\n88.270\nIndia\n1994\n1\n8.10\n\n\n3150\nDARJEELING\n27.050\n88.270\nIndia\n1995\n1\n5.60\n\n\n3151\nDARJEELING\n27.050\n88.270\nIndia\n1997\n1\n5.70\n\n\n\n\n3152 rows × 7 columns"
  },
  {
    "objectID": "posts/Hw1/index.html#write-a-geographic-scatter-function-for-yearly-temperature-increases",
    "href": "posts/Hw1/index.html#write-a-geographic-scatter-function-for-yearly-temperature-increases",
    "title": "Building Global Temperature Database and Use Ploty Express to Make Interesting Figures",
    "section": "3. Write a Geographic Scatter Function for Yearly Temperature Increases",
    "text": "3. Write a Geographic Scatter Function for Yearly Temperature Increases\nWith LinearRegression from sklearn, we can get a coeficient of temperature function based on the data.\n\nfrom sklearn.linear_model import LinearRegression\n\ndef coef(data_group):\n    \"\"\"\n    This is a function that intake a datagroup with year and temp to output its linear regression's coefficient alpha. \n    \"\"\"\n    x = data_group[[\"Year\"]]  # 2 brackets because X should be a df\n    y = data_group[\"Temp\"]    # 1 bracket because y should be a series\n    LR = LinearRegression()\n    LR.fit(x, y)\n    return round(LR.coef_[0], 4)  # round it to force decimal \n\n\ndef temperature_coefficient_plot(db_file, country, year_begin, year_end, month, min_obs, **kwargs):\n    \"\"\"\n    This is a function that take database and its corresponding requirements to output a temperature increase coefficient plot on Geo mapbox. \n    \"\"\"\n    #  using query_climate_database to get our temperature dataframe\n    df = query_climate_database(db_file, country, year_begin, year_end, month)\n    #  using a simple for loop to check how many observance made by each unique stations and drop based on min_obs\n    obs = []\n    for station in df['NAME'].unique():\n        obs.append([station, df.isin([station]).sum()['NAME']])\n    obs = pd.DataFrame(obs)\n    df = pd.merge(df, obs, left_on = \"NAME\", right_on = 0).drop(columns = [0])\n    df = df[df[1]&gt;min_obs].drop(columns = [1])\n    #  using coef to take the first coefficient of linear regression for our data and round it to the forth decimal place. \n    Estimated = pd.DataFrame(df.groupby('NAME').apply(coef))\n    #  Then merge it into the original dataframe\n    df = pd.merge(df, Estimated, on = 'NAME')\n    df = df.rename({0 : 'Estimated Yearly Increase (°C)'}, axis=1)\n    #  draw the graph, using calendar.month_name to convert month number to month name. \n    title = f'Estimate of yearly increase in temperatures in {calendar.month_name[month]} for stations in {country}, year {year_begin}-{year_end}'\n    fig = px.scatter_mapbox(df,\n                        lat = \"LATITUDE\",\n                        lon = \"LONGITUDE\",\n                        hover_name = \"NAME\",\n                        color = \"Estimated Yearly Increase (°C)\",\n                        title = title,\n                        **kwargs)\n    return fig\n\n\ncolor_map = px.colors.diverging.RdGy_r\nfig = temperature_coefficient_plot(\"temps.db\", \"India\", 1980, 2020, 1,  # this is the test graph provided in the assignment\n                                   min_obs = 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\nfig.show()\n\n\n\n\n\ncolor_map = px.colors.diverging.RdGy_r\nfig = temperature_coefficient_plot(\"temps.db\", \"United Kingdom\", 1980, 2020, 1,  # this is the test graph provided in the assignment\n                                   min_obs = 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\nfig.show()"
  },
  {
    "objectID": "posts/Hw1/index.html#create-two-more-interesting-figures",
    "href": "posts/Hw1/index.html#create-two-more-interesting-figures",
    "title": "Building Global Temperature Database and Use Ploty Express to Make Interesting Figures",
    "section": "4. Create Two More Interesting Figures",
    "text": "4. Create Two More Interesting Figures\n\n(a) Writing one more query functon\nThis is a more convenient query function to help us fetch dataframe from the database with inclusion of month_begin and month_end without limiting to a single month like the previous function.\n\nfrom climate_database import query\nimport inspect\nprint(inspect.getsource(query))\n\ndef query(db_file, country, year_begin, year_end, month_begin, month_end):\n    \"\"\"\n    This is a function that uses database file and corresponding criteria to filter out \n    and output a dataframe with corresponding requirements using sql query\n    \"\"\"\n    conn = sqlite3.connect(db_file)\n    cmd = \\\n        '''SELECT S.NAME , S.LATITUDE, S.LONGITUDE, C.Name, T.Year, T.Month, T.Temp\n        FROM stations S\n        LEFT JOIN temperatures T on S.id = T.id\n        LEFT JOIN countries C on C.[ISO 3166]= SUBSTRING(S.id, 1, 2)\n        WHERE C.NAME = {country} AND T.year &gt;= {year_begin} AND T.year &lt;= {year_end} AND T.month &gt;= {month_begin} AND T.month &lt;= {month_end}\n        ''' .format(country = repr(country), year_begin = year_begin, year_end = year_end, month_begin = month_begin, month_end = month_end)\n    result_df = pd.read_sql_query(cmd, conn)\n    conn.close()\n    return result_df\n\n\n\n\nquery(db_file = \"temps.db\", \n                country = \"India\",     \n                year_begin = 1980, \n                year_end = 2020,\n                month_begin = 1,\n                month_end = 12)\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nName\nYear\nMonth\nTemp\n\n\n\n\n0\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1980\n1\n23.48\n\n\n1\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1980\n2\n27.16\n\n\n2\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1980\n3\n30.07\n\n\n3\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1980\n4\n32.39\n\n\n4\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1980\n5\n33.04\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n37671\nDARJEELING\n27.050\n88.270\nIndia\n1996\n11\n11.90\n\n\n37672\nDARJEELING\n27.050\n88.270\nIndia\n1996\n12\n8.80\n\n\n37673\nDARJEELING\n27.050\n88.270\nIndia\n1997\n1\n5.70\n\n\n37674\nDARJEELING\n27.050\n88.270\nIndia\n1997\n2\n4.90\n\n\n37675\nDARJEELING\n27.050\n88.270\nIndia\n1997\n3\n10.70\n\n\n\n\n37676 rows × 7 columns\n\n\n\n\n\n(b) Creating the Figures\nThis is a plot of average temperature in a country for multiple years for different months with px.bar function.\n\ndef temp_in_a_year(db_file, country, year1, year2, month_begin, month_end, **kwargs):\n    \"\"\"\n    With our query function we can get any countries year month data from the database. \n    Using year as facet col to show multipe year's plot in one graph\n    \"\"\"\n    df = query(db_file, country, year1, year2, month_begin, month_end)\n    title = f'Average temperature from {calendar.month_name[month_begin]} to {calendar.month_name[month_end]} for stations in {country}, year {year1} to {year2}'\n    new_df = pd.DataFrame(df.groupby(['Year', 'Month'])['Temp'].apply(np.average)).reset_index()\n    fig = px.bar(new_df, x=\"Month\", y =\"Temp\", title = title, facet_col = \"Year\", facet_col_wrap=4, **kwargs)\n    return fig\n\n\nfig = temp_in_a_year(\"temps.db\", \"India\", 2002, 2012, 1, 12)\nfig.show()\n\n\n\n\n\nfig = temp_in_a_year(\"temps.db\", \"United States\", 1999, 2002, 1, 12)\nfig.show()\n\n\n\n\nWe can also show a graph of temperature of same month over the years for a specific country with px.line figures which is more suited for over the year changes.\n\ndef temp_in_a_month(db_file, country, year_begin, year_end, month, **kwargs):\n    \"\"\"\n    Using query function to take a single months data for multiple years\n    Using px.line to graph this\n    \"\"\"\n    df = query(db_file, country, year_begin, year_end, month, month)\n    title = f'Average temperature in {calendar.month_name[month]} for stations in {country}, year {year_begin} to {year_end}'\n    new_df = pd.DataFrame(df.groupby('Year')['Temp'].apply(np.average))\n    fig = px.line(new_df, title = title, **kwargs)\n    return fig\n\n\nfig = temp_in_a_month(\"temps.db\", \"India\", 1999, 2010, 12)\nfig.show()\n\n\n\n\n\nfig = temp_in_a_month(\"temps.db\", \"United States\", 1980, 2010, 12)\nfig.show()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "myblog",
    "section": "",
    "text": "Image Classification with Keras\n\n\n\n\n\n\nhw5\n\n\nneuro networks\n\n\nkeras\n\n\n\n\n\n\n\n\n\nFeb 29, 2024\n\n\nXipeng Du\n\n\n\n\n\n\n\n\n\n\n\n\nHeat Equation with Jax\n\n\n\n\n\n\nhw4\n\n\njax\n\n\nnumpy\n\n\n\n\n\n\n\n\n\nFeb 22, 2024\n\n\nXipeng Du\n\n\n\n\n\n\n\n\n\n\n\n\nWeb Development with Flask - A Simple Implementation of Online Message Bank\n\n\n\n\n\n\nhw3\n\n\nweb development\n\n\n\n\n\n\n\n\n\nFeb 10, 2024\n\n\nXipeng Du\n\n\n\n\n\n\n\n\n\n\n\n\nWeb Scraping with Scrapy on TMDB\n\n\n\n\n\n\nhw2\n\n\nweb scraping\n\n\n\n\n\n\n\n\n\nFeb 4, 2024\n\n\nXipeng Du\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding Global Temperature Database and Use Ploty Express to Make Interesting Figures\n\n\n\n\n\n\nhw1\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJan 26, 2024\n\n\nXipeng Du\n\n\n\n\n\n\n\n\n\n\n\n\nData Visualization of Palmer Penguins\n\n\n\n\n\n\nhw0\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJan 15, 2024\n\n\nXipeng Du\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJan 6, 2024\n\n\nXipeng\n\n\n\n\n\n\nNo matching items"
  }
]