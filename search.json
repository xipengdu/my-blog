[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Contact info",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/Final Project/index.html",
    "href": "posts/Final Project/index.html",
    "title": "Option Pricing with One-dimension Convolutional Neural Network",
    "section": "",
    "text": "Option is a financial contract that give the buyer the right to buy or sell certain quantity of assets as specific strike price on or before maturity date, and the buyer needs to pay premium or “option price” in this context to the seller of this contract. Option pricing is a way to evaluate the fair value of an option which corresponds to its striking price, maturity time and risk involved with the stock. In this project, we aim to use one-dimension convolutional neural network to predict the intrinsic value of the call and put options with regard to its final payoff from the contract. Without loss of generality, we used the S&P 500 index as the stock of choice.\nThe whole project contains three parts: Data preprocessing, CNN model construction and result analysis, and Flask Web Application creation. We first utilize the API and Yahoo Finance to get rudimentary data, do systematic data preprocessing using various techniques, including cleaning, scaling, and database creation using sqlite3. Then we construct the 1D CNN model to predict the option price using pytorch. CNN, namely Convolutional Neural Network, is a type of neural network that is mainly used for image recognition and classification. Here we adopt the 1D version of CNN to predict the intrinsic value of the options. Then we proceed with a robust training session for hyperparameter tuning, early stopping, model evaluation, and result visualization. Finally, we create a web application using Flask to visualize the result and provide a user-friendly interface for users to interact with the model.\nAll code, data, and results are available on our GitHub repository: https://github.com/Yichen-Wang-2003/24W-PIC16B-Group4.git\nRegarding more details, please refer to the flow chart below."
  },
  {
    "objectID": "posts/Final Project/index.html#data-acquisition",
    "href": "posts/Final Project/index.html#data-acquisition",
    "title": "Option Pricing with One-dimension Convolutional Neural Network",
    "section": "1.1 Data Acquisition",
    "text": "1.1 Data Acquisition\nFor the first step of our project is acquiring the data. We found the source data that satisfy our need: Optiondx[link]. This websites contains a text based datasets with end of date data for free. The following is the features. Then, realizing we might need real time stock price data for target, we utilize a package called yfinance which calls yahoo finance’s unofficial api to download data from a time range by yf.download function, which gets us an adjusted closed data at the end of each trading date.\n# getting stock prices for target evaluation\nimport pandas as pd\nimport yfinance as yf\n\ntarget = pd.DataFrame(yf.download(['SPY'], start=\"2023-06-01\", \n    end=\"2023-12-31\")['Adj Close'])\ntarget\n[*********************100%%**********************] 1 of 1 completed\n\n\n\n\n\n\n\n\nAdj Close\n\n\n\n\nDate\n\n\n\n\n\n\n\n\n2023-06-01\n\n\n415.798767\n\n\n\n\n2023-06-02\n\n\n421.811676\n\n\n\n\n2023-06-05\n\n\n421.003418\n\n\n\n\n2023-06-06\n\n\n421.920135\n\n\n\n\n2023-06-07\n\n\n420.461212\n\n\n\n\n…\n\n\n…\n\n\n\n\n2023-12-22\n\n\n472.182892\n\n\n\n\n2023-12-26\n\n\n474.176697\n\n\n\n\n2023-12-27\n\n\n475.034058\n\n\n\n\n2023-12-28\n\n\n475.213501\n\n\n\n\n2023-12-29\n\n\n473.837769\n\n\n\n\n\n147 rows × 1 columns\n\n\n## 1.2 Data Cleaning\nFor data cleaning, we first import necessary packages including numpy, matplotlib, and sqlite3.\n# importing neccessary packages\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sqlite3\nWe have first removed all the contracts that are later than 2023 as a cutoff point which we will be unable to evaluate their target value at a future date.\n# Read in the data from Jan 2023 to May 2023\ndf_2023_h1 = pd.DataFrame()\nfor i in [202301, 202302, 202303, 202304,  202305]:\n    df_2023_h1 = pd.concat([df_2023_h1, pd.read_table(f'data/spy_eod_{i}.txt', \n    sep=',')], ignore_index=True)\ndf_2023_h1.columns = df_2023_h1.columns.str.strip()\n\n# also drop expiration date later than 2024\ndf_2023_h1 = df_2023_h1[df_2023_h1['[EXPIRE_DATE]'] &lt;= ' 2023-12-31']\ndf_2023_h1 = df_2023_h1[df_2023_h1['[EXPIRE_DATE]'] &gt;= ' 2023-06-01']\ndf_2023_h1 = df_2023_h1.reset_index()\nWe strip away all the spaces of the column names in this step for easier access, and removed the entries that target cannot be calculated.\n# change the string dates to datetime64\ndf_2023_h1['[QUOTE_DATE]'] = df_2023_h1['[QUOTE_DATE]'].apply(np.datetime64)\ndf_2023_h1['[EXPIRE_DATE]'] = df_2023_h1['[EXPIRE_DATE]'].apply(np.datetime64)\n\n# merge our adj close stock data on EXPIRE_DATE\ntarget['[EXPIRE_DATE]'] = target.index\ntarget['[EXPIRE_DATE]'].astype('datetime64[ns]')\n\ndf_2023_h1 = pd.merge(df_2023_h1, target, on = '[EXPIRE_DATE]')"
  },
  {
    "objectID": "posts/Final Project/index.html#setting-targets",
    "href": "posts/Final Project/index.html#setting-targets",
    "title": "Option Pricing with One-dimension Convolutional Neural Network",
    "section": "1.3 Setting targets",
    "text": "1.3 Setting targets\nThen, we have to set a target for our machine learning model. We first utilized a naive estimation of the option price, then start to focus on the intrinsic value of call option. with price = (K - S).  Later, we set our target as intrinsic value of price based on payoff of call and put options, based on the function of discounted price = (K - S) * e^(-rt) where r is a risk free investment rate.\nfrom data_cleansing_function import target_setting\nimport inspect\nprint(inspect.getsource(target_setting))\ndf_2023_h1 = target_setting(df_2023_h1)\ntarget = df_2023_h1['discounted_price']\ndef target_setting(df):\n    \"\"\"\n    vectorized operation to calculate the target value based on formula\n    \"\"\"\n    df['-rt'] = -0.04*(df['[EXPIRE_UNIX]'] - df['[QUOTE_UNIXTIME]'])/(3600*365*24)  \n    # unix time is based on seconds\n    df['price_diff'] = df['[STRIKE]'] - df['Adj Close']\n    df['exp(-rt)'] = df['-rt'].apply(lambda x: math.exp(x))\n    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]   \n    df['discounted_price'] = df['price_diff'] * df['exp(-rt)']\n    return df\n    \nWe normalize all numerical columns with a standard scaler.\ndf_2023_h1 = df_2023_h1[['[EXPIRE_UNIX]', '[QUOTE_DATE]', '[EXPIRE_DATE]',\n '[STRIKE]', '[UNDERLYING_LAST]', '[C_DELTA]', '[C_GAMMA]', '[C_VEGA]',\n       '[C_THETA]', '[C_RHO]', '[C_IV]', '[C_VOLUME]','[C_BID]', '[C_ASK]', \n       '[P_DELTA]', '[P_GAMMA]', '[P_VEGA]', '[P_THETA]',\n       '[P_RHO]', '[P_IV]', '[P_VOLUME]', '[P_BID]', '[P_ASK]', 'Adj Close']]\n\ndf_2023_h1 = df_2023_h1.replace(r'^\\s*$', 0, regex=True)\n\n# Basic normalization and standardization\n# run block of code and catch warnings\nimport warnings\nfrom sklearn.preprocessing import StandardScaler\nwith warnings.catch_warnings():\n    # ignore all caught warnings\n    warnings.filterwarnings(\"ignore\")\n    # execute code that will generate warnings\n    numeric_cols = ['[EXPIRE_UNIX]', '[STRIKE]', '[UNDERLYING_LAST]', \n    '[C_DELTA]', '[C_GAMMA]', '[C_VEGA]',\n       '[C_THETA]', '[C_RHO]', '[C_IV]', '[C_VOLUME]','[C_BID]', '[C_ASK]', \n       '[P_DELTA]', '[P_GAMMA]', '[P_VEGA]', '[P_THETA]',\n       '[P_RHO]', '[P_IV]', '[P_VOLUME]', '[P_BID]', '[P_ASK]']  \n    scaler = StandardScaler()\n    df_2023_h1[numeric_cols] = scaler.fit_transform(df_2023_h1[numeric_cols])\ntarget\n0        -279.724554\n1        -269.902630\n2        -260.080706\n3        -250.258782\n4        -245.347819\n             ...    \n127485    -17.254494\n127486    -12.346804\n127487     -7.439115\n127488     -2.531426\n127489      2.376263\nName: discounted_price, Length: 127490, dtype: float64\nOutput the data to sqlite database.\n# output to sqlite database for others to use\nconn = sqlite3.connect(\"data/tables.db\")\ndf_2023_h1.to_sql(\"df_2023_h1_feature\", conn, if_exists = \"replace\", index=False)\ntarget.to_sql(\"df_2023_h1_target\", conn, if_exists = \"replace\", index=False)\nconn.close()"
  },
  {
    "objectID": "posts/Final Project/index.html#data-structures",
    "href": "posts/Final Project/index.html#data-structures",
    "title": "Option Pricing with One-dimension Convolutional Neural Network",
    "section": "1.4 Data Structures",
    "text": "1.4 Data Structures"
  },
  {
    "objectID": "posts/Final Project/index.html#baseline-model-linear-regression",
    "href": "posts/Final Project/index.html#baseline-model-linear-regression",
    "title": "Option Pricing with One-dimension Convolutional Neural Network",
    "section": "Baseline model: Linear Regression",
    "text": "Baseline model: Linear Regression\nWe need to set a target for the neuron network to beat, if any model cannot beat a linear approximation of stock market in the long run it is a failure.\n# custom train test val split with a smaller dataset. \n# Read in the data from the database\nconn = sqlite3.connect('data/tables.db')\n# show database content\ncursor = conn.cursor()\ncursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\nprint(cursor.fetchall())\n# Extract two tables from it and store them in two pd df\nds = pd.read_sql_query(\"SELECT * from df_2023_h1_feature\", conn)\ntarget = pd.read_sql_query(\"SELECT * from df_2023_h1_target\", conn)\n\nds = ds.drop(['[QUOTE_DATE]', '[EXPIRE_DATE]', 'Adj Close'], axis=1)\n\nX_train = ds[0:10000]\ny_train = target[0:10000]\nX_val = ds[10001:11001]\ny_val = target[10001:11001]\nX_test = ds[11002: 12002]\ny_test = target[11002: 12002]\nprint(X_train.shape, y_train.shape, X_val.shape, y_val.shape, \n    X_test.shape, y_test.shape)\n[('df_2023_h1_feature',), ('df_2023_h1_target',)]\n(10000, 21) (10000, 1) (1000, 21) (1000, 1) (1000, 21) (1000, 1)\ntarget = pd.read_sql_query(\"SELECT * from df_2023_h1_target\", conn)\ntarget\n\n\n\n\n\n\n\n\ndiscounted_price\n\n\n\n\n\n\n0\n\n\n-279.724554\n\n\n\n\n1\n\n\n-269.902630\n\n\n\n\n2\n\n\n-260.080706\n\n\n\n\n3\n\n\n-250.258782\n\n\n\n\n4\n\n\n-245.347819\n\n\n\n\n…\n\n\n…\n\n\n\n\n127485\n\n\n-17.254494\n\n\n\n\n127486\n\n\n-12.346804\n\n\n\n\n127487\n\n\n-7.439115\n\n\n\n\n127488\n\n\n-2.531426\n\n\n\n\n127489\n\n\n2.376263\n\n\n\n\n\n127490 rows × 1 columns\n\n\nUtilizing a simple linear regression model from sklearn, we can get a baseline model to compare with our neural network.\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics\n\nreg = LinearRegression()\nreg.fit(X_train, y_train)\n\n\n\nLinearRegression()\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\n\nLinearRegression\n\nLinearRegression()\n\n\n\n\n\nWith this code block we can output a r2_list to check r2 and mse for a range of values for their relationship with the distance with train set, and output plot graphs.\nr2_list = []\nmse_list = []\nX_list = []\nfor i in range(10):\n    X_test = ds.iloc[10000 + i * 10000:11000 + i * 10000]\n    y_test = target.iloc[10000 + i * 10000:11000 + i * 10000]\n    y_pred = reg.predict(X_test)\n    X_list.append(10000 + i * 10000)\n    r2_list.append(metrics.r2_score(y_true=y_test, y_pred=y_pred))\n    mse_list.append(metrics.mean_squared_error(y_true=y_test, y_pred=y_pred))\nplt.plot(X_list, r2_list)\nplt.title('R2 Score for baseline model')\nText(0.5, 1.0, 'R2 Score for baseline model')\n\n\n\nR2 Score baseline\n\n\nplt.plot(X_list, mse_list)\nplt.title('MSE Loss for baseline model')\nText(0.5, 1.0, 'MSE Loss for baseline model')\n\n\n\nMSE Loss baseline\n\n\nLinear regression model is not performing well after 80000 rows, which shows that the linear approximation of stock is not good enough for prediction further into the future. We shall see the CNN performance in the next section."
  },
  {
    "objectID": "posts/Final Project/index.html#feature-and-target-selection",
    "href": "posts/Final Project/index.html#feature-and-target-selection",
    "title": "Option Pricing with One-dimension Convolutional Neural Network",
    "section": "2.1 Feature and Target Selection",
    "text": "2.1 Feature and Target Selection\nThe first step of our model building process is to select the features and target. We will use PyTorch to build the CNN model. PyTorch is a popular open-source machine learning library based on the Torch library, flexible and powerful for machine learning tasks. We will use the torch and torchvision libraries to build the CNN model.\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor \nimport sqlite3\nimport math\nAfter importing all the necessary libraries, we will load the preprocessed dataset from the database. By using the sqlite3 library, we can connect to the database and load the dataset into a pandas dataframe. As shown below, we first create a cursor and use the SQL command to fetch the data from the database. We have two tables in the database, one for feature, and the other for target.\n# Read in the data from the database\nconn = sqlite3.connect('tables.db')\ncursor = conn.cursor()\ncursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\nprint(cursor.fetchall())\n# Extract two tables from it and store them in two pd df\nds = pd.read_sql_query(\"SELECT * from df_2023_h1_feature\", conn)\ntarget = pd.read_sql_query(\"SELECT * from df_2023_h1_target\", conn)\n[('df_2023_h1_feature',), ('df_2023_h1_target',)]\nWe first take a look at the feature table named ds, and we make a copy of it for backup. As shown below, the feature table has a structure of 331609 rows × 22 columns, which has been preprossed in the previous section. We will then do feature selection from it.\nds_new = ds.copy()\nds\n\n\n\n\n\n\n\n\n\n\n\n\nIndex\n[QUOTE_UNIXTIME]\n[EXPIRE_UNIX]\n[STRIKE]\n[UNDERLYING_LAST]\n[C_DELTA]\n…\n\n\n\n\n0\n-1.69160\n-1.531564\n-1.054517\n-2.406592\n1.054125\n…\n\n\n1\n-1.69160\n-1.531564\n-0.936052\n-2.406592\n1.041020\n…\n\n\n2\n-1.69160\n-1.531564\n-0.888665\n-2.406592\n1.035470\n…\n\n\n3\n-1.69160\n-1.531564\n-0.876819\n-2.406592\n1.048893\n…\n\n\n4\n-1.69160\n-1.531564\n-0.864972\n-2.406592\n1.030873\n…\n\n\n…\n…\n…\n…\n…\n…\n…\n\n\n331604\n1.78216\n1.881977\n0.367073\n1.410238\n-0.175447\n…\n\n\n331605\n1.78216\n1.881977\n0.426306\n1.410238\n-0.288188\n…\n\n\n331606\n1.78216\n1.881977\n0.485538\n1.410238\n-0.406236\n…\n\n\n331607\n1.78216\n1.881977\n0.544771\n1.410238\n-0.524528\n…\n\n\n331608\n1.78216\n1.881977\n0.604004\n1.410238\n-0.647832\n…\n\n\n\nTheen we take a glance at the target table with merely one column named “discounted_price”. This column will be the target for our CNN model, as all feature engineering has been done in the data preprocessing phase.\ntarget\n\n\n\n\n\n\n\n\ndiscounted_price\n\n\n\n\n\n\n0\n\n\n-63.916774\n\n\n\n\n1\n\n\n-54.101350\n\n\n\n\n2\n\n\n-50.175181\n\n\n\n\n3\n\n\n-49.193639\n\n\n\n\n4\n\n\n-48.212096\n\n\n\n\n…\n\n\n…\n\n\n\n\n331604\n\n\n-18.619560\n\n\n\n\n331605\n\n\n-13.711848\n\n\n\n\n331606\n\n\n-8.804136\n\n\n\n\n331607\n\n\n-3.896425\n\n\n\n\n331608\n\n\n1.011287\n\n\n\n\n\n331609 rows × 1 columns\n\n\nFor the feature and target selections, we first concatenate the feature and target tables together.\n\n# Merge target and ds directly, as they match and have the same length\nds_new = pd.concat([ds_new, target], axis=1)\n\n\nds_new.columns\n\nIndex(['[QUOTE_UNIXTIME]', '[EXPIRE_UNIX]', '[STRIKE]', '[UNDERLYING_LAST]',\n       '[C_DELTA]', '[C_GAMMA]', '[C_VEGA]', '[C_THETA]', '[C_RHO]', '[C_IV]',\n       '[C_VOLUME]', '[C_BID]', '[C_ASK]', '[P_DELTA]', '[P_GAMMA]',\n       '[P_VEGA]', '[P_THETA]', '[P_RHO]', '[P_IV]', '[P_VOLUME]', '[P_BID]',\n       '[P_ASK]', 'discounted_price'],\n      dtype='object')\n\n\nWe finalize the features and target_1 as our final features and target for the 1D CNN model. We select all option greeks for call and put as well as the quote and expire dates for features; and we multiply the “price_diff” by the exp(-rt) to get the target_1.\n\nfeatures = ds_new[['[QUOTE_UNIXTIME]', '[EXPIRE_UNIX]', '[STRIKE]', \n                   '[UNDERLYING_LAST]', '[C_DELTA]', '[C_GAMMA]', '[C_VEGA]',\n       '[C_THETA]', '[C_RHO]', '[C_IV]', '[C_VOLUME]','[C_BID]', \n       '[C_ASK]', '[P_DELTA]', '[P_GAMMA]', '[P_VEGA]', '[P_THETA]',\n       '[P_RHO]', '[P_IV]', '[P_VOLUME]', '[P_BID]', '[P_ASK]']].values\ntarget_1= ds_new['discounted_price']"
  },
  {
    "objectID": "posts/Final Project/index.html#train-validation-and-test-data-split",
    "href": "posts/Final Project/index.html#train-validation-and-test-data-split",
    "title": "Option Pricing with One-dimension Convolutional Neural Network",
    "section": "2.2 Train, Validation, and Test Data Split",
    "text": "2.2 Train, Validation, and Test Data Split\nAfter feature engineering, we now do the train, validation, and test data split. We will directly use slicing to create the three datasets but not sklearn’s train_test_split function, because each expire date has multiple strike prices, so we need to ensure the integrity of each expire date’s information. The proportion of the train, validation, and test datasets is 0.8, 0.1, and 0.1, respectively. We will use the first 80% of the data for training, the next 10% for validation, and the last 10% for testing.\n\n# Manually using slicing to create the train, validation and test dataset in percentage of 80, 10, 10\nX_train = features[:int(0.8*len(features))]\nX_val = features[int(0.8*len(features)):int(0.9*len(features))]\nX_test = features[int(0.9*len(features)):]\ny_train = target_1[:int(0.8*len(target_1))]\ny_val = target_1[int(0.8*len(target_1)):int(0.9*len(target_1))]\ny_test = target_1[int(0.9*len(target_1)):]\n\nThen we convert these datasets to PyTorch tensors, and then we print out the shapes of the three datasets to ensure that the data split is successful.\n\n# Convert the data to tensor\nX_train = torch.from_numpy(X_train).type(torch.Tensor)\nX_val = torch.from_numpy(X_val).type(torch.Tensor)\nX_test = torch.from_numpy(X_test).type(torch.Tensor)\ny_train = torch.from_numpy(y_train.values).type(torch.Tensor)\ny_val = torch.from_numpy(y_val.values).type(torch.Tensor)\ny_test = torch.from_numpy(y_test.values).type(torch.Tensor)\n\n\nprint(\"Train shapes:\", X_train.shape, y_train.shape)\nprint(\"Validation shapes:\", X_val.shape, y_val.shape)\nprint(\"Test shapes:\", X_test.shape, y_test.shape)\n\nTrain shapes: torch.Size([265287, 22]) torch.Size([265287])\nValidation shapes: torch.Size([33161, 22]) torch.Size([33161])\nTest shapes: torch.Size([33161, 22]) torch.Size([33161])\n\n\nWe need to do some unsqueeze operations to make the data suitable for the CNN model. The CNN model requires the input to be in the shape of (batch_size, channels, sequence_length), so we need to unsqueeze the second dimension of the input data to make it suitable for the CNN model.\n\nX_train = X_train.unsqueeze(2)\nprint(X_train.shape)\nX_val = X_val.unsqueeze(2)\nprint(X_val.shape)\nX_test = X_test.unsqueeze(2)\nprint(X_test.shape)\n\ntorch.Size([265287, 22, 1])\ntorch.Size([33161, 22, 1])\ntorch.Size([33161, 22, 1])"
  },
  {
    "objectID": "posts/Final Project/index.html#d-cnn-model-construction",
    "href": "posts/Final Project/index.html#d-cnn-model-construction",
    "title": "Option Pricing with One-dimension Convolutional Neural Network",
    "section": "2.3 1D CNN Model Construction",
    "text": "2.3 1D CNN Model Construction\nAs all the data is ready, we now construct the 1D CNN Model for the intrinsic value prediction. Before building the class of the model, we first import all torch related libraries and then define the model, including the torch.nn.functional library.\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\n\nIn order to ensure the robustness and generalization of the model, the CNN we create has the following structure:\n\nInitial Convolution Block:\n\nConv1d: 22 input channels, 64 output channels, kernel size=3, stride=1, padding=1\nBatchNorm1d: 64 features (output channels)\n\n\n\nInterpretation: The initial convolution block is used to extract the features from the input data, and the batch normalization layer is used to normalize the features to ensure that the features are in the same scale.\n\n\nResidual Chunk 1:\n\nConv1d: 64 input channels, 64 output channels, kernel size=3, padding=1\nBatchNorm1d: 64 features\nConv1d: 64 input channels, 64 output channels, kernel size=3, padding=1\nBatchNorm1d: 64 features\nNote: Residual connection adds the input of the chunk to its output after these layers.\n\n\n\nInterpretation: The first residual chunk preserves the number of channels at 64 but allows the model to learn an identity function easily, ensuring the layer can improve performance without hurting existing capabilities. It also helps solve gradient vanishing problem.\n\n\nResidual Chunk 2:\n\nConv1d: 64 input channels, 128 output channels, kernel size=3, padding=1, stride=2\nBatchNorm1d: 128 features\nConv1d: 128 input channels, 128 output channels, kernel size=3, padding=1\nBatchNorm1d: 128 features\nShortcut Connection (for residual addition):\n\nConv1d: 64 input channels, 128 output channels, kernel size=1, stride=2\n\n\n\n\nInterpretation: The second residual chunk includes a shortcut connection with a 1D convolutional layer (self.res2_shortcut) to match the dimensionality change for the residual connection. Other functionalities remain the same as the first Res Chunk.\n\n\nDropout Layer:\n\nDropout: 0.3 probability.\nInterpretation: The dropout layer is pplied to reduce overfitting by randomly setting input elements to zero during training with a probability of 0.3 (self.dropout).\n\nAdaptive Pooling and Fully Connected Layers:\n\nAdaptiveAvgPool1d: Output size of 1\nLinear FC layer: 128 input features, 128 output features\nDropout: 0.3 probability\nLinear FC layer: 128 input features, 128 output features\nDropout: 0.3 probability\nLinear FC layer: 128 input features, 128 output features\nDropout: 0.3 probability\nLinear FC layer: 128 input features, 1 output feature\n\n\n\nInterpretation: The adaptive pooling layer is used to output a fixed-length output irrespective of input size, facilitating the connection to fully connected layers. The linear fully connected layers with dropoutapplied between them help to further prevent overfitting. The last fully connected layer reduces the output to 1 dimension.\n\nAnd the visualization of the model structure is shown below: \nWe then import the python file CNN_CODE.py to access the CNN class and all necessary code, and then inspect to use them.\n\nfrom CNN_CODE import Convolution1D\nimport inspect\nprint(inspect.getsource(Convolution1D))\n\nclass Convolution1D(nn.Module):\n    def __init__(self):\n        '''\n        Convolutional Neural Network with 1D convolutions\n        params:\n        - input_channels: number of input channels\n        - output_channels: number of output channels\n        - kernel_size: size of the kernel\n        - stride: stride of the kernel\n        - padding: padding of the kernel\n        '''\n        super(Convolution1D, self).__init__()\n        \n        # Initial Convolution\n        # 22 input channels (features), 64 output channels, 3x3 kernel\n        self.conv1 = nn.Conv1d(in_channels=22, out_channels=64, \n                               kernel_size=3, stride=1, padding=1)\n        # Batch Normalization with 64 features\n        self.bninit = nn.BatchNorm1d(64)\n\n        # Residual chunk 1\n        self.res1_conv1 = nn.Conv1d(64, 64, 3, padding=1)\n        self.res1_bn1 = nn.BatchNorm1d(64)\n        self.res1_conv2 = nn.Conv1d(64, 64, 3, padding=1)\n        self.res1_bn2 = nn.BatchNorm1d(64)\n        \n        # Residual chunk 2\n        self.res2_conv1 = nn.Conv1d(64, 128, 3, padding=1, \n                                stride=2)  # Reduce dimensionality\n        self.res2_bn1 = nn.BatchNorm1d(128)\n        self.res2_conv2 = nn.Conv1d(128, 128, 3, padding=1)\n        self.res2_bn2 = nn.BatchNorm1d(128)\n        self.res2_shortcut = nn.Conv1d(64, 128, 1, \n                            stride=2)  # Shortcut to match dimensions\n\n\n        # Dropout layer\n        self.dropout = nn.Dropout(0.3)\n        # Final global pooling and fully connected layers\n        self.global_pool = nn.AdaptiveAvgPool1d(1)\n        self.fc1 = nn.Linear(128, 128)\n        self.fc2 = nn.Linear(128, 128)\n        self.fc3 = nn.Linear(128, 128)\n        self.fc4 = nn.Linear(128, 1)\n        \n    def forward(self, x):\n        # Initial conv layer\n        x = F.relu(self.bninit(self.conv1(x)))\n        \n        # Residual Chunk 1\n        res1 = self.res1_conv1(x)\n        res1 = F.relu(self.res1_bn1(res1))\n        res1 = self.res1_conv2(res1)\n        x = F.relu(x + res1)\n        \n        # Residual Chunk 2\n        res2 = self.res2_conv1(x)\n        res2 = F.relu(self.res2_bn1(res2))\n        res2 = self.res2_conv2(res2)\n        shortcut = self.res2_shortcut(x)\n        x = F.relu(res2 + shortcut)\n\n        # Final layers\n        x = self.global_pool(x)\n        x = torch.flatten(x, 1)\n        x = self.dropout(x)\n        x = F.relu(self.fc1(x))\n        x = self.dropout(F.relu(self.fc2(x)))\n        x = self.dropout(F.relu(self.fc3(x)))\n        x = self.fc4(x)\n        \n        return x"
  },
  {
    "objectID": "posts/Final Project/index.html#model-training-and-validation",
    "href": "posts/Final Project/index.html#model-training-and-validation",
    "title": "Option Pricing with One-dimension Convolutional Neural Network",
    "section": "2.4 Model Training and Validation",
    "text": "2.4 Model Training and Validation\nAs we constructed our CNN model, we now train and validate the model. We first define the loss function and the optimizer, using the Mean Squared Error (MSE) loss function to calculate the loss and the Adam optimizer to optimize the model. We set the learning rate as 0.01 and the weight decay as 0.0001. We also set various hyperparameters, including the number of epochs and the device to train the model. We set the number of epochs to 100 and the device to “cuda” if it is available; otherwise, we use “cpu”.\nAlso, for preventing early stopping, we use the patience parameter to set the number of epochs to wait for improvement before stopping the training process. We set the patience to 15, which means that if the validation loss does not improve for 15 epochs, the training process will stop. We also set the L2 regularization to 0.0001 to prevent overfitting.\nThen, we create a loop to train and validate the model. We first set the model to training mode and then iterate through the training dataset, then compute the loss and update the model parameters using the optimizer, with the backward propagation.\nAfter this, we set the model to evaluation mode and iterate through the validation dataset to calculate the validation loss. We also print out the training and validation loss for each epoch to monitor the training process. Should the early stopping condition is met, we use torch.save to save the model and stop the training process. The training and validation loss of each epoch are stored, and they are printed as below.\n\nfrom torch import optim\n\nmodel = Convolution1D()\nval_loss_list, train_loss_list =[],[]\nfinal_epoch = 0\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-2, weight_decay=1e-4)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# For early stopping\npatience = 15\noptimal_val_loss = np.inf\ncurrent_patience = 0\n# Training phase\nnum_epochs = 100\nfor epoch in range(num_epochs):\n    model.train()\n    optimizer.zero_grad()\n    outputs = model(X_train).squeeze(-1)\n    loss = criterion(outputs, y_train)\n    train_loss_list.append(loss)\n    # Backward and optimize\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    # Validation phase\n    model.eval()\n    with torch.no_grad():\n        val_outputs = model(X_val).squeeze(-1)\n        val_loss = criterion(val_outputs, y_val)\n        val_loss_list.append(val_loss)\n        # Early stopping\n        if val_loss &lt; optimal_val_loss:\n            optimal_val_loss = val_loss\n            torch.save(model.state_dict(), 'best_model.pt')\n            current_patience = 0\n        else:\n            current_patience += 1\n            if current_patience == patience:\n                print(f'Early stopping at epoch {epoch+1}')\n                final_epoch = epoch\n                # load best model\n                model.load_state_dict(torch.load('best_model.pt'))\n                break\n    \n    if (epoch+1) % 1 == 0:\n        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}, \n              Val Loss: {val_loss.item()}')\n\nEpoch [1/100], Loss: 9172.8681640625, Val Loss: 1818.197265625\nEpoch [2/100], Loss: 9102.5693359375, Val Loss: 1751.77685546875\nEpoch [3/100], Loss: 8275.830078125, Val Loss: 1398.82080078125\nEpoch [4/100], Loss: 5244.42724609375, Val Loss: 681.4796752929688\nEpoch [5/100], Loss: 7110.5087890625, Val Loss: 1006.462890625\nEpoch [6/100], Loss: 3426.317626953125, Val Loss: 1126.9154052734375\nEpoch [7/100], Loss: 4286.83154296875, Val Loss: 938.3817749023438\nEpoch [8/100], Loss: 4022.037109375, Val Loss: 682.0269165039062\nEpoch [9/100], Loss: 3196.9482421875, Val Loss: 917.9542236328125\nEpoch [10/100], Loss: 3701.21533203125, Val Loss: 181.23085021972656\nEpoch [11/100], Loss: 1524.900634765625, Val Loss: 389.5296936035156\nEpoch [12/100], Loss: 1103.0982666015625, Val Loss: 1594.5777587890625\nEpoch [13/100], Loss: 1861.9698486328125, Val Loss: 2287.84619140625\nEpoch [14/100], Loss: 2340.36279296875, Val Loss: 1141.52294921875\nEpoch [15/100], Loss: 1081.2857666015625, Val Loss: 313.52301025390625\nEpoch [16/100], Loss: 581.6431274414062, Val Loss: 108.076416015625\nEpoch [17/100], Loss: 932.9679565429688, Val Loss: 140.25146484375\nEpoch [18/100], Loss: 1336.6429443359375, Val Loss: 145.90013122558594\nEpoch [19/100], Loss: 1214.3756103515625, Val Loss: 147.10081481933594\nEpoch [20/100], Loss: 815.1370239257812, Val Loss: 339.05621337890625\nEpoch [21/100], Loss: 572.9165649414062, Val Loss: 836.8012084960938\nEpoch [22/100], Loss: 672.4065551757812, Val Loss: 1281.7899169921875\nEpoch [23/100], Loss: 980.5892944335938, Val Loss: 1051.82470703125\nEpoch [24/100], Loss: 892.7532958984375, Val Loss: 498.3086853027344\nEpoch [25/100], Loss: 542.4771728515625, Val Loss: 207.02662658691406\nEpoch [26/100], Loss: 491.0587463378906, Val Loss: 146.8715362548828\nEpoch [27/100], Loss: 674.1841430664062, Val Loss: 121.95735168457031\nEpoch [28/100], Loss: 770.651611328125, Val Loss: 80.98929595947266\nEpoch [29/100], Loss: 654.6914672851562, Val Loss: 91.96054077148438\nEpoch [30/100], Loss: 492.27069091796875, Val Loss: 207.66490173339844\nEpoch [31/100], Loss: 465.4485168457031, Val Loss: 369.939453125\nEpoch [32/100], Loss: 586.9057006835938, Val Loss: 391.50579833984375\nEpoch [33/100], Loss: 624.5753173828125, Val Loss: 241.49102783203125\nEpoch [34/100], Loss: 500.69305419921875, Val Loss: 101.10918426513672\nEpoch [35/100], Loss: 427.59552001953125, Val Loss: 51.98616027832031\nEpoch [36/100], Loss: 483.1625061035156, Val Loss: 47.67477798461914\nEpoch [37/100], Loss: 545.8229370117188, Val Loss: 41.39034652709961\nEpoch [38/100], Loss: 508.1427917480469, Val Loss: 36.485774993896484\nEpoch [39/100], Loss: 438.0169677734375, Val Loss: 54.16953659057617\nEpoch [40/100], Loss: 421.1383056640625, Val Loss: 88.96914672851562\nEpoch [41/100], Loss: 471.10845947265625, Val Loss: 100.9154281616211\nEpoch [42/100], Loss: 480.3539123535156, Val Loss: 75.54594421386719\nEpoch [43/100], Loss: 429.4358215332031, Val Loss: 47.808441162109375\nEpoch [44/100], Loss: 406.1731262207031, Val Loss: 37.47623825073242\nEpoch [45/100], Loss: 435.10394287109375, Val Loss: 32.7606086730957\nEpoch [46/100], Loss: 452.689697265625, Val Loss: 29.308931350708008\nEpoch [47/100], Loss: 420.9671630859375, Val Loss: 36.494937896728516\nEpoch [48/100], Loss: 396.0536193847656, Val Loss: 55.476470947265625\nEpoch [49/100], Loss: 404.5439453125, Val Loss: 68.0870361328125\nEpoch [50/100], Loss: 423.883544921875, Val Loss: 56.49263381958008\nEpoch [51/100], Loss: 408.332275390625, Val Loss: 35.67536163330078\nEpoch [52/100], Loss: 384.791748046875, Val Loss: 25.1159725189209\nEpoch [53/100], Loss: 388.8431091308594, Val Loss: 23.977157592773438\nEpoch [54/100], Loss: 401.4180603027344, Val Loss: 23.906402587890625\nEpoch [55/100], Loss: 395.3701477050781, Val Loss: 24.9521484375\nEpoch [56/100], Loss: 379.8179931640625, Val Loss: 29.47606658935547\nEpoch [57/100], Loss: 381.36181640625, Val Loss: 33.6275520324707\nEpoch [58/100], Loss: 389.26708984375, Val Loss: 31.371234893798828\nEpoch [59/100], Loss: 382.2471008300781, Val Loss: 26.508771896362305\nEpoch [60/100], Loss: 369.72607421875, Val Loss: 24.71722984313965\nEpoch [61/100], Loss: 373.2747802734375, Val Loss: 24.789255142211914\nEpoch [62/100], Loss: 372.1296691894531, Val Loss: 25.448476791381836\nEpoch [63/100], Loss: 372.6578063964844, Val Loss: 27.88591957092285\nEpoch [64/100], Loss: 366.294189453125, Val Loss: 32.301979064941406\nEpoch [65/100], Loss: 366.24237060546875, Val Loss: 34.39863967895508\nEpoch [66/100], Loss: 362.286376953125, Val Loss: 31.689903259277344\nEpoch [67/100], Loss: 360.7981872558594, Val Loss: 28.11236572265625\nEpoch [68/100], Loss: 357.0233459472656, Val Loss: 27.745023727416992\nEarly stopping at epoch 69\n\n\nAs shown above, the training phase terminates at epoch 69 due to early stopping, and we can see the rapid decrease of both training and validation loss in the first couple of epochs, which means that the model is learning well. For the last epoch, which is epoch 62, the validation loss is around 27.74, which is a good result for our model. This is because the intrinsic value of the options has a range of over 600, and the square root of the loss is around 5.26, which is a small percentage of the intrinsic value."
  },
  {
    "objectID": "posts/Final Project/index.html#model-testing-and-analysis-of-testing-results",
    "href": "posts/Final Project/index.html#model-testing-and-analysis-of-testing-results",
    "title": "Option Pricing with One-dimension Convolutional Neural Network",
    "section": "2.5 Model Testing and Analysis of Testing Results",
    "text": "2.5 Model Testing and Analysis of Testing Results\nAfter finishing the training and validation process, we test the model using the test dataset. We first load the saved model and set it to evaluation mode. Then we iterate through the test dataset to calculate the test loss. We also print out the test loss to evaluate the performance of the model, as shown below.\n\nmodel.eval()\nwith torch.no_grad():\n    predictions = model(X_test).squeeze(-1)\n    test_loss = criterion(predictions, y_test)\n    print(f'Test Loss: {test_loss.item()}')\ncriterion = torch.nn.MSELoss()\nmse_loss = criterion(predictions, y_test)\nprint(f\"MSE Loss: {mse_loss.item()}\")\nprint(f\"RMSE Loss: {(mse_loss.item()**(0.5))}\")\n# R2 score\nfrom sklearn.metrics import r2_score\nr2 = r2_score(y_test, predictions)\nprint(f\"R2 Score: {r2}\")\n\nTest Loss: 34.36250305175781\nMSE Loss: 34.36250305175781\nRMSE Loss: 5.8619538595725755\nR2 Score: 0.9812109767170123\n\n\nAs shown above, we get the test loss(MSE) of around 34.26, which indicates that the model is performing well on the test dataset. As the fluctuation of the intrinsic value of the options is large with a range of 350, the test loss of 34.26 is a good result for our model. We then visualize the predicted intrinsic value and the actual intrinsic value of the options to see how well the model performs. We plot the predicted intrinsic value and the actual intrinsic value of the options within the test dataset using matplotlib, as shown below.\n\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(14,7))\nplt.plot(predictions, label='Predicted',color = 'red')\nplt.plot(y_test, label = \"True\", color = 'blue')\nplt.legend()\nplt.title('Test Set Predictions')\n\nText(0.5, 1.0, 'Test Set Predictions')\n\n\n\n\n\n\n\n\n\nThe visualization above sufficiently showcases the complexity of the intrinsic value and the model’s strong ability to capture the features of the intrinsic value. As the blue curve stands for the ground truth, and the red curve stands for the predictions, we can see that the red curve is very close to the blue curve and two curves largely overlap. This indicates that the model we construct is robust and generalizes well to the test dataset.\nWe also visualize the train and validation losses, as we need to see if there is any overfitting or underfitting. We first convert each element in two lists storing validation and train losses from torch tensor to numpy array. Then we use matplotlib to draw the plot.\nAs shown below, the train and validation losses are plotted against the number of epochs. We observe that there is no evidence of overfitting here. Both losses decrease rapidly at the beginning with some fluctuations,but in general they tend to level off, suggesting that the model is learning generalizable patterns.\n\nval_loss_numpy, train_loss_numpy = [],[]\nfor i in val_loss_list:\n    val_loss_numpy.append(i.numpy())\nfor j in train_loss_list:\n    train_loss_numpy.append(j.detach().numpy())\n\n\n\nimport matplotlib.pyplot as plt\nplt.plot(train_loss_numpy, label='Train Loss')\nplt.plot(val_loss_numpy, label='Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.title(\"Train vs Validation Loss\")\nplt.show()\n\n\n\n\n\n\n\n\nFrom our experience with the baseline Linear Regression Model and Autogluon Model, the R2 score drops quickly against time periods, and MSE loss shoots up fast. With the CNN model, we tested both against time as a metric against previous models. The timestamp in these graphs is standardized.\n\ntest_part = y_test.numpy()\n# print(len(test_part))\npred_part = predictions.numpy()\n\n# print(len(another))\ntest_part = pd.DataFrame(test_part)\npred_part = pd.DataFrame(pred_part)\n\nds_test = ds_new[int(0.9*len(features)):].reset_index(drop=True)\n\n# print(ds_test)\ntest_part.columns = ['true_intrinsic']\npred_part.columns = ['pred_intrinsic']\ntogether = pd.concat([ds_test, test_part],axis=1)\ntogether = pd.concat([together, pred_part],axis=1)\n# print(together)\nprint(together.columns)\n\nIndex(['[QUOTE_UNIXTIME]', '[EXPIRE_UNIX]', '[STRIKE]', '[UNDERLYING_LAST]',\n       '[C_DELTA]', '[C_GAMMA]', '[C_VEGA]', '[C_THETA]', '[C_RHO]', '[C_IV]',\n       '[C_VOLUME]', '[C_BID]', '[C_ASK]', '[P_DELTA]', '[P_GAMMA]',\n       '[P_VEGA]', '[P_THETA]', '[P_RHO]', '[P_IV]', '[P_VOLUME]', '[P_BID]',\n       '[P_ASK]', 'discounted_price', 'true_intrinsic', 'pred_intrinsic'],\n      dtype='object')\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\n\nresult_df = together\n\nplt.xlabel('timestamp')\nplt.ylabel('R2 score')\nplt.title(\"R2 score Across Time\")\nplt.plot(result_df[['[QUOTE_UNIXTIME]', 'true_intrinsic', \n'pred_intrinsic']].groupby('[QUOTE_UNIXTIME]').apply(lambda x: \nmetrics.r2_score(x['true_intrinsic'], x['pred_intrinsic'])))\nplt.show()\nThe R2 score trend shown below is kind of turbulent, and in general the R2 score remains stable over time. Also, R2 score has a much better performance than the baseline model as we can see its value never goes below 0.95.\n\n\n\nR2 score Across Time\n\n\nplt.xlabel('timestamp')\nplt.ylabel('MSE Loss')\nplt.title(\"MSE Loss Across Time\")\nplt.plot(result_df[['[QUOTE_UNIXTIME]', 'true_intrinsic', \n'pred_intrinsic']].groupby('[QUOTE_UNIXTIME]').apply(lambda x: \nmetrics.mean_squared_error(x['true_intrinsic'], x['pred_intrinsic'])))\nplt.show()\nTHe MSE loss also performs a fluctuating trend, but it is way smaller than the baseline model’s MSE loss. So the CNN model is a good choice for the intrinsic value prediction of options and the performance is pretty good.\n[\nWith similar test and train sizes, the CNN model produces much more stable results over extended periods of time than other models. With a rather random and non-Gaussian dataset, this BEYOND-PIC16B-level model performs relatively well into the future and can produce better results than traditional machine learning algorithms utilized in PIC16A."
  },
  {
    "objectID": "posts/Final Project/index.html#web-app-implementation-outline",
    "href": "posts/Final Project/index.html#web-app-implementation-outline",
    "title": "Option Pricing with One-dimension Convolutional Neural Network",
    "section": "3.0 Web App Implementation Outline",
    "text": "3.0 Web App Implementation Outline\nThe implementation strategy for our Flask web application development can be encapsulated as follows:\n\nWeb Application Framework Construction: Utilize the Flask framework to build a web application that processes user-uploaded financial data and displays prediction outcomes.\nModel Preparation: Define a Convolutional Neural Network (CNN) specifically designed for financial data prediction. This model includes multiple convolutional layers, batch normalization layers, and residual connections to boost performance and training efficiency—a milestone already achieved by our team.\nModel Loading and Data Preprocessing: Load the pre-trained model (best_model_ultimatel.pth) and perform data preprocessing. This step involves reading data from an uploaded CSV file and applying the preprocess_data function to execute feature scaling, data splitting, and other preparatory operations, mirroring the CNN model setup.\nModel Training and Prediction: Employ the PyTorch library to load the pre-trained model and input the preprocessed data for prediction.\nResults Presentation: Exhibit the model’s performance metrics on the test dataset, such as Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and R-squared score, along with the prediction outcomes. These are visualized using Plotly charts, providing a comprehensive view to the users.\n\n\n\n\nOur whole flask folder(flask.op.PerfectFINALver)\n\n\n\napp.py: Central to the application, this file contains key functions like Convolution1D, preprocess_data, Index, show_prediction, get_table_info, view_uploaded_database, and view_database, which are integral to the app’s operation.\nTemplates folder: This directory holds all the HTML files needed for the app’s user interface, enabling interactions and data display. It includes index.html, plot.html, view_uploaded_data.html, show_prediction.html, Test_Set_Predictions.html, and view_data.html.\nData folder: Contains sample CSV files, df_2023_h1_feature.csv and df_2023_h1_target.csv, used for data upload and result demonstration in the app.\nModel folder: Houses the pre-trained model file essential for making data predictions.\n__pycache__ folder: A system-generated directory that caches bytecode, enhancing the program’s execution speed.\nStatic folder: Stores static files, crucial for the app’s styling and interactive features.\n\nThis organizational structure ensures that each aspect of our Flask web application is well-arranged and easily accessible, supporting efficient development and maintenance."
  },
  {
    "objectID": "posts/Final Project/index.html#overview-and-impact-analysis-of-functions-in-app.py",
    "href": "posts/Final Project/index.html#overview-and-impact-analysis-of-functions-in-app.py",
    "title": "Option Pricing with One-dimension Convolutional Neural Network",
    "section": "3.1 Overview and Impact Analysis of Functions in app.py",
    "text": "3.1 Overview and Impact Analysis of Functions in app.py\nIn the app.py file, several key functionalities are pivotal to the web application’s operation, specifically tailored for financial data analysis and option pricing evaluation. These include:\n\nUpload and Preprocess Financial Data: This functionality allows users to upload their financial datasets and applies preprocessing techniques to prepare the data for analysis.\nView Uploaded CSV Files: Users can view the list of uploaded CSV files, providing an overview of the data available for processing.\nView Details of the Uploaded CSV Files: This feature enables users to delve into the specifics of each uploaded CSV file, examining the data more closely.\nEvaluate Option Pricing and Display Evaluation Results: The application assesses the pricing of options based on the uploaded data and displays the results, offering valuable insights into option valuation.\n\nThe app.py file incorporates essential libraries and modules to facilitate web development, data processing, machine learning, and visualization:\nfrom flask import Flask, render_template, url_for, request, flash, redirect\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom werkzeug.utils import secure_filename\nfrom torch import nn\nimport torch.nn.functional as F\nfrom flask import jsonify\nimport sqlite3\nimport math\nfrom sklearn.preprocessing import StandardScaler\nimport plotly.graph_objects as go\nimport os\n\nConvolution1D Class\nThe Convolution1D class within our application is a custom neural network module extending nn.Module from PyTorch, designed for one-dimensional convolutional operations. This class embodies a series of convolutional, batch normalization, and fully connected layers, structured to facilitate complex pattern recognition in financial data. Notably, this implementation includes residual connections and dropout for robustness and generalization.\nNote: Building on the foundational work previously established by our team members, this segment is presented without extensive analysis to minimize redundancy.\n\n\nTorch.load\nIn this section of the code, we address the computational efficiency and resource optimization for running our neural network model. The code snippet demonstrates the dynamic allocation of processing units, preferring GPU over CPU for faster computation, which is crucial for handling complex models like Convolution1D. We then load the model’s state dictionary from the file ‘best_model_ultimate.pt’. The map_location argument ensures that the loaded state dict is moved to the appropriate device. This step initializes the model with previously trained parameters, allowing for further training,evaluation, or inference without retraining from scratch. Here’s a detailed look at the operations:\n# Check for available GPU, otherwise use CPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# Initialize the Convolution1D model and transfer it to the chosen device\nmodel_loaded = Convolution1D().to(device)\n# Load the previously trained model state\nmodel_loaded.load_state_dict(torch.load('model/best_model_new.pt', map_location=device))\n# Set the model to evaluation mode\nmodel_loaded.eval()\n\n\nPreprocess_data\nThe preprocess_data function tasked with preparing the financial data for subsequent analysis. It performs several key steps:\n\nData Loading: Reads the financial datasets from the specified CSV files.\nNormalization: Applies StandardScaler to normalize specific columns, ensuring uniform data scaling.\nData Merging: Combines the feature and target data into a single DataFrame for comprehensive analysis.\nData Splitting: Segregates the dataset into training, validation, and test sets, facilitating a structured approach to model training and evaluation.\n\nGiven that these processes align with our prior data handling endeavors, we will highlight only the main actions and points, avoiding detailed analysis to prevent redundancy.\n\n\nIndex()\nThe index function in app.py serves as the main entry point for our Flask web application. It handles both GET and POST requests, managing file uploads, data preprocessing, model evaluation, and rendering the results. This function aligns with our previous work, emphasizing efficient data processing and model integration. Key points include handling file uploads, data preprocessing, model prediction, error calculation, and result visualization. Below is the detailed code:\n@app.route('/', methods=['GET', 'POST'])\ndef index():\n    \"\"\"\n    This function serves as the endpoint for the root URL ('/'). \n    It handles both GET and POST requests,\n    rendering the index page of the web application. \n    During a POST request, it processes uploaded files\n    for option pricing evaluation, performs predictions, and returns the results \n    along with rendering the HTML template.\n\n    Returns:\n        render_template: Renders the HTML template based on the \n        request method and the operations performed.\n    \"\"\"\n    # Check if the request method is POST, which indicates that data has been \n    # submitted to the server\n    if request.method == 'POST':\n        # Check if both files are present in the request\n        if 'file1' not in request.files or 'file2' not in request.files:\n            # If either file is missing, flash a message to the user and \n            # reload the page\n            flash('No file part')\n            return redirect(request.url)\n        file1 = request.files['file1']\n        file2 = request.files['file2']\n        \n        # Check if file names are not empty, meaning that the user \n        # has selected files\n        if file1.filename == '' or file2.filename == '':\n            # If no file is selected, flash a message and reload the page\n            flash('No selected file')\n            return redirect(request.url)\n        \n        # Check if both files exist and proceed with processing\n        if file1 and file2:\n            # Secure the file names and prepare the file paths\n            filename1 = secure_filename(file1.filename)\n            filename2 = secure_filename(file2.filename)\n            file1_path = os.path.join(app.config['UPLOAD_FOLDER'], \n            filename1)\n            file2_path = os.path.join(app.config['UPLOAD_FOLDER'], \n            filename2)\n            # Save the files to the server\n            file1.save(file1_path)\n            file2.save(file2_path)\n            # Notify the user that files have been uploaded successfully\n            flash('Files successfully uploaded')\n\n            # Preprocess the data from the uploaded files\n            X_train, X_val, X_test, y_train, y_val, \n            y_test = preprocess_data(file1_path,  file2_path)\n\n            # Evaluate the model on the uploaded data\n            criterion = nn.MSELoss()  # Mean Squared Error Loss function\n            with torch.no_grad():  # No gradient computation for evaluation \n                # to save memory and computations\n                output = model_loaded(X_test.to(device)).squeeze(-1)  \n                # Model prediction\n                predictions = output\n                test_loss = criterion(predictions, y_test.to(device))  \n                # Calculate the test loss\n\n            # Compute the Root Mean Square Error (RMSE) for the test data\n            mse_loss = criterion(predictions, y_test)\n            rmse_loss = mse_loss.item() ** (0.5)\n            \n            # Compute the R-squared (R2) score to measure the goodness of fit\n            from sklearn.metrics import r2_score\n            r2 = r2_score(y_test, predictions)\n            \n            # Create a plot of the predictions against the true values\n            fig = go.Figure()\n            fig.add_trace(go.Scatter(x=np.arange(len(output)), \n            y=output.squeeze().numpy(), mode='lines', name='Predicted', \n            line=dict(color='red')))\n            fig.add_trace(go.Scatter(x=np.arange(len(y_test)), \n            y=y_test.numpy(), mode='lines', name='True', line=dict(color='blue')))\n            fig.update_layout(title='Test Set Predictions', xaxis_title='Index', \n            yaxis_title='Value')\n            \n            # Save the plot output to a file\n            plot_output_path = os.path.join('static', 'Test_Set_Predictions.html')\n            fig.write_html(plot_output_path)\n            \n            # Render the index HTML template with the results and plot path\n            return render_template('index.html', test_loss=test_loss.item(), \n            mse_loss=mse_loss.item(),\n                                   rmse_loss=rmse_loss, r2_score=r2,\n                                    plot=plot_output_path)\n\n    # If the request method is GET, render the index HTML \n    # template without any prediction or plot\n    return render_template('index.html', prediction=None, plot=None)\nIn summary, the index function is integral to the Flask application. It is pivotal for deploying predictive models effectively, ensuring that the application not only performs its intended analytical tasks but also provides a user-friendly interface for interaction and result interpretation.\n\n\nshow_prediction\nThe show_prediction function in app.py is designed to handle GET requests specifically for displaying the prediction results on a separate page. This functionality is crucial for providing users with access to the outcome of their data analysis, ensuring a clear and dedicated view of the predictive results.\nHere’s a detailed analysis of the function:\n\nPath Definition: Initially, the function defines the path to the prediction results file, typically stored in the static directory. This standardization of file location facilitates consistent access and retrieval of the results.\nExistence Check: The function then checks if the prediction results file exists at the specified path. This check is essential to prevent errors that would occur if the application attempted to display a non-existent file.\nConditional Rendering: If the file exists, the function proceeds to render an HTML template (Test_Set_Predictions.html) specifically designed to display the prediction results. This template is passed the path of the prediction plot file, ensuring that the correct data is displayed.\nRedirection: In cases where the prediction results file does not exist, the function redirects the user to the index page. This redirection mechanism prevents user confusion and ensures a smooth user experience by guiding them back to the starting point of the application.\n\nHere is the complete code snippet:\n@app.route('/show_prediction', methods=['GET'])\ndef show_prediction():\n    \n    # Define the path to the prediction results file, \n    # assumed to be in the 'static' directory\n    plot_output_path = os.path.join('static', 'Test_Set_Predictions.html')\n    \n    # Check if the prediction results file exists\n    if not os.path.exists(plot_output_path):\n        # If the file does not exist, redirect the user to the index page\n        return redirect(url_for('index'))\n\n    # If the file exists, render the template to display the prediction results,\n    # passing the path of the prediction plot file to the template\n    return render_template('Test_Set_Predictions.html', \n    plot_output_path=plot_output_path)\n\n\nget_table_info\nThe get_table_info function is designed to streamline the process of extracting filenames from full file paths, which is a common requirement in web applications handling file uploads. By utilizing os.path.basename, it efficiently strips the directory path, leaving only the file name. This functionality is particularly useful in scenarios where the display or processing of filenames, independent of their storage paths, is required. Here’s how the function operates:\ndef get_table_info(file1_path, file2_path):\n    return [os.path.basename(file1_path), os.path.basename(file2_path)]\n\n\nview_uploaded_data\nThe view_uploaded_data function in the Flask web application serves to showcase the files that have been uploaded by users. This endpoint, accessible via a GET request, retrieves and displays the contents of the upload directory in the application’s user interface. Here’s an in-depth look at the function and its code:\n@app.route('/view_uploaded_data', methods=['GET'])\ndef view_uploaded_data():\n    \"\"\"\n    Handles the GET request to display the uploaded database files.\n\n    This endpoint fetches the list of files present in the upload \n    directory and displays them\n    on the 'view_uploaded_data.html' page. \n    This allows users to see which files have been\n    uploaded to the application.\n\n    The function retrieves the filenames from the specified \n    upload folder set in the app's configuration\n    and passes these filenames to the rendering template.\n    \n    \"\"\"\n    \n    # Retrieve the list of files in the upload directory\n    files = os.listdir(app.config['UPLOAD_FOLDER'])\n    \n    # Render the HTML template, passing the list of files \n    for display on the webpage\n    return render_template('view_uploaded_data.html', files=files)\n\n\nview_data\nThe view_data endpoint in the Flask application is designed to showcase the details of the uploaded CSV files through a GET request. It facilitates the inspection of the contents of these files, enhancing the user’s ability to interact with and analyze the uploaded data. The function operates by expecting filename1 and filename2 as query parameters, utilizing these to locate and display the respective files’ contents. Here’s a closer look at the function and its operations:\n@app.route('/view_data', methods=['GET'])\ndef view_data():\n    \"\"\"\n    The function checks for the existence of the specified files \n    in the upload directory.\n    If both files exist, it reads them as CSVs and prepares the data for viewing. \n    If either\n    file is missing or an error occurs during file reading,\n    the user is redirected to the file upload view \n    with an appropriate error message.\n    \"\"\"\n    # Retrieve filenames from the request's query parameters\n    filename1 = request.args.get('filename1')\n    filename2 = request.args.get('filename2')\n    \n    # Validate that both filenames are provided\n    if not filename1 or not filename2:\n        flash('No data file selected.')\n        return redirect(url_for('view_uploaded_data'))\n    \n    # Construct full file paths\n    file1_path = os.path.join(app.config['UPLOAD_FOLDER'], filename1)\n    file2_path = os.path.join(app.config['UPLOAD_FOLDER'], filename2)\n\n    # Check if both files exist in the specified upload folder\n    if not os.path.exists(file1_path) or not os.path.exists(file2_path):\n        flash('Data file not found.')\n        return redirect(url_for('view_uploaded_data'))\n\n    try:\n        # Attempt to read the files as CSVs and store their contents\n        ds = pd.read_csv(file1_path)\n        target = pd.read_csv(file2_path)\n        table_data = {'Dataset 1': ds, 'Dataset 2': target}\n    except Exception as e:\n        # Handle any error that occurs during file reading \n        # and flash an error message\n        flash(f'Error accessing CSV files: {e}')\n        return redirect(url_for('view_uploaded_data'))\n    # Render the view template with the loaded table data and filenames\n    return render_template('view_data.html', tables=table_data,\n     filename1=filename1, filename2=filename2)\nThe concluding part of our Flask application’s code features the standard Python idiom to check if the script is executed as the main program and not imported as a module. This check is crucial for initiating the Flask server only when the script is run directly, ensuring that the application’s startup process is controlled and deliberate. The app.run(debug=True) line activates the Flask application server with debug mode enabled, which is beneficial during development for its detailed error feedback and live reloading capabilities. Here’s the segment:\n# This conditional statement checks if the script is run as the main program.\n# Ensure that code is only executed when the script is run directly,\n#  and not when imported as a module.\nif __name__ == '__main__':\n    # The app.run(debug=True) command starts the Flask application server.\n    # The debug=True argument enables debug mode, which provides useful \n    # feedback in the browser for development,\n    # including detailed tracebacks and live reloading on code changes.\n    app.run(debug=True)"
  },
  {
    "objectID": "posts/Final Project/index.html#evaluative-overview-of-html-templates-implications-for-user-interface-and-experience",
    "href": "posts/Final Project/index.html#evaluative-overview-of-html-templates-implications-for-user-interface-and-experience",
    "title": "Option Pricing with One-dimension Convolutional Neural Network",
    "section": "3.2 Evaluative Overview of HTML Templates: Implications for User Interface and Experience",
    "text": "3.2 Evaluative Overview of HTML Templates: Implications for User Interface and Experience\n\n\n\nOverview of Templates Folder\n\n\nOur web application uses a set of HTML templates, each carefully crafted to improve how users interact with and see data. These templates are essential to the app’s design, blending good looks with practical use. They guide users from the start of entering data to the end of seeing the results. - index.html: Serves as the primary gateway to the web application, designed for predicting call option values. It incorporates Bootstrap for styling, providing a responsive layout with file upload forms, navigation buttons, and result display sections, thus ensuring a user-friendly experience.\n\nplot.html: Utilizes Plotly for dynamic data visualization, offering interactive charts that render complex datasets in an easily digestible format, enhancing the analytical capabilities of the application.\nview_uploaded_data.html: Lists the uploaded database files, enabling users to access and review the data they have provided, fostering transparency and control over the processed information.\nview_data.html: Exhibits the content from two distinct datasets, providing a comprehensive view of the data under analysis and facilitating a deeper understanding of the predictive context.\nshow_prediction.html: Displays the outcome of predictive analyses, guiding users to detailed visual representations and insights derived from their data.\nTest_Set_Predictions.html: Incorporates Plotly JavaScript to generate interactive data visualizations, allowing for an engaging and informative exploration of predictive results within the web environment.\n\nNext, we will conduct a comprehensive analysis of each template’s functionalities and contributions, examining the web pages in our prototype to assess their quality and effectiveness.\n\n3.2.1 Welcome Page\nOur welcome page design seamlessly integrates various elements to optimize user experience and functionality. Users are provided with intuitive \"Browse\" buttons to swiftly upload local CSV files. Upon clicking the \"Upload & Save\" button, evaluation results are promptly generated within a few-second timeframe, ensuring efficiency in data processing. Additionally, users can easily access and review their uploaded dataset (csv files) online by selecting the \"View Data\". For a comprehensive analysis, users can explore interactive prediction graphs through the \"View Prediction\" feature, enhancing their ability to interpret and analyze data trends effectively.\n\n\n\nWelcome Page\n\n\n\nColor Scheme and Aesthetics in index.html\n\nColor Choice: In index.html, we deliberately choose a light blue background color (#ADD8E6) to promote a serene ambiance conducive to focused reading and analysis. The semi-transparent white background (rgba(255, 255, 255, 0.8)) within container elements aims to highlight content while maintaining visual harmony.\n\n&lt;style&gt;\n    body {\n        background-color: #ADD8E6;\n        padding-top: 20px;\n        display: flex;\n        justify-content: center;\n        align-items: center;\n        height: 100vh;\n    }\n    .container {\n        max-width: 800px;\n        text-align: center;\n        background-color: rgba(255, 255, 255, 0.8);\n        padding: 40px;\n        border-radius: 20px;\n    }\n    /* Additional CSS styles are defined in this section */\n&lt;/style&gt;\n\n\nModular Design with Bootstrap in index.html\n\nGrid System Utilization: We utilize Bootstrap’s grid system to create a responsive layout, dividing content into grid columns with classes like container, row, and col-*, ensuring seamless alignment across devices. This fosters a visually appealing and user-friendly design. Here’s how it’s integrated:\n\n&lt;div class=\"container\"&gt;\n    &lt;!-- Content structured using Bootstrap's grid system --&gt;\n&lt;/div&gt;\n\nCustom CSS Classes Integration: Custom CSS classes are integrated to style elements such as buttons, forms, and text, maintaining a cohesive design language and enhancing visual appeal.\n\n.btn-group .btn {\n    margin: 0 10px;\n    font-size: 36px;\n}\n\nResponsive Design Features: Leveraging Bootstrap’s responsive utilities like breakpoints and flexbox classes (d-flex, justify-content-center, align-items-center), we ensure our webpage adapts smoothly to various screen sizes and orientations for optimal user experience.\n\n&lt;div class=\"container\"&gt;\n    &lt;iframe src=\"{{ plot }}\" width=\"100%\" height=\"500px\"&gt;&lt;/iframe&gt;\n&lt;/div&gt;\n\nComponent Reusability: By using Bootstrap’s built-in components such as navigation bars, buttons, and forms, we enhance code modularity and maintain a consistent design language throughout the project, saving time and effort in UI development.\n\n&lt;div class=\"btn-group\" role=\"group\" aria-label=\"Actions\"&gt;\n    &lt;a href=\"{{ url_for('view_data') }}\" class=\"btn btn-info\"&gt;View Data&lt;/a&gt;\n    &lt;a href=\"{{ url_for('show_prediction') }}\" class=\"btn btn-info\"&gt;View Prediction&lt;/a&gt;\n&lt;/div&gt;\n\n\n\n3.2.2 Evaluation & Results Page\nAfter clicking the \"Upload & Save\" button, the client is directed to our Evaluation & Results page, where they can view both numerical results and an interactive graph which provides visual insights, allowing users to explore and analyze the data trends effectively.\n\n\n\nEvaluation & Results Page\n\n\nNumerical Results Display in evaluate_and_visualize.html\n{% if test_loss is defined %}\n    &lt;h2&gt;Evaluation Results:&lt;/h2&gt;\n    &lt;p&gt;Test Loss: {{ test_loss }}&lt;/p&gt;\n    &lt;p&gt;MSE Loss: {{ mse_loss }}&lt;/p&gt;\n    &lt;p&gt;RMSE Loss: {{ rmse_loss }}&lt;/p&gt;\n    &lt;p&gt;R2 Score: {{ r2_score }}&lt;/p&gt;\n    &lt;iframe src=\"{{ plot }}\" width=\"100%\" height=\"500px\"&gt;&lt;/iframe&gt;\n{% endif %}\n\nClarity: The page presents evaluation metrics such as Test Loss, Mean Squared Error (MSE) Loss, Root Mean Squared Error (RMSE) Loss, and R-squared (R2) Score in a clear and structured manner using HTML &lt;p&gt; tags. This clarity enhances the understanding of model performance.\nPrecision: The page ensures the precision and accuracy of displayed numerical values, crucial for data analysis. It typically maintains a precision of at least ten decimal places, ensuring data accuracy for in-depth analysis and comparison.\nContextual Rendering: The use of conditional blocks ensures that numerical results are displayed only when relevant data is available, preventing confusion and presenting information contextually.\n\nEmbedding Plotly Graph in evaluate_and_visualize.html\n&lt;iframe src=\"{{ plot }}\" width=\"100%\" height=\"500px\"&gt;&lt;/iframe&gt;\n\nVisual Analysis: The page enhances data analysis by embedding a Plotly graph within an &lt;iframe&gt;. This interactive graph provides visual insights into trends and patterns, complementing the numerical results.\nInteractivity: Users can interact with the embedded Plotly graph, such as zooming, panning, and hovering over data points to view detailed information. This interactivity fosters a deeper understanding of data trends and anomalies.\nIntegration: The seamless integration of the Plotly graph within the HTML page enhances user experience, allowing for a comprehensive analysis of model performance with both numerical and visual data representations.\n\n\n\n3.2.3 View Data Page\nBy clicking the “View Data” button, users are directed to the data page, where all previously uploaded data files are displayed. Each file is accompanied by a “View” button on the right-hand side, allowing users to view the uploaded data online with a single click.\n\n\n\nView Data Page\n\n\nFile Listing in view_data.html\n&lt;!-- Table body section to display the content. --&gt;\n&lt;tbody&gt;\n    &lt;!-- Loop through each file in the 'files' list passed from the Flask backend. --&gt;\n    {% for file in files %}\n    &lt;tr&gt;\n        &lt;td&gt;{{ file }}&lt;/td&gt; &lt;!-- Displaying the file name. --&gt;\n        &lt;td&gt;&lt;a href=\"{{ url_for('view_data', filename1=file,\n         filename2=file) }}\"&gt;View&lt;/a&gt;&lt;/td&gt; &lt;!-- Link to view data. --&gt;\n    &lt;/tr&gt;\n    {% endfor %}\n&lt;/tbody&gt;\n\nThe “File Name” column displays each uploaded file’s name, fetched from the files list passed from the backend. Within each table row (\n\n), the file name is displayed in the “File Name” column (\n\n{{ file }}\n\n). This ensures that users can easily identify and select the files they want to view.\n\nActionable Links in view_data.html\n&lt;td&gt;&lt;a href=\"{{ url_for('view_data', filename=file) }}\"&gt;View&lt;/a&gt;&lt;/td&gt;\n\nThe “Actions” column provides a convenient “View” link for each file, allowing users to seamlessly navigate to detailed data views with a single click. This intuitive design enhances the overall user experience and promotes efficient data exploration.\nThe “View” links are dynamically generated using Flask’s powerful url_for function, ensuring accurate and reliable navigation to the corresponding data views.\n\n\n\n3.2.4 View Uploaded Data Page\nWhen users click the “View” button on the View Data Page, they are seamlessly navigated to a dynamically generated web page that displays the data they have previously uploaded. This feature provides users with a convenient and intuitive way to access and examine their uploaded datasets within the application, enhancing the overall user experience and facilitating efficient data analysis.\n\n\n\nView Uploaded Data Page\n\n\nDynamic Content Rendering in view_uploaded_data.html\nThe &lt;tbody&gt; section of the table dynamically displays uploaded files. This functionality is achieved through the use of server-side templating, specifically with Jinja2 in Flask, allowing for iteration over a list of files and rendering each as a row in the table.\n\nFunctionality: Iteration over the files array to create a table row for each file.\nData Binding: Server-side rendering with { file } to bind file names directly into the table.\nDynamic URL Generation: The url_for function generates actionable links for each file, enabling user interaction.\n\nHere is the code snippet in view_uploaded_data.html illustrating this functionality:\n&lt;tbody&gt;\n    &lt;!-- Table body section to display the content. --&gt;\n    {% for file in files %}\n    &lt;!-- Loop through each file in the 'files' list\n     passed from the Flask backend. --&gt;\n    &lt;tr&gt;\n        &lt;td&gt;{{ file }}&lt;/td&gt; &lt;!-- Displaying the file name. --&gt;\n        &lt;td&gt;&lt;a href=\"{{ url_for('view_data', \n        filename1=file, \n        filename2=file) }}\"&gt;View&lt;/a&gt;&lt;/td&gt;\n    &lt;/tr&gt;\n    {% endfor %}\n&lt;/tbody&gt;\n\n\n3.2.5 View Prediction Page\nUpon selecting the “View Prediction” button on the initial interface, users are directed to a page displaying test set predictions, featuring an interactive graph created with Plotly. This visualization fosters an engaging user experience, permitting detailed examination of the predictions. The graph’s interactive features—zoom, pan, and data point hover—provide enriched informational access, allowing tailored analytical perspectives.\n\n\n\nView Prediction Page\n\n\nThe graph offers intuitive controls for a seamless user experience. Users can double-click to revert to the original zoom level after examining specific intervals. The operation bar in the upper right corner allows users to download the plot as a PNG, autoscale the axes, and reset the axes effortlessly.\n\n\n\nPrediction Focused on Certain Interval\n\n\n\n\n\nOperation Bar\n\n\nDynamic Plot Rendering with Plotly in plot.html\n&lt;script src=\"https://cdn.plot.ly/plotly-latest.min.js\"&gt;&lt;/script&gt;\n&lt;div id=\"plot\"&gt;&lt;/div&gt;\n&lt;script&gt;\n    var plot_path = \"{{ plot_path }}\";\n    Plotly.d3.html(plot_path, function(error, data) {\n        if (error) {\n            return console.warn(error);\n        }\n        document.getElementById('plot').innerHTML = data;\n    });\n&lt;/script&gt;\n\nIntegration with Plotly: The inclusion of the Plotly library through the &lt;script&gt; tag is a significant aspect of this template.\nAsynchronous Data Fetching: The JavaScript block fetches the plot data asynchronously using Plotly’s d3.html function. This method loads the plot data from a specified path (plot_path), which is dynamically provided by the server-side application. This approach ensures that the webpage remains responsive and that the plot is updated seamlessly without the need for a full page reload.\nDynamic Content Loading: The template utilizes an empty\n\nelement with the id plot, which serves as a placeholder for the graph. The actual content of the graph is loaded dynamically through JavaScript, enabling the webpage to render data-driven plots efficiently.\n\nConditional Rendering and Link Generation in show_prediction.html\n{% if prediction %}\n    &lt;h3&gt;Predicted Mean: {{ prediction }}&lt;/h3&gt;\n    &lt;p&gt;&lt;a href=\"{{ plot }}\" target=\"_blank\"&gt;View Test Set Predictions Plot&lt;/a&gt;&lt;/p&gt;\n{% else %}\n    &lt;p&gt;No prediction available&lt;/p&gt;\n{% endif %}\n&lt;a href=\"{{ url_for('index') }}\" class=\"btn btn-primary\"&gt;Go Back&lt;/a&gt;\n\nConditional Content Display: The template employs Jinja2’s conditional syntax {% if prediction %} to check the presence of a prediction variable. This approach ensures that the user interface adapts to the data context, displaying the prediction results when available. If the prediction variable contains a value, the template renders an &lt;h3&gt; tag showing the predicted mean, enhancing the user’s understanding of the model output.\nDynamic Link Creation: The template dynamically generates a link to a plot (&lt;a href=\"{{ plot }}\" target=\"_blank\"&gt;) when prediction data is available. This link, opened in a new tab (target=“_blank”), leads to a detailed visualization of the test set predictions."
  },
  {
    "objectID": "posts/Final Project/index.html#summary",
    "href": "posts/Final Project/index.html#summary",
    "title": "Option Pricing with One-dimension Convolutional Neural Network",
    "section": "Summary",
    "text": "Summary\nOur web application, through its meticulously designed templates and key features, offers a streamlined and enriched user experience. We are committed to enabling users to efficiently manage, analyze, and interpret their data within a cohesive and intuitive environment. By providing a user-friendly interface and powerful functionality, we sincerely aim to empower users to harness the full potential of their data. Our goal is to support users in making informed decisions and fostering a deeper understanding of their analytical contexts."
  },
  {
    "objectID": "posts/Final Project/index.html#conclusion",
    "href": "posts/Final Project/index.html#conclusion",
    "title": "Option Pricing with One-dimension Convolutional Neural Network",
    "section": "4. Conclusion",
    "text": "4. Conclusion\nBased on above step-by-step instruction and illustrations about how we initiate our project of creating a 1D CNN model for predicting the intrinsic values of options and developing a Flask web application for operationalizing the model, we have successfully demonstrated the process of constructing a robust model for financial-related data analysis and integrating it into a user-friendly web interface. Our project embodies various stages, including data preprocessing, model development, training, evaluation, and web application deployment, thereby showcasing the end-to-end process of building a practical machine learning application.\nRegarding the ethical ramifications of our project, we emphasize that due to the limitations of data and project complexity, this webapp should NEVER be used for ALL kinds of profit-making activities such as investment, but is only suitable for displaying the complexity and insights of option pricing related data. We adhere to ethical guidelines and promote responsible data handling throughout the development process. We respect copyright and ensure that the data used for the creation of WebApps comes from authoritative websites. We consider data privacy, transparency, and ensuring that users of our webapp have control over their data, understand how it is used, and know the entire process of our webapp creation.\nHope it shows some insights of option pricing, and thanks for reading!"
  },
  {
    "objectID": "posts/Hw1/index.html",
    "href": "posts/Hw1/index.html",
    "title": "Building Global Temperature Database and Use Ploty Express to Make Interesting Figures",
    "section": "",
    "text": "import sqlite3\nimport numpy as np\nimport pandas as pd\nfrom plotly import express as px\nimport plotly.io as pio\nfrom scipy import stats\nimport calendar\npio.renderers.default=\"iframe\""
  },
  {
    "objectID": "posts/Hw1/index.html#importing-the-necessary-modules.",
    "href": "posts/Hw1/index.html#importing-the-necessary-modules.",
    "title": "Building Global Temperature Database and Use Ploty Express to Make Interesting Figures",
    "section": "",
    "text": "import sqlite3\nimport numpy as np\nimport pandas as pd\nfrom plotly import express as px\nimport plotly.io as pio\nfrom scipy import stats\nimport calendar\npio.renderers.default=\"iframe\""
  },
  {
    "objectID": "posts/Hw1/index.html#creating-a-database",
    "href": "posts/Hw1/index.html#creating-a-database",
    "title": "Building Global Temperature Database and Use Ploty Express to Make Interesting Figures",
    "section": "Creating a Database",
    "text": "Creating a Database\nFrom the file temps.csv, station-metadata.csv, fips-10-4-to-iso-country-codes.csv, we can build a database with three such tables. From the class we did a little data wrangling to split the month variable for temps csv, and I am going to do the same.\nI will first work on the temperature table. By using chunksize argument to make temps.csv into managable chunks and use prepare_df function to generate month column based on original “valuex” columns and divide temperature by 100 to get the Celsius for temperature.\n\nconn = sqlite3.connect(\"temps.db\") # create a database in current directory called temps.db\n\n\ndf_iter = pd.read_csv(\"temps.csv\", chunksize = 100000)\ndef prepare_df(df):\n    \"\"\"\n    Using prepare_df to change all the valuex to a month column using stack and setindex function. \n    Finnally change the temperature to celsius degree.\n    \"\"\"\n    df = df.set_index(keys=[\"ID\", \"Year\"])\n    df = df.stack()\n    df = df.reset_index()\n    df = df.rename(columns = {\"level_2\"  : \"Month\" , 0 : \"Temp\"})\n    df[\"Month\"] = df[\"Month\"].str[5:].astype(int)\n    df[\"Temp\"]  = df[\"Temp\"] / 100\n    return(df)\n\n\nfor i, df in enumerate(df_iter):\n    df = prepare_df(df)\n    df.to_sql(\"temperatures\", conn, if_exists = \"replace\" if i == 0 else \"append\", index = False)\n\nThen, using read_csv function to load both csv as tables into the temps.db database.\n\nstations = pd.read_csv(\"station-metadata.csv\")\nstations.to_sql(\"stations\", conn, if_exists = \"replace\", index=False)  # stations.csv to a table in database\ncountries = pd.read_csv(\"fips-10-4-to-iso-country-codes.csv\")\ncountries.to_sql(\"countries\", conn, if_exists = \"replace\", index=False)  # countries.csv to a table in database\n\n279\n\n\nCheck that there are three tables and its columns and close the connection.\n\ncursor = conn.cursor()\ncursor.execute(\"SELECT name FROM sqlite_master WHERE type = 'table'\")\nprint(cursor.fetchall())\n\n[('temperatures',), ('stations',), ('countries',)]\n\n\n\ncursor.execute(\"SELECT sql FROM sqlite_master WHERE type='table';\")  # use sqlite master to show all the tables and columns in databse\nfor result in cursor.fetchall():\n    print(result[0])\n\nCREATE TABLE \"temperatures\" (\n\"ID\" TEXT,\n  \"Year\" INTEGER,\n  \"Month\" INTEGER,\n  \"Temp\" REAL\n)\nCREATE TABLE \"stations\" (\n\"ID\" TEXT,\n  \"LATITUDE\" REAL,\n  \"LONGITUDE\" REAL,\n  \"STNELEV\" REAL,\n  \"NAME\" TEXT\n)\nCREATE TABLE \"countries\" (\n\"FIPS 10-4\" TEXT,\n  \"ISO 3166\" TEXT,\n  \"Name\" TEXT\n)\n\n\n\nconn.close()"
  },
  {
    "objectID": "posts/Hw1/index.html#write-a-query-function",
    "href": "posts/Hw1/index.html#write-a-query-function",
    "title": "Building Global Temperature Database and Use Ploty Express to Make Interesting Figures",
    "section": "2. Write a Query Function",
    "text": "2. Write a Query Function\nImport query_climate_database from climate_database.py which I used sql manipulation to produce panda dataframe needed for this task. Since the first two letters of country id in the station table is coresponding to FIPS 10-4 columns for countries table, we can use JOIN and SUBSTRING function to help us to display the full country name.\n\nfrom climate_database import query_climate_database\nimport inspect\nprint(inspect.getsource(query_climate_database))  # import query function and inspect it\n\ndef query_climate_database(db_file, country, year_begin, year_end, month):\n    \"\"\"\n    This is a function that uses database file and corresponding criteria to filter out \n    and output a dataframe with corresponding requirements using sql query\n    \"\"\"\n    conn = sqlite3.connect(db_file)\n    cmd = \\\n        '''SELECT S.NAME , S.LATITUDE, S.LONGITUDE, C.Name, T.Year, T.Month, T.Temp\n        FROM stations S\n        LEFT JOIN temperatures T on S.id = T.id\n        LEFT JOIN countries C on C.[FIPS 10-4]= SUBSTRING(S.id, 1, 2)\n        WHERE C.NAME = {country} AND T.year &gt;= {year_begin} AND T.year &lt;= {year_end} AND T.month = {month}\n        ''' .format(country = repr(country), year_begin = year_begin, year_end = year_end, month = month)\n    result_df = pd.read_sql_query(cmd, conn)\n    conn.close()\n    result_df = result_df.rename(columns={'Name' : 'Country'})\n    return result_df\n\n\n\n\nquery_climate_database(db_file = \"temps.db\",  # test for India\n                       country = \"India\", \n                       year_begin = 1980, \n                       year_end = 2020,  \n                       month = 1)\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nCountry\nYear\nMonth\nTemp\n\n\n\n\n0\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1980\n1\n23.48\n\n\n1\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1981\n1\n24.57\n\n\n2\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1982\n1\n24.19\n\n\n3\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1983\n1\n23.51\n\n\n4\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1984\n1\n24.81\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3147\nDARJEELING\n27.050\n88.270\nIndia\n1983\n1\n5.10\n\n\n3148\nDARJEELING\n27.050\n88.270\nIndia\n1986\n1\n6.90\n\n\n3149\nDARJEELING\n27.050\n88.270\nIndia\n1994\n1\n8.10\n\n\n3150\nDARJEELING\n27.050\n88.270\nIndia\n1995\n1\n5.60\n\n\n3151\nDARJEELING\n27.050\n88.270\nIndia\n1997\n1\n5.70\n\n\n\n\n3152 rows × 7 columns"
  },
  {
    "objectID": "posts/Hw1/index.html#write-a-geographic-scatter-function-for-yearly-temperature-increases",
    "href": "posts/Hw1/index.html#write-a-geographic-scatter-function-for-yearly-temperature-increases",
    "title": "Building Global Temperature Database and Use Ploty Express to Make Interesting Figures",
    "section": "3. Write a Geographic Scatter Function for Yearly Temperature Increases",
    "text": "3. Write a Geographic Scatter Function for Yearly Temperature Increases\nWith LinearRegression from sklearn, we can get a coeficient of temperature function based on the data.\n\nfrom sklearn.linear_model import LinearRegression\n\ndef coef(data_group):\n    \"\"\"\n    This is a function that intake a datagroup with year and temp to output its linear regression's coefficient alpha. \n    \"\"\"\n    x = data_group[[\"Year\"]]  # 2 brackets because X should be a df\n    y = data_group[\"Temp\"]    # 1 bracket because y should be a series\n    LR = LinearRegression()\n    LR.fit(x, y)\n    return round(LR.coef_[0], 4)  # round it to force decimal \n\n\ndef temperature_coefficient_plot(db_file, country, year_begin, year_end, month, min_obs, **kwargs):\n    \"\"\"\n    This is a function that take database and its corresponding requirements to output a temperature increase coefficient plot on Geo mapbox. \n    \"\"\"\n    #  using query_climate_database to get our temperature dataframe\n    df = query_climate_database(db_file, country, year_begin, year_end, month)\n    #  using a simple for loop to check how many observance made by each unique stations and drop based on min_obs\n    obs = []\n    for station in df['NAME'].unique():\n        obs.append([station, df.isin([station]).sum()['NAME']])\n    obs = pd.DataFrame(obs)\n    df = pd.merge(df, obs, left_on = \"NAME\", right_on = 0).drop(columns = [0])\n    df = df[df[1]&gt;min_obs].drop(columns = [1])\n    #  using coef to take the first coefficient of linear regression for our data and round it to the forth decimal place. \n    Estimated = pd.DataFrame(df.groupby('NAME').apply(coef))\n    #  Then merge it into the original dataframe\n    df = pd.merge(df, Estimated, on = 'NAME')\n    df = df.rename({0 : 'Estimated Yearly Increase (°C)'}, axis=1)\n    #  draw the graph, using calendar.month_name to convert month number to month name. \n    title = f'Estimate of yearly increase in temperatures in {calendar.month_name[month]} for stations in {country}, year {year_begin}-{year_end}'\n    fig = px.scatter_mapbox(df,\n                        lat = \"LATITUDE\",\n                        lon = \"LONGITUDE\",\n                        hover_name = \"NAME\",\n                        color = \"Estimated Yearly Increase (°C)\",\n                        title = title,\n                        **kwargs)\n    return fig\n\n\ncolor_map = px.colors.diverging.RdGy_r\nfig = temperature_coefficient_plot(\"temps.db\", \"India\", 1980, 2020, 1,  # this is the test graph provided in the assignment\n                                   min_obs = 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\nfig.show()\n\n\n\n\n\ncolor_map = px.colors.diverging.RdGy_r\nfig = temperature_coefficient_plot(\"temps.db\", \"United Kingdom\", 1980, 2020, 1,  # this is the test graph provided in the assignment\n                                   min_obs = 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\nfig.show()"
  },
  {
    "objectID": "posts/Hw1/index.html#create-two-more-interesting-figures",
    "href": "posts/Hw1/index.html#create-two-more-interesting-figures",
    "title": "Building Global Temperature Database and Use Ploty Express to Make Interesting Figures",
    "section": "4. Create Two More Interesting Figures",
    "text": "4. Create Two More Interesting Figures\n\n(a) Writing one more query functon\nThis is a more convenient query function to help us fetch dataframe from the database with inclusion of month_begin and month_end without limiting to a single month like the previous function.\n\nfrom climate_database import query\nimport inspect\nprint(inspect.getsource(query))\n\ndef query(db_file, country, year_begin, year_end, month_begin, month_end):\n    \"\"\"\n    This is a function that uses database file and corresponding criteria to filter out \n    and output a dataframe with corresponding requirements using sql query\n    \"\"\"\n    conn = sqlite3.connect(db_file)\n    cmd = \\\n        '''SELECT S.NAME , S.LATITUDE, S.LONGITUDE, C.Name, T.Year, T.Month, T.Temp\n        FROM stations S\n        LEFT JOIN temperatures T on S.id = T.id\n        LEFT JOIN countries C on C.[ISO 3166]= SUBSTRING(S.id, 1, 2)\n        WHERE C.NAME = {country} AND T.year &gt;= {year_begin} AND T.year &lt;= {year_end} AND T.month &gt;= {month_begin} AND T.month &lt;= {month_end}\n        ''' .format(country = repr(country), year_begin = year_begin, year_end = year_end, month_begin = month_begin, month_end = month_end)\n    result_df = pd.read_sql_query(cmd, conn)\n    conn.close()\n    return result_df\n\n\n\n\nquery(db_file = \"temps.db\", \n                country = \"India\",     \n                year_begin = 1980, \n                year_end = 2020,\n                month_begin = 1,\n                month_end = 12)\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nName\nYear\nMonth\nTemp\n\n\n\n\n0\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1980\n1\n23.48\n\n\n1\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1980\n2\n27.16\n\n\n2\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1980\n3\n30.07\n\n\n3\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1980\n4\n32.39\n\n\n4\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1980\n5\n33.04\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n37671\nDARJEELING\n27.050\n88.270\nIndia\n1996\n11\n11.90\n\n\n37672\nDARJEELING\n27.050\n88.270\nIndia\n1996\n12\n8.80\n\n\n37673\nDARJEELING\n27.050\n88.270\nIndia\n1997\n1\n5.70\n\n\n37674\nDARJEELING\n27.050\n88.270\nIndia\n1997\n2\n4.90\n\n\n37675\nDARJEELING\n27.050\n88.270\nIndia\n1997\n3\n10.70\n\n\n\n\n37676 rows × 7 columns\n\n\n\n\n\n(b) Creating the Figures\nThis is a plot of average temperature in a country for multiple years for different months with px.bar function.\n\ndef temp_in_a_year(db_file, country, year1, year2, month_begin, month_end, **kwargs):\n    \"\"\"\n    With our query function we can get any countries year month data from the database. \n    Using year as facet col to show multipe year's plot in one graph\n    \"\"\"\n    df = query(db_file, country, year1, year2, month_begin, month_end)\n    title = f'Average temperature from {calendar.month_name[month_begin]} to {calendar.month_name[month_end]} for stations in {country}, year {year1} to {year2}'\n    new_df = pd.DataFrame(df.groupby(['Year', 'Month'])['Temp'].apply(np.average)).reset_index()\n    fig = px.bar(new_df, x=\"Month\", y =\"Temp\", title = title, facet_col = \"Year\", facet_col_wrap=4, **kwargs)\n    return fig\n\n\nfig = temp_in_a_year(\"temps.db\", \"India\", 2002, 2012, 1, 12)\nfig.show()\n\n\n\n\n\nfig = temp_in_a_year(\"temps.db\", \"United States\", 1999, 2002, 1, 12)\nfig.show()\n\n\n\n\nWe can also show a graph of temperature of same month over the years for a specific country with px.line figures which is more suited for over the year changes.\n\ndef temp_in_a_month(db_file, country, year_begin, year_end, month, **kwargs):\n    \"\"\"\n    Using query function to take a single months data for multiple years\n    Using px.line to graph this\n    \"\"\"\n    df = query(db_file, country, year_begin, year_end, month, month)\n    title = f'Average temperature in {calendar.month_name[month]} for stations in {country}, year {year_begin} to {year_end}'\n    new_df = pd.DataFrame(df.groupby('Year')['Temp'].apply(np.average))\n    fig = px.line(new_df, title = title, **kwargs)\n    return fig\n\n\nfig = temp_in_a_month(\"temps.db\", \"India\", 1999, 2010, 12)\nfig.show()\n\n\n\n\n\nfig = temp_in_a_month(\"temps.db\", \"United States\", 1980, 2010, 12)\nfig.show()"
  },
  {
    "objectID": "posts/Hw3/index.html",
    "href": "posts/Hw3/index.html",
    "title": "Web Development with Flask - A Simple Implementation of Online Message Bank",
    "section": "",
    "text": "In this blog post, I’m going to make a super cool simple online message bank that you can develop yourself with flask. Here’s a link to my project repository which is built upon this example."
  },
  {
    "objectID": "posts/Hw3/index.html#setups-with-terminal-command",
    "href": "posts/Hw3/index.html#setups-with-terminal-command",
    "title": "Web Development with Flask - A Simple Implementation of Online Message Bank",
    "section": "1. Setups with Terminal Command",
    "text": "1. Setups with Terminal Command\nAfter cloning the repository, you can run this page on your local machine with flask run."
  },
  {
    "objectID": "posts/Hw3/index.html#explaining-functions-in-app.py",
    "href": "posts/Hw3/index.html#explaining-functions-in-app.py",
    "title": "Web Development with Flask - A Simple Implementation of Online Message Bank",
    "section": "2. Explaining Functions in app.py",
    "text": "2. Explaining Functions in app.py\n\n(1) get_message_db()\ndef get_message_db():\n  # Use create table if not exists to create a message_db with handle and message for text. \n  try:\n      return g.message_db\n  except:\n      g.message_db = sqlite3.connect(\"messages_db.sqlite\")\n      cmd = 'CREATE TABLE IF NOT EXISTS messages_table (handle TEXT, message TEXT)' \n      cursor = g.message_db.cursor()\n      cursor.execute(cmd)\n      return g.message_db\nThis is the first function we use to check and create a table called messages_table in our database file. With the sql command CREATE TABLE IF NOT EXISTS table (column1, column2), we can create a table in our database if it doesn’t exists. This table includes two text columns with handle and message.\n\n\n(2) basic route defintions\n@app.route('/')\ndef main():\n    return render_template('hello.html')\n\n@app.route('/hello')\ndef hello():\n    return render_template('hello.html')\nmain() and hello() are the most basic functions that call render_template with our html files that we have written.\n\n\n(3) insert_message(request)\ndef insert_message(request):\n    \"\"\"\n    use sql command insert into table .. values .. to insert input into the database. \n    \"\"\"\n    cmd = f'INSERT INTO messages_table (handle, message) VALUES (\"{request.form[\"handle\"]}\", \"{request.form[\"message\"]}\")'\n    cursor = g.message_db.cursor()\n    cursor.execute(cmd)\n    g.message_db.commit()\n    pass\nThis function is used to insert values into our table in the database. The values from the website will form a request object that can be used as input of this function. Then, we can use sql command INSERT INTO table (column) VALUES (value) to insert both handle and message into two corresponding columns in the table.\n\n\n(4) submit()\n@app.route('/submit', methods=['POST', 'GET'])\ndef submit():\n    get_message_db()\n    if request.method == 'GET':\n        return render_template('submit.html')     \n    else:\n        # split successful and error case for inputs. \n        insert_message(request)\n        try: \n            insert_message(request)\n            return render_template('submit.html', thanks = True, handle = request.form['handle'])\n        except:\n            return render_template('submit.html', error = True)\nWith method 'POST' and 'GET', we can split two version of returned webpage. Then, after successfully running insert(request) for this page, we can render template with thanks or error.\n\n\n(5) random_message(n)\ndef random_messages(n): \n    # fetch n elements from the table with random() \n    cmd = f'SELECT * FROM messages_table ORDER BY RANDOM() LIMIT {n};'\n    cursor = get_message_db().cursor()\n    result = cursor.execute(cmd).fetchall()\n    return result\nIn this function, we can fetch n ammounts of random entries in our database’s table with the SELECT * FROM table ORDER BY RANDOM LIMIT n command. Then, this function return a list with two columns and n rows.\n\n\n(6) view()\n@app.route('/view')\ndef view():\n    result = random_messages(5)\n    return render_template('view.html', result = result)\nThis function will call random_messages(5) to return 5 rows of handle and message. Then, we feed this list into view.html as result function, and render it. Then, we will cover how to implement function arguments in the html files."
  },
  {
    "objectID": "posts/Hw3/index.html#explaining-html-template-files",
    "href": "posts/Hw3/index.html#explaining-html-template-files",
    "title": "Web Development with Flask - A Simple Implementation of Online Message Bank",
    "section": "3. Explaining HTML Template Files",
    "text": "3. Explaining HTML Template Files\n{% extends 'base.html' %}\n\n{% block header %}\n  &lt;h1&gt;{% block title %}View Messages{% endblock %}&lt;/h1&gt;\n{% endblock %}\n\n{% block content %}\n  &lt;p&gt;\n    Displaying up to five random messages from past submissions! \n    &lt;br&gt;\n    {% for entry in result %} \n    &lt;message&gt;{{entry[1]}} &lt;/message&gt;\n                               - &lt;i&gt;{{entry[0]}}&lt;/i&gt;\n    &lt;br&gt;\n  {% endfor %}\n  &lt;/p&gt;\n{% endblock %}\nThis is the view.html template. With base.html on top of this webpage, &lt;h1&gt; is a type of text format used for header. Then, we use &lt;p&gt; text format for the remainder of the text. Then, we use for loop function in jinja to display each random entries. I have also made a text format called &lt;message&gt; to differentiate message and handle, and I also italized handle with &lt;i&gt;.\n\n4. Implementation of the Website\nWe can input our name and message into the website in the submit page.   \nThen, we can view a bunch of previous messages that is in the database in the view page."
  },
  {
    "objectID": "posts/Hw5/index.html",
    "href": "posts/Hw5/index.html",
    "title": "Image Classification with Keras",
    "section": "",
    "text": "In this blog post, I’m going to make a super cool machine learning example for image classification. First, we need to set up our package and environment. In this example, we will use keras and a dataset from tensor flow dataset called cats vs dogs which we need to classify the two animals based on pictures.\nimport os\nos.environ['KERAS_BACKEND'] = \"tensorflow\"  # actually tensorflow is the default backend. \n\nfrom keras import utils\nimport tensorflow_datasets as tfds\nimport matplotlib.pyplot as plt\nimport keras\nfrom keras import layers\nfrom random import randint\nIn this step, we need to split train test and validation data.\ntrain_ds, validation_ds, test_ds = tfds.load(\n    \"cats_vs_dogs\",\n    # 40% for training, 10% for validation, and 10% for test (the rest unused)\n    split=[\"train[:40%]\", \"train[40%:50%]\", \"train[50%:60%]\"],\n    as_supervised=True,  # Include labels\n)\n\nprint(f\"Number of training samples: {train_ds.cardinality()}\")\nprint(f\"Number of validation samples: {validation_ds.cardinality()}\")\nprint(f\"Number of test samples: {test_ds.cardinality()}\")\n\nNumber of training samples: 9305\nNumber of validation samples: 2326\nNumber of test samples: 2326\nIn this step, we are resizing all the pictures into 150x150, and batch our datasets with batch(batch_size), prefetch(tf_data.AUTOTUNE) and cache() to reduce our run time.\nresize_fn = keras.layers.Resizing(150, 150)\n\ntrain_ds = train_ds.map(lambda x, y: (resize_fn(x), y))\nvalidation_ds = validation_ds.map(lambda x, y: (resize_fn(x), y))\ntest_ds = test_ds.map(lambda x, y: (resize_fn(x), y))\nfrom tensorflow import data as tf_data\nbatch_size = 64\n\ntrain_ds = train_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\nvalidation_ds = validation_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\ntest_ds = test_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\nThen, we define a show(ds) function that takes our datasets to show three random dog and cats.\nclass_names = ['cat', 'dog']\ndef show(ds):\n  \"\"\"\n  imput a batch of cat and dog pictures, and show three random cat and dogs\n  utilizing randint(0, 63) to get a random picture everytime\n  \"\"\"\n  plt.figure(figsize=(10, 10))\n  for images, labels in ds.take(1):  # take(1) will get a single picture\n    count = 0\n    j = 0\n    while j &lt; 3:\n      i = randint(0, 63)\n      if labels[i] == 0:             # make sure we get a cat to display\n        ax = plt.subplot(2, 3, j+1)\n        plt.imshow(images[i].numpy().astype(\"uint8\"))\n        plt.title(class_names[labels[i]])\n        j += 1                        # add one iteration to make sure we only display three pictures for each\n      else:\n        continue\n      count = 0\n    j = 0\n    while j &lt; 3:\n      i = randint(0, 63)\n      if labels[i] == 1:              # make sure we get a dog to display\n        ax = plt.subplot(2, 3, j+4)\n        plt.imshow(images[i].numpy().astype(\"uint8\"))\n        plt.title(class_names[labels[i]])\n        j += 1                         # add one iteration to make sure we only display three pictures for each\n      else:\n        continue\nshow(train_ds)\n\n2024-03-02 19:48:39.796322: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\nWe utilize as_numpy_iterator() and for loop to output the number of cats and dogs to calculate a baseline accuracy based on proportion.\nlabels_iterator = train_ds.unbatch().map(lambda image, label: label).as_numpy_iterator()\ncat = 0\ndog = 0\ntest_labels = []\ntest_images = []\nfor i in labels_iterator:   # with each iteration to take one label and add counts to each number\n    test_labels.append(i)\n    if i == 0:\n        cat += 1\n    else:\n        dog += 1\nprint(cat, dog)\n\n4637 4668\nprint(\"baseline accuracy is\", dog/(cat+dog))    # our default accuracy is the more labels counts divide by all label counts\n\nbaseline accuracy is 0.5016657710908113"
  },
  {
    "objectID": "posts/Hw5/index.html#basic-model-1",
    "href": "posts/Hw5/index.html#basic-model-1",
    "title": "Image Classification with Keras",
    "section": "Basic Model 1",
    "text": "Basic Model 1\nFirst, we use Dropout to drop 0.2 amount of data everytime before training for randomized event. Then, we can utilize Conv2D and MaxPooling2D layers to reduce the size of features for learning. Afterwards, we Flatten and Dense our features to the size of number of class.\n\nmodel1 = keras.models.Sequential([\n    layers.Input((150, 150, 3)),        # input 150, 150, 3\n    layers.Dropout(0.2),                # drop random 20% to randomize each training\n    layers.Conv2D(64, (3, 3), activation='relu'),      # conv2d to reduce amount of features\n    layers.MaxPooling2D((2, 2)),                       # condense amount of features\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.Flatten(),                                  # flat all features into 1d\n    layers.Dense(64, activation='relu'),               # dense features before the final dense\n    layers.Dense(2) # number of classes\n])\nmodel1.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dropout (Dropout)           (None, 150, 150, 3)       0         \n                                                                 \n conv2d (Conv2D)             (None, 148, 148, 64)      1792      \n                                                                 \n max_pooling2d (MaxPooling2  (None, 74, 74, 64)        0         \n D)                                                              \n                                                                 \n conv2d_1 (Conv2D)           (None, 72, 72, 64)        36928     \n                                                                 \n max_pooling2d_1 (MaxPoolin  (None, 36, 36, 64)        0         \n g2D)                                                            \n                                                                 \n conv2d_2 (Conv2D)           (None, 34, 34, 64)        36928     \n                                                                 \n flatten (Flatten)           (None, 73984)             0         \n                                                                 \n dense (Dense)               (None, 64)                4735040   \n                                                                 \n dense_1 (Dense)             (None, 2)                 130       \n                                                                 \n=================================================================\nTotal params: 4810818 (18.35 MB)\nTrainable params: 4810818 (18.35 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\nmodel1.compile(optimizer='adam',\n              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\nhistory1 = model1.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 11s 63ms/step - accuracy: 0.5159 - loss: 76.1365 - val_accuracy: 0.5696 - val_loss: 0.6758\nEpoch 2/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 53ms/step - accuracy: 0.5825 - loss: 0.6631 - val_accuracy: 0.5684 - val_loss: 0.6837\nEpoch 3/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 54ms/step - accuracy: 0.6374 - loss: 0.6212 - val_accuracy: 0.5619 - val_loss: 0.6831\nEpoch 4/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 54ms/step - accuracy: 0.6731 - loss: 0.5693 - val_accuracy: 0.5623 - val_loss: 0.7289\nEpoch 5/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 11s 74ms/step - accuracy: 0.6965 - loss: 0.5291 - val_accuracy: 0.5585 - val_loss: 0.7798\nEpoch 6/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 11s 74ms/step - accuracy: 0.7303 - loss: 0.4808 - val_accuracy: 0.5383 - val_loss: 0.8293\nEpoch 7/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 54ms/step - accuracy: 0.6968 - loss: 0.5329 - val_accuracy: 0.5494 - val_loss: 0.8959\nEpoch 8/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 54ms/step - accuracy: 0.7210 - loss: 0.5069 - val_accuracy: 0.5482 - val_loss: 0.8249\nEpoch 9/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 11s 74ms/step - accuracy: 0.7247 - loss: 0.5064 - val_accuracy: 0.5370 - val_loss: 1.0278\nEpoch 10/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 53ms/step - accuracy: 0.7753 - loss: 0.4601 - val_accuracy: 0.5696 - val_loss: 1.0642\nEpoch 11/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 54ms/step - accuracy: 0.8312 - loss: 0.3351 - val_accuracy: 0.5327 - val_loss: 1.2938\nEpoch 12/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 53ms/step - accuracy: 0.8702 - loss: 0.2647 - val_accuracy: 0.5464 - val_loss: 1.2194\nEpoch 13/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 11s 74ms/step - accuracy: 0.8606 - loss: 0.2829 - val_accuracy: 0.5473 - val_loss: 1.3734\nEpoch 14/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 55ms/step - accuracy: 0.8708 - loss: 0.2841 - val_accuracy: 0.5443 - val_loss: 1.4037\nEpoch 15/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 11s 74ms/step - accuracy: 0.8942 - loss: 0.2424 - val_accuracy: 0.5460 - val_loss: 1.5973\nEpoch 16/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 11s 74ms/step - accuracy: 0.9099 - loss: 0.2066 - val_accuracy: 0.5615 - val_loss: 1.5195\nEpoch 17/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 53ms/step - accuracy: 0.9230 - loss: 0.1856 - val_accuracy: 0.5709 - val_loss: 1.5209\nEpoch 18/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 11s 74ms/step - accuracy: 0.9338 - loss: 0.1692 - val_accuracy: 0.5636 - val_loss: 1.6108\nEpoch 19/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 53ms/step - accuracy: 0.9346 - loss: 0.1631 - val_accuracy: 0.5658 - val_loss: 1.3993\nEpoch 20/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 53ms/step - accuracy: 0.9485 - loss: 0.1382 - val_accuracy: 0.5731 - val_loss: 1.5646\n\n\nWe utilizing plot to show us accuracy and loss for training and validation graph.\n\ndef plot_history(history):\n    acc = history.history['accuracy']\n    val_acc = history.history['val_accuracy']\n\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n\n    plt.figure(figsize=(8, 8))\n    plt.subplot(2, 1, 1)\n    plt.plot(acc, label='Training Accuracy')\n    plt.plot(val_acc, label='Validation Accuracy')\n    plt.legend(loc='lower right')\n    plt.ylabel('Accuracy')\n    plt.ylim([min(plt.ylim()),1])\n    plt.title('Training and Validation Accuracy')\n\n    plt.subplot(2, 1, 2)\n    plt.plot(loss, label='Training Loss')\n    plt.plot(val_loss, label='Validation Loss')\n    plt.legend(loc='upper right')\n    plt.ylabel('Cross Entropy')\n    plt.ylim([0,1.0])\n    plt.title('Training and Validation Loss')\n    plt.xlabel('epoch')\n    plt.show()\n\n\nplot_history(history1)\n\n\n\n\n\n\n\n\n\nThe accuracy of model1 stabilized between 55% and 60% during training. \nCompare that to the baseline, this model is 5-10 percent better. \nI observe a little overfitting at the end since the val accuracy dropped a little."
  },
  {
    "objectID": "posts/Hw5/index.html#model-2",
    "href": "posts/Hw5/index.html#model-2",
    "title": "Image Classification with Keras",
    "section": "3. Model 2",
    "text": "3. Model 2\nWe can add Randomflip and RandomRotation after input, I use horizontal flip since utilizing vertical flip will yield worse result and also use random rotation factor with 0.1 pi.  Let’s see an example before implement it into model2.\n\nplt.figure(figsize=(12, 12))\nfor images, labels in train_ds.take(1):\n    ax = plt.subplot(2, 3, 1)\n    plt.imshow(images[3].numpy().astype(\"uint8\"))\n    plt.title('Original')\nlayer1 = layers.RandomFlip()                    # random flip layer\nlayer2 = layers.RandomRotation(factor =  0.1)   # random rotaion layer 1\nlayer3 = layers.RandomRotation(factor =  0.3)   # random rotation layer 2\n # we then output pictures through these layer for image[3] a cute cat picture\nax = plt.subplot(2, 3, 2)\nplt.imshow(layer1(images[3].numpy().astype(\"uint8\")).numpy().astype(\"uint8\"))\nplt.title('RandomFlip1')\nax = plt.subplot(2, 3, 3)\nplt.imshow(layer1(images[3].numpy().astype(\"uint8\")).numpy().astype(\"uint8\"))\nplt.title('RandomFlip2')\nax = plt.subplot(2, 3, 4)\nplt.imshow(layer2(images[3].numpy().astype(\"uint8\")).numpy().astype(\"uint8\"))\nplt.title('RandomRotation1')\nax = plt.subplot(2, 3, 5)\nplt.imshow(layer2(images[3].numpy().astype(\"uint8\")).numpy().astype(\"uint8\"))\nplt.title('RandomRotation2')\nax = plt.subplot(2, 3, 6)\nplt.imshow(layer3(images[3].numpy().astype(\"uint8\")).numpy().astype(\"uint8\"))\nplt.title('RandomRotation3')\n\nText(0.5, 1.0, 'RandomRotation3')\n\n\n\n\n\n\n\n\n\nThen, we can implement it into the model’s layer.\n\nmodel2 = keras.models.Sequential([\n    layers.Input((150, 150, 3)),\n    layers.RandomFlip(mode = 'horizontal'),     # flip random images for horizontal direction\n    layers.RandomRotation(factor =  0.1),       # rotate random image for (-0.1pi) or (0.1pi)\n    layers.Dropout(0.2),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.Flatten(),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(2) # number of classes\n])\nmodel2.summary()\n\nModel: \"sequential_7\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ random_flip_3 (RandomFlip)           │ (None, 150, 150, 3)         │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ random_rotation_3 (RandomRotation)   │ (None, 150, 150, 3)         │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_7 (Dropout)                  │ (None, 150, 150, 3)         │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_21 (Conv2D)                   │ (None, 148, 148, 64)        │           1,792 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_14 (MaxPooling2D)      │ (None, 74, 74, 64)          │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_22 (Conv2D)                   │ (None, 72, 72, 64)          │          36,928 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_15 (MaxPooling2D)      │ (None, 36, 36, 64)          │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_23 (Conv2D)                   │ (None, 34, 34, 64)          │          36,928 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ flatten_7 (Flatten)                  │ (None, 73984)               │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_14 (Dense)                     │ (None, 64)                  │       4,735,040 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_15 (Dense)                     │ (None, 2)                   │             130 │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 4,810,818 (18.35 MB)\n\n\n\n Trainable params: 4,810,818 (18.35 MB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\nmodel2.compile(optimizer='adam',\n              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\nhistory2 = model2.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 290ms/step - accuracy: 0.5360 - loss: 72.9558 - val_accuracy: 0.5400 - val_loss: 0.6841\nEpoch 2/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 43s 292ms/step - accuracy: 0.5446 - loss: 0.6838 - val_accuracy: 0.5460 - val_loss: 0.6807\nEpoch 3/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 35s 237ms/step - accuracy: 0.5495 - loss: 0.6803 - val_accuracy: 0.5193 - val_loss: 0.6901\nEpoch 4/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 290ms/step - accuracy: 0.5389 - loss: 0.6837 - val_accuracy: 0.5211 - val_loss: 0.6901\nEpoch 5/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 44s 300ms/step - accuracy: 0.5454 - loss: 0.6851 - val_accuracy: 0.5374 - val_loss: 0.6809\nEpoch 6/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 290ms/step - accuracy: 0.5509 - loss: 0.6786 - val_accuracy: 0.5447 - val_loss: 0.6795\nEpoch 7/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 290ms/step - accuracy: 0.5447 - loss: 0.6778 - val_accuracy: 0.5318 - val_loss: 0.6878\nEpoch 8/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 35s 238ms/step - accuracy: 0.5496 - loss: 0.6792 - val_accuracy: 0.5464 - val_loss: 0.6788\nEpoch 9/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 35s 238ms/step - accuracy: 0.5655 - loss: 0.6768 - val_accuracy: 0.5284 - val_loss: 0.6847\nEpoch 10/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 35s 236ms/step - accuracy: 0.5700 - loss: 0.6767 - val_accuracy: 0.5365 - val_loss: 0.6845\nEpoch 11/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 290ms/step - accuracy: 0.5496 - loss: 0.6767 - val_accuracy: 0.5520 - val_loss: 0.6802\nEpoch 12/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 35s 239ms/step - accuracy: 0.5675 - loss: 0.6766 - val_accuracy: 0.5632 - val_loss: 0.6769\nEpoch 13/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 35s 239ms/step - accuracy: 0.5655 - loss: 0.6693 - val_accuracy: 0.5615 - val_loss: 0.6774\nEpoch 14/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 34s 236ms/step - accuracy: 0.5613 - loss: 0.6718 - val_accuracy: 0.5567 - val_loss: 0.6768\nEpoch 15/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 35s 236ms/step - accuracy: 0.5574 - loss: 0.6788 - val_accuracy: 0.5430 - val_loss: 0.6843\nEpoch 16/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 290ms/step - accuracy: 0.5640 - loss: 0.6770 - val_accuracy: 0.5967 - val_loss: 0.6618\nEpoch 17/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 35s 237ms/step - accuracy: 0.5720 - loss: 0.6664 - val_accuracy: 0.6019 - val_loss: 0.6606\nEpoch 18/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 290ms/step - accuracy: 0.5999 - loss: 0.6596 - val_accuracy: 0.6071 - val_loss: 0.6574\nEpoch 19/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 290ms/step - accuracy: 0.6149 - loss: 0.6486 - val_accuracy: 0.6238 - val_loss: 0.6453\nEpoch 20/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 290ms/step - accuracy: 0.6232 - loss: 0.6408 - val_accuracy: 0.6324 - val_loss: 0.6439\n\n\n\nplot_history(history2)\n\n\n\n\n\n\n\n\n\nThe accuracy of model2 stabilized slightly above 60% during training. \nCompare that to the baseline, this model is 10 percent better. \nI didn’t observe overfitting since the val accuracy kept increasing."
  },
  {
    "objectID": "posts/Hw5/index.html#model-3",
    "href": "posts/Hw5/index.html#model-3",
    "title": "Image Classification with Keras",
    "section": "4. Model 3",
    "text": "4. Model 3\nIn this iteration of the model, we added a layer of preprocessor for standardization after input.\n\ni = keras.Input(shape=(150, 150, 3))\n# The pixel values have the range of (0, 255), but many models will work better if rescaled to (-1, 1.)\n# outputs: `(inputs * scale) + offset`\nscale_layer = keras.layers.Rescaling(scale=1 / 127.5, offset=-1)\nx = scale_layer(i)\npreprocessor = keras.Model(inputs = i, outputs = x)\n\n\nmodel3 = keras.models.Sequential([\n    layers.Input((150, 150, 3)),\n    preprocessor,               # add preprocessor to standardize after input\n    layers.RandomFlip(mode = 'horizontal'),\n    layers.RandomRotation(factor =  0.1),\n    layers.Dropout(0.2),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.Flatten(),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(2) # number of classes\n])\nmodel3.summary()\n\nModel: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n model (Functional)          (None, 150, 150, 3)       0         \n                                                                 \n random_flip (RandomFlip)    (None, 150, 150, 3)       0         \n                                                                 \n random_rotation (RandomRot  (None, 150, 150, 3)       0         \n ation)                                                          \n                                                                 \n dropout_1 (Dropout)         (None, 150, 150, 3)       0         \n                                                                 \n conv2d_3 (Conv2D)           (None, 148, 148, 64)      1792      \n                                                                 \n max_pooling2d_2 (MaxPoolin  (None, 74, 74, 64)        0         \n g2D)                                                            \n                                                                 \n conv2d_4 (Conv2D)           (None, 72, 72, 64)        36928     \n                                                                 \n max_pooling2d_3 (MaxPoolin  (None, 36, 36, 64)        0         \n g2D)                                                            \n                                                                 \n conv2d_5 (Conv2D)           (None, 34, 34, 64)        36928     \n                                                                 \n flatten_1 (Flatten)         (None, 73984)             0         \n                                                                 \n dense_2 (Dense)             (None, 64)                4735040   \n                                                                 \n dense_3 (Dense)             (None, 2)                 130       \n                                                                 \n=================================================================\nTotal params: 4810818 (18.35 MB)\nTrainable params: 4810818 (18.35 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\nmodel3.compile(optimizer='adam',\n              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\nhistory3 = model3.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 [==============================] - 122s 829ms/step - loss: 0.6917 - accuracy: 0.5808 - val_loss: 0.6528 - val_accuracy: 0.6337\nEpoch 2/20\n146/146 [==============================] - 651s 4s/step - loss: 0.6289 - accuracy: 0.6344 - val_loss: 0.6190 - val_accuracy: 0.6612\nEpoch 3/20\n146/146 [==============================] - 107s 730ms/step - loss: 0.5899 - accuracy: 0.6798 - val_loss: 0.5592 - val_accuracy: 0.7334\nEpoch 4/20\n146/146 [==============================] - 103s 704ms/step - loss: 0.5586 - accuracy: 0.7114 - val_loss: 0.5291 - val_accuracy: 0.7455\nEpoch 5/20\n146/146 [==============================] - 105s 719ms/step - loss: 0.5392 - accuracy: 0.7189 - val_loss: 0.5165 - val_accuracy: 0.7502\nEpoch 6/20\n146/146 [==============================] - 109s 743ms/step - loss: 0.5218 - accuracy: 0.7369 - val_loss: 0.5213 - val_accuracy: 0.7549\nEpoch 7/20\n146/146 [==============================] - 110s 755ms/step - loss: 0.4991 - accuracy: 0.7542 - val_loss: 0.5120 - val_accuracy: 0.7571\nEpoch 8/20\n146/146 [==============================] - 114s 777ms/step - loss: 0.4793 - accuracy: 0.7692 - val_loss: 0.4963 - val_accuracy: 0.7769\nEpoch 9/20\n146/146 [==============================] - 119s 818ms/step - loss: 0.4610 - accuracy: 0.7840 - val_loss: 0.5279 - val_accuracy: 0.7554\nEpoch 10/20\n146/146 [==============================] - 107s 729ms/step - loss: 0.4514 - accuracy: 0.7910 - val_loss: 0.5552 - val_accuracy: 0.7420\nEpoch 11/20\n146/146 [==============================] - 105s 718ms/step - loss: 0.4388 - accuracy: 0.8004 - val_loss: 0.4987 - val_accuracy: 0.7769\nEpoch 12/20\n146/146 [==============================] - 105s 721ms/step - loss: 0.4227 - accuracy: 0.8055 - val_loss: 0.5246 - val_accuracy: 0.7713\nEpoch 13/20\n146/146 [==============================] - 107s 730ms/step - loss: 0.4079 - accuracy: 0.8143 - val_loss: 0.5109 - val_accuracy: 0.7782\nEpoch 14/20\n146/146 [==============================] - 107s 731ms/step - loss: 0.3969 - accuracy: 0.8253 - val_loss: 0.4877 - val_accuracy: 0.7842\nEpoch 15/20\n146/146 [==============================] - 108s 738ms/step - loss: 0.3765 - accuracy: 0.8323 - val_loss: 0.4731 - val_accuracy: 0.7885\nEpoch 16/20\n146/146 [==============================] - 108s 736ms/step - loss: 0.3737 - accuracy: 0.8352 - val_loss: 0.4981 - val_accuracy: 0.7898\nEpoch 17/20\n146/146 [==============================] - 107s 731ms/step - loss: 0.3666 - accuracy: 0.8355 - val_loss: 0.4616 - val_accuracy: 0.7966\nEpoch 18/20\n146/146 [==============================] - 105s 721ms/step - loss: 0.3588 - accuracy: 0.8447 - val_loss: 0.4868 - val_accuracy: 0.7893\nEpoch 19/20\n146/146 [==============================] - 110s 753ms/step - loss: 0.3389 - accuracy: 0.8521 - val_loss: 0.4909 - val_accuracy: 0.8022\nEpoch 20/20\n146/146 [==============================] - 111s 760ms/step - loss: 0.3312 - accuracy: 0.8576 - val_loss: 0.4923 - val_accuracy: 0.8014\n\n\n\nplot_history(history3)\n\n\n\n\n\n\n\n\n\nThe accuracy of model3 stabilized between 85% and 86% during training. \nCompare that to the baseline, this model is about 35 percent better. \nI observe a little overfitting at the end since the val accuracy dropped a little."
  },
  {
    "objectID": "posts/Hw5/index.html#model-4",
    "href": "posts/Hw5/index.html#model-4",
    "title": "Image Classification with Keras",
    "section": "5. Model 4",
    "text": "5. Model 4\nWe are utilizing a exisiting base model MobileNetV3Large as a layer, and then we delete our preprocess layer since MobileNetV3Large already contains it.\n\nIMG_SHAPE = (150, 150, 3)\nbase_model = keras.applications.MobileNetV3Large(input_shape=IMG_SHAPE,\n                                               include_top=False,\n                                               weights='imagenet')\nbase_model.trainable = False\n\ni = keras.Input(shape=IMG_SHAPE)\nx = base_model(i, training = False)\nbase_model_layer = keras.Model(inputs = i, outputs = x)\n\nWARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\n\n\nWARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\n\n\n\nmodel4 = keras.models.Sequential([\n    layers.Input((150, 150, 3)),\n    layers.RandomFlip(mode = 'horizontal'),\n    layers.RandomRotation(factor =  0.1),\n    base_model_layer,   # add base model layer\n    layers.Flatten(),\n    layers.Dense(64),\n    layers.Dense(2) # number of classes\n])\nmodel4.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n random_flip_1 (RandomFlip)  (None, 150, 150, 3)       0         \n                                                                 \n random_rotation_1 (RandomR  (None, 150, 150, 3)       0         \n otation)                                                        \n                                                                 \n model (Functional)          (None, 5, 5, 960)         2996352   \n                                                                 \n flatten (Flatten)           (None, 24000)             0         \n                                                                 \n dense (Dense)               (None, 64)                1536064   \n                                                                 \n dense_1 (Dense)             (None, 2)                 130       \n                                                                 \n=================================================================\nTotal params: 4532546 (17.29 MB)\nTrainable params: 1536194 (5.86 MB)\nNon-trainable params: 2996352 (11.43 MB)\n_________________________________________________________________\n\n\n\nmodel4.compile(optimizer='adam',\n              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\nhistory4 = model4.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 [==============================] - 38s 245ms/step - loss: 1.1496 - accuracy: 0.9368 - val_loss: 0.3478 - val_accuracy: 0.9660\nEpoch 2/20\n146/146 [==============================] - 32s 220ms/step - loss: 0.2353 - accuracy: 0.9612 - val_loss: 0.1140 - val_accuracy: 0.9725\nEpoch 3/20\n146/146 [==============================] - 33s 225ms/step - loss: 0.0759 - accuracy: 0.9746 - val_loss: 0.0872 - val_accuracy: 0.9708\nEpoch 4/20\n146/146 [==============================] - 33s 226ms/step - loss: 0.0542 - accuracy: 0.9785 - val_loss: 0.0862 - val_accuracy: 0.9738\nEpoch 5/20\n146/146 [==============================] - 33s 229ms/step - loss: 0.0571 - accuracy: 0.9804 - val_loss: 0.0886 - val_accuracy: 0.9742\nEpoch 6/20\n146/146 [==============================] - 34s 232ms/step - loss: 0.0423 - accuracy: 0.9842 - val_loss: 0.0976 - val_accuracy: 0.9686\nEpoch 7/20\n146/146 [==============================] - 34s 233ms/step - loss: 0.0438 - accuracy: 0.9836 - val_loss: 0.1011 - val_accuracy: 0.9695\nEpoch 8/20\n146/146 [==============================] - 34s 235ms/step - loss: 0.0414 - accuracy: 0.9846 - val_loss: 0.1086 - val_accuracy: 0.9695\nEpoch 9/20\n146/146 [==============================] - 35s 240ms/step - loss: 0.0384 - accuracy: 0.9856 - val_loss: 0.1145 - val_accuracy: 0.9708\nEpoch 10/20\n146/146 [==============================] - 34s 236ms/step - loss: 0.0366 - accuracy: 0.9874 - val_loss: 0.1079 - val_accuracy: 0.9729\nEpoch 11/20\n146/146 [==============================] - 35s 238ms/step - loss: 0.0400 - accuracy: 0.9865 - val_loss: 0.1161 - val_accuracy: 0.9742\nEpoch 12/20\n146/146 [==============================] - 35s 241ms/step - loss: 0.0360 - accuracy: 0.9881 - val_loss: 0.1130 - val_accuracy: 0.9742\nEpoch 13/20\n146/146 [==============================] - 35s 242ms/step - loss: 0.0330 - accuracy: 0.9884 - val_loss: 0.1358 - val_accuracy: 0.9716\nEpoch 14/20\n146/146 [==============================] - 36s 244ms/step - loss: 0.0327 - accuracy: 0.9886 - val_loss: 0.1168 - val_accuracy: 0.9699\nEpoch 15/20\n146/146 [==============================] - 35s 237ms/step - loss: 0.0343 - accuracy: 0.9889 - val_loss: 0.1455 - val_accuracy: 0.9729\nEpoch 16/20\n146/146 [==============================] - 36s 244ms/step - loss: 0.0513 - accuracy: 0.9844 - val_loss: 0.1241 - val_accuracy: 0.9699\nEpoch 17/20\n146/146 [==============================] - 35s 243ms/step - loss: 0.0375 - accuracy: 0.9866 - val_loss: 0.1460 - val_accuracy: 0.9630\nEpoch 18/20\n146/146 [==============================] - 34s 236ms/step - loss: 0.0385 - accuracy: 0.9897 - val_loss: 0.1641 - val_accuracy: 0.9592\nEpoch 19/20\n146/146 [==============================] - 36s 244ms/step - loss: 0.0359 - accuracy: 0.9880 - val_loss: 0.1828 - val_accuracy: 0.9656\nEpoch 20/20\n146/146 [==============================] - 38s 260ms/step - loss: 0.0386 - accuracy: 0.9886 - val_loss: 0.1577 - val_accuracy: 0.9686\n\n\n\nplot_history(history4)\n\n\n\n\n\n\n\n\n\nThe accuracy of model4 stabilized between 96% and 98% during training. \nCompare that to the baseline, this model is about 46 percent better. \nI observe a lot overfitting at the end since the val accuracy drops and increase after epoch 3."
  },
  {
    "objectID": "posts/Hw5/index.html#tests",
    "href": "posts/Hw5/index.html#tests",
    "title": "Image Classification with Keras",
    "section": "6. Tests",
    "text": "6. Tests\nLets test our best performing model4 on our unseen test_ds!\n\nresults = model4.evaluate(test_ds)\nprint(f'Test loss is {results[0]}, test accuracy is {results[1]}')\n\n37/37 [==============================] - 6s 164ms/step - loss: 0.1822 - accuracy: 0.9656\nTest loss is 0.1821718066930771, test accuracy is 0.9656062126159668\n\n\nThe last test accuracy is about 96.56%, and this looks pretty good!"
  },
  {
    "objectID": "posts/Hw2/index.html",
    "href": "posts/Hw2/index.html",
    "title": "Web Scraping with Scrapy on TMDB",
    "section": "",
    "text": "In this blog post, I’m going to make a super cool web scraper that scrapes your favorite movie in the TMDB.  Here’s a link to my project repository."
  },
  {
    "objectID": "posts/Hw2/index.html#setups-with-terminal-command",
    "href": "posts/Hw2/index.html#setups-with-terminal-command",
    "title": "Web Scraping with Scrapy on TMDB",
    "section": "1. Setups with Terminal Command",
    "text": "1. Setups with Terminal Command\nHere’s how we set up the project. Assuming that you already got scrapy, first, we want to set up a scrapy project using terminal commands. Go to the folder that you want to create the project.\nconda activate PIC16B-24W \nscrapy startproject TMDB_scraper \ncd TMDB_scraper\nWe will see that scrapy.cfg is in this folder, this is where we want to run the other terminal commands. Then, let’s create a py file named tmdb_spider.ipynb in \\TMDB_scraper\\spiders which is all the spider files are located. Add the following lines to the file:\n# to run \n# scrapy crawl tmdb_spider -o movies.csv -a subdir=671-harry-potter-and-the-philosopher-s-stone\n\nimport scrapy\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n    def __init__(self, subdir=None, *args, **kwargs):\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]"
  },
  {
    "objectID": "posts/Hw2/index.html#setup-the-spider-file",
    "href": "posts/Hw2/index.html#setup-the-spider-file",
    "title": "Web Scraping with Scrapy on TMDB",
    "section": "2. Setup the spider file",
    "text": "2. Setup the spider file\nLet’s go through different parse functions that I used to consctruct this spider.\ndef parse(self, response): \n    \"\"\"\n    add /cast to the url to go to full cast page\n    and use parse_full_credits and call parse_full_credit\n    \"\"\"\n    current_url = response.urljoin(f'{response.url}/cast') \n    yield scrapy.Request(current_url, callback=self.parse_full_credits)\nThis is the parse() implementation in our spider class. This will be the first function that will be ran when we call this spider in the terminal. This works very similar to scrapy.request(start_url, self.parse) which return us a response object to play with. Use urljoin to add /cast at the end to take us to the full cast list for your favorite movies. Then, we go to the next function parse_full_credits.\ndef parse_full_credits(self, response):\n        \"\"\"\n        This function is going to parse the full cast and crew page, \n        div @class=info is where all the actors who have an acting role in the movie is\n        use xpath to take different actors' name and information and put them in a dictionary\n        Then, added credit_department=Acting to let next function to go to actor's personal page with acting credits\n        \"\"\"\n        self.actors = {\"actor\": response.xpath(\"//div[@class='info']/p/a/text()\").getall(), \n                         \"web\": response.xpath(\"//div[@class='info']/p/a/@href\").getall()}\n        for i in range(len(self.actors[\"actor\"])):\n            yield scrapy.Request(\"https://www.themoviedb.org\" + self.actors[\"web\"][i] + \"?credit_department=Acting\"\n                                 , callback=self.parse_actor_page)\nTo write this method, I used xpath to take different actors’ name using text(), and their corresponding link with @href from the same element which is from the div[@class ='info']/p/a. We also use string manipulation to go to the actor page with the specific credit category for acting, because we dont want to see a movie that our favorite actors are only crew for the movie. We can set up a loop for each actor in our favorite movie then call our next function parse_actor_page.\ndef parse_actor_page(self, response):\n        \"\"\"\n        This function is parsing actor's personal page, \n        Xpath direction h2[@class = 'title'] contains information of all the movies or tv names that the actor have an acting role in this page. \n        Then, return a dictionary with actor name and movies and tv names for each movie or tv\n        \"\"\"\n        actor_name = response.xpath(\"//h2[@class='title']/a/text()\").get()\n        movie_or_TV_name_list = response.xpath(\"//a[@class='tooltip']/bdi/text()\").getall()\n        for movie_or_TV_name in movie_or_TV_name_list:\n            yield {\"actor\" : actor_name, \"movie_or_TV_name\" : movie_or_TV_name}\nFinally, for parse_actor_page we want to yield a single dictionary with certain actors for each movies or TV they work in, which we get by associating xpath a[@class='tooltip']/bdi’s text in the html elements and loop through all the movie or TV names."
  },
  {
    "objectID": "posts/Hw2/index.html#making-recommandations",
    "href": "posts/Hw2/index.html#making-recommandations",
    "title": "Web Scraping with Scrapy on TMDB",
    "section": "3. Making Recommandations",
    "text": "3. Making Recommandations\nSince my professor’s favorite movie is Harry Potter and the Philosopher’s stone. Let’s run the spider with scrapy crawl tmdb_spider -o movies.csv -a subdir=671-harry-potter-and-the-philosopher-s-stone, and getting results.csv file with columns for actor names and the movies and TV shows on which they featured in.\n\nimport pandas as pd\nimport plotly.express as px\nimport plotly.io as pio\npio.renderers.default = \"iframe\"\ndf = pd.read_csv('results.csv')\ndf\n\n\n\n\n\n\n\n\nactor\nmovie_or_TV_name\n\n\n\n\n0\nDaniel Radcliffe\nHave I Got a Bit More News for You\n\n\n1\nDaniel Radcliffe\nDavid Holmes: The Boy Who Lived\n\n\n2\nDaniel Radcliffe\n100 Years of Warner Bros.\n\n\n3\nDaniel Radcliffe\nMulligan\n\n\n4\nDaniel Radcliffe\nDigman!\n\n\n...\n...\n...\n\n\n2952\nRupert Grint\nThe View\n\n\n2953\nRupert Grint\nGMTV\n\n\n2954\nRupert Grint\nThe Tonight Show with Jay Leno\n\n\n2955\nRupert Grint\nAn Audience with...\n\n\n2956\nRupert Grint\nToday\n\n\n\n\n2957 rows × 2 columns\n\n\n\nWe can see that this movie list is populated with our star actor Daniel Radcliffe. Let’s show three movies or TV shows for each actor.\n\nnew_list = pd.DataFrame()\nfor actor in df.actor.unique():\n    new_list = pd.concat([new_list, df[df.actor == actor][0:3]], ignore_index = True)\nnew_list\n\n\n\n\n\n\n\n\nactor\nmovie_or_TV_name\n\n\n\n\n0\nDaniel Radcliffe\nHave I Got a Bit More News for You\n\n\n1\nDaniel Radcliffe\nDavid Holmes: The Boy Who Lived\n\n\n2\nDaniel Radcliffe\n100 Years of Warner Bros.\n\n\n3\nJamie Yeates\nCreating the World of Harry Potter\n\n\n4\nJamie Yeates\nHarry Potter and the Chamber of Secrets\n\n\n...\n...\n...\n\n\n176\nEmma Watson\nPickled\n\n\n177\nEmma Watson\nPrada Paradoxe\n\n\n178\nRupert Grint\nEnemy of Man\n\n\n179\nRupert Grint\nDavid Holmes: The Boy Who Lived\n\n\n180\nRupert Grint\nKnock at the Cabin\n\n\n\n\n181 rows × 2 columns\n\n\n\nThis is a small enough list for us to read for our favorite movie!  Now we can also make some pretty plots for this dataframe. For example, these is a bar graph for occurance of movies for all the actors appeared in this movie.\n\npx.bar(df.actor.value_counts(), labels = {'value': \"movies and TV\", 'variable' : 'occurance'}, title=\"Appearance in Movies or Shows for Actors in HP and the Philosopher's stone\")  # using value counts to get occurance of a value in a dataframe column. \n\n\n\n\nSuppose that I only want to see Harry Potter movies, this is also an option to do that.\n\nhpdf = df[df.movie_or_TV_name.str[0:12] == \"Harry Potter\"].reset_index(drop=True)  # using string method for all movie and tv names to get starting phrase with Harry Potter\nhpdf\n\n\n\n\n\n\n\n\nactor\nmovie_or_TV_name\n\n\n\n\n0\nDaniel Radcliffe\nHarry Potter 20th Anniversary: Return to Hogwarts\n\n\n1\nDaniel Radcliffe\nHarry Potter and the Deathly Hallows: Part 2\n\n\n2\nDaniel Radcliffe\nHarry Potter and the Deathly Hallows: Part 1\n\n\n3\nDaniel Radcliffe\nHarry Potter and the Half-Blood Prince\n\n\n4\nDaniel Radcliffe\nHarry Potter and the Order of the Phoenix\n\n\n...\n...\n...\n\n\n242\nRupert Grint\nHarry Potter and the Order of the Phoenix\n\n\n243\nRupert Grint\nHarry Potter and the Goblet of Fire\n\n\n244\nRupert Grint\nHarry Potter and the Prisoner of Azkaban\n\n\n245\nRupert Grint\nHarry Potter and the Chamber of Secrets\n\n\n246\nRupert Grint\nHarry Potter and the Philosopher's Stone\n\n\n\n\n247 rows × 2 columns\n\n\n\nI am also interested that how many of the actors act through the entire HP movies, and how many actors only acted a few times, and I also included Harry Potter 20th Anniversary.\n\nprint(hpdf.actor.value_counts())  # use the same trick to get appearance counts\npx.bar(hpdf.actor.value_counts(), labels = {'variable': 'occurance', 'value': \"movies\"},title=\"Appearance in Each Harry Potter Movies or Shows for Actors\")\n\nactor\nTom Felton           10\nMatthew Lewis        10\nDaniel Radcliffe      9\nBonnie Wright         9\nEmma Watson           9\n                     ..\nSaunders Triplets     1\nJean Southern         1\nKieri Kennedy         1\nLeila Hoffman         1\nElizabeth Spriggs     1\nName: count, Length: 63, dtype: int64\n\n\n\n\n\nThis data only contains the actors that appeared in the first film which is a bummer. It is also very interesting that Tom Felton and Mathew Lewis appears more times than Daniel Radcliffe who is the protagonist of the series!"
  },
  {
    "objectID": "posts/Hw4/index.html",
    "href": "posts/Hw4/index.html",
    "title": "Heat Equation with Jax",
    "section": "",
    "text": "In this blog post, I’m going to make a super cool optimization for heat equation with jax and jit(Just-In-Time compilation)."
  },
  {
    "objectID": "posts/Hw4/index.html#set-up-the-basics",
    "href": "posts/Hw4/index.html#set-up-the-basics",
    "title": "Heat Equation with Jax",
    "section": "Set up the basics",
    "text": "Set up the basics\nFor this problem, we will use N = 101 sized matrix and epsilon as diffusion coefficient, and import needed packages.\n\n# import plotly.io as pio\n# pio.renderers.default = \"iframe\"\n# to make sure that graphs are properly displayed\n\n\nN = 101\nepsilon = 0.2\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport jax\nfrom jax.experimental import sparse\nimport jax.numpy as jnp\nimport inspect\nimport time\n\nThen, we can set up a matrix with 1 unit of heat in the middle point of the matrix.\n\n# construct initial condition: 1 unit of heat at midpoint. \nu0 = np.zeros((N, N))\nu0[int(N/2), int(N/2)] = 1.0\nplt.imshow(u0)\n\n\n\n\n\n\n\n\nImport our function defined in the heat_equation.py file.\n\nfrom heat_equation import get_A\nfrom heat_equation import get_sparse_A\nfrom heat_equation import advance_time_matvecmul\nfrom heat_equation import advance_time_spmatvec\nfrom heat_equation import advance_time_numpy\nfrom heat_equation import advance_time_jax\n\nget_A function get the corresponding A matrix for matrix multiplication for our N.\n\nprint(inspect.getsource(get_A))\n\ndef get_A(N):\n    \"\"\"\n    take N as input for matrix size and output matrix A for matrix multiplication used in heat equation\n    \"\"\"\n    n = N * N\n    diagonals = [-4 * np.ones(n), np.ones(n-1), np.ones(n-1), np.ones(n-N), np.ones(n-N)]\n    diagonals[1][(N-1)::N] = 0\n    diagonals[2][(N-1)::N] = 0\n    A = np.diag(diagonals[0]) + np.diag(diagonals[1], 1) + np.diag(diagonals[2], -1) + np.diag(diagonals[3], N) + np.diag(diagonals[4], -N)\n    return A\n\n\n\nadvance_time_matvecmul function multiply the input u matrix with A to simulate time advancement and reshape it back to N x N.\n\nprint(inspect.getsource(advance_time_matvecmul))\n\ndef advance_time_matvecmul(A, u, epsilon):\n    \"\"\"Advances the simulation by one timestep, via matrix-vector multiplication\n    Args:\n        u: N x N grid state at timestep k\n        epsilon: stability constant\n\n    Returns:\n        N x N Grid state at instant k+1\n    \"\"\"\n    N = u.shape[0]\n    u = u + epsilon * (A @ u.flatten()).reshape((N, N))\n    return u\n\n\n\nThis function runs our iteration for 2 * M times and out put a time to run for M times, then output a image representation for every 300 iterations. This function will also time these advancement iterations.\n\ndef iterate_throught_func(advance_func, M = 2700, init = u0, epsilon = epsilon, **kwargs):\n    \"\"\"\n    this function take advance_func function, M iterations, init initial matrix, epsilon diffusion coefficient, \n    and output after M iterations, and output a heatmap for every 300 iterations\n    also times how long does it take to advance function for M times\n    \"\"\"\n    fig, axs = plt.subplots(3, 3, figsize = (10, 10))\n    fig.tight_layout()\n    u_i = init\n    storage = []\n    # use time() to time the total run time of 2700 iterations\n    start = time.time()\n    for i in range(M):\n        storage.append(u_i)\n        u_i = advance_func(u = u_i, epsilon = epsilon, **kwargs)\n    end = time.time()\n    print(end - start)\n    # run it the second time to get picture\n    u_i = init\n    storage = []\n    for i in range(M):\n        storage.append(u_i)\n        u_i = advance_func(u = u_i, epsilon = epsilon, **kwargs)\n    for i in range(int(M/300)):\n        axs[int(i/3), int(i%3)].imshow(storage[i*300])\n        axs[int(i/3), int(i%3)].set_title(f\"Heatmap at t = {i*300}\")\n\n\niterate_throught_func(advance_time_matvecmul, 2700, A = get_A(N))  # advance our first function for 2700 times\n\n36.03676176071167"
  },
  {
    "objectID": "posts/Hw4/index.html#with-jax",
    "href": "posts/Hw4/index.html#with-jax",
    "title": "Heat Equation with Jax",
    "section": "2. With Jax",
    "text": "2. With Jax\nget_sparse_A will return a matrix A in sparsed form with sparse.BC00.fromdense(), and utilize it in our function to multiply. With sparsed matrix, matrix multiplication will be much faster for each iteration of time advancement. Then, I have added advance_time_spmatvec = jax.jit(advance_time_spmatvec) to jit our function which will speed up our function.\n\nprint(inspect.getsource(get_sparse_A)) \nprint(inspect.getsource(advance_time_spmatvec))\n\ndef get_sparse_A(N): \n    \"\"\"\n    take N as input for matrix size and output sparsed matrix A for matrix multiplication used in heat equation\n    \"\"\"\n    A_sp_matrix = sparse.BCOO.fromdense(get_A(N))\n    return A_sp_matrix\n\ndef advance_time_spmatvec(A, u, epsilon):\n    \"\"\"\n    take N as input for matrix size and output sparsed matrix A for matrix multiplication used in heat equation\n    then jit this function with jax jit in next line\n    \"\"\"\n    N = u.shape[0]\n    u = u + epsilon * (A @ u.flatten()).reshape((N, N))\n    return u\n\n\n\n\niterate_throught_func(advance_time_spmatvec, 2700, A = get_sparse_A(N))\n\n0.7655937671661377"
  },
  {
    "objectID": "posts/Hw4/index.html#with-numpy",
    "href": "posts/Hw4/index.html#with-numpy",
    "title": "Heat Equation with Jax",
    "section": "3. With Numpy",
    "text": "3. With Numpy\nadvance_time_numpy will utilize np.roll function to add value to four different direction and reduce the original point based on the heat diffusion funciton. With np.roll() we can apply the heat equation iteration in a vectorized form instead of doing O(N^3) level of matrix multiplication.\n\nprint(inspect.getsource(advance_time_numpy))\n\ndef advance_time_numpy(u, epsilon):\n    \"\"\"\n    take matrix u and diffusion coefficient epsilon as input, output u in the next iteration\n    by utilizing np.roll function to add value to four different direction and reduce the original point\n    \"\"\"\n    u = np.pad(u, 2)\n    u = u + epsilon * (np.roll(u, 1, axis = 0) + np.roll(u, -1, axis = 0) + np.roll(u, 1, axis = 1) + np.roll(u, -1, axis = 1) - 4 * u)\n    return u[2:-2, 2: -2]\n\n\n\n\niterate_throught_func(advance_time_numpy, 2700)\n\n0.25177907943725586"
  },
  {
    "objectID": "posts/Hw4/index.html#with-jax-1",
    "href": "posts/Hw4/index.html#with-jax-1",
    "title": "Heat Equation with Jax",
    "section": "4. With jax",
    "text": "4. With jax\nThis is similar to the numpy method which we use jnp.roll instead of np.roll to add value to four different direction and reduce the original point based on the heat diffusion funciton, and we can also jit the function to make it to run faster.\n\nprint(inspect.getsource(advance_time_jax))\n\ndef advance_time_jax(u, epsilon):\n    \"\"\"\n    take matrix u and diffusion coefficient epsilon as input, output u in the next iteration\n    by utilizing np.roll function to add value to four different direction and reduce the original point\n    then jit this function with jax jit in next line\n    \"\"\"\n    u = jnp.pad(u, 2)\n    u = u + epsilon * (jnp.roll(u, 1, axis = 0) + jnp.roll(u, -1, axis = 0) + jnp.roll(u, 1, axis = 1) + jnp.roll(u, -1, axis = 1) - 4 * u)\n    return u[2:-2, 2: -2]\n\n\n\n\niterate_throught_func(advance_time_jax, 2700)\n\n0.06670808792114258"
  },
  {
    "objectID": "posts/Hw4/index.html#compare-the-methods",
    "href": "posts/Hw4/index.html#compare-the-methods",
    "title": "Heat Equation with Jax",
    "section": "5. Compare the methods",
    "text": "5. Compare the methods\nThe matrix multiplication is the slowest. Then, with sparsed matrix and jitted first function give us an about 20x faster than Part 1.  Utilizing np.roll() give the cpu a much easier task to calculate by only moving the heat points around and add them together, and jax just give a better performance than np in general. Method one appears to be the easiest, since the code is already provided."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!"
  },
  {
    "objectID": "posts/Hw0/index.html",
    "href": "posts/Hw0/index.html",
    "title": "Data Visualization of Palmer Penguins",
    "section": "",
    "text": "Data Visualization of Palmer Penguins\nPalmer penguins are fun and we should visualize some of their datas. These are the three different species of Palmer penguins.  image source\nImporting packages and reading penguins datasets.\n\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)  # accessing the csv and set as a panda dataframe\n\nFrom the example in this Github page of Palmer Penguins, it looks very intuitive to plot a scatter plot with flipper length and body mass from the data. We can also make the visualization clear using the ‘hue’ arguement to show different color for each datapoint for each species.\n\nsns.scatterplot(penguins, x = 'Body Mass (g)', y = 'Flipper Length (mm)', hue = 'Species').set(title='Flipper Length and Body Mass relationships in three species of penguins')  # plotting a scatterplot with Body mass and Flipper length with different species of penguins. \nplt.legend(bbox_to_anchor=(1, 1),loc=2)  # setting legends outside of the graph for clarity\nplt.title('Body Mass and Flipper Length Relationships based on different speicies of penguins')  # title for the graph\n\nText(0.5, 1.0, 'Body Mass and Flipper Length Relationships based on different speicies of penguins')"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "myblog",
    "section": "",
    "text": "Option Pricing with One-dimension Convolutional Neural Network\n\n\n\n\n\n\nfinal project\n\n\n\n\n\n\n\n\n\nMar 1, 2024\n\n\nYichen Wang, Xipeng Du, Manshu Huang\n\n\n\n\n\n\n\n\n\n\n\n\nImage Classification with Keras\n\n\n\n\n\n\nhw5\n\n\nneuro networks\n\n\nkeras\n\n\n\n\n\n\n\n\n\nFeb 29, 2024\n\n\nXipeng Du\n\n\n\n\n\n\n\n\n\n\n\n\nHeat Equation with Jax\n\n\n\n\n\n\nhw4\n\n\njax\n\n\nnumpy\n\n\n\n\n\n\n\n\n\nFeb 22, 2024\n\n\nXipeng Du\n\n\n\n\n\n\n\n\n\n\n\n\nWeb Development with Flask - A Simple Implementation of Online Message Bank\n\n\n\n\n\n\nhw3\n\n\nweb development\n\n\n\n\n\n\n\n\n\nFeb 10, 2024\n\n\nXipeng Du\n\n\n\n\n\n\n\n\n\n\n\n\nWeb Scraping with Scrapy on TMDB\n\n\n\n\n\n\nhw2\n\n\nweb scraping\n\n\n\n\n\n\n\n\n\nFeb 4, 2024\n\n\nXipeng Du\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding Global Temperature Database and Use Ploty Express to Make Interesting Figures\n\n\n\n\n\n\nhw1\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJan 26, 2024\n\n\nXipeng Du\n\n\n\n\n\n\n\n\n\n\n\n\nData Visualization of Palmer Penguins\n\n\n\n\n\n\nhw0\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJan 15, 2024\n\n\nXipeng Du\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJan 6, 2024\n\n\nXipeng\n\n\n\n\n\n\nNo matching items"
  }
]